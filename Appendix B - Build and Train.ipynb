{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zJ7C4qZl-9Lx"
   },
   "source": [
    "Testing Deep Learning on zernike polynomials up to degree 4\n",
    "\n",
    "All values for $Z_n$ to be between -1 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gj1XIzSd-9L1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from random import randrange\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import opticspy\n",
    "# Using forked opticspy - https://github.com/aambrose-unh/opticspy/tree/plot_additions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4YCvlYiU-9L7"
   },
   "outputs": [],
   "source": [
    "from keras.initializers import RandomUniform\n",
    "from keras.layers import LeakyReLU\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir_linux = '/content/'\n",
    "# for using on google colab\n",
    "\n",
    "parent_dir = ''\n",
    "# for using on local windows machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bVqaPiBJ-9L4"
   },
   "outputs": [],
   "source": [
    "def extract_test_plan(test_num,\n",
    "            file_path='test_plan_generator.csv'):\n",
    "    print(file_path)\n",
    "    full_test_plan = pd.read_csv(file_path,header=0)\n",
    "    test_plan = full_test_plan[full_test_plan['test_num'] == test_num]\n",
    "    test_plan = test_plan.reset_index(drop=True)\n",
    "    \n",
    "    return test_plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_TNLl1Vt-9L5"
   },
   "outputs": [],
   "source": [
    "#Create directory to store info\n",
    "def create_dir(testing,parent_dir,linux=False,name_append=''):\n",
    "    import os\n",
    "    os.chdir(parent_dir)\n",
    "    if linux==False:  \n",
    "        if os.path.isdir('.\\\\Test_{}{}'.format(testing,name_append)):\n",
    "            os.chdir('.\\\\Test_{}{}'.format(testing,name_append))\n",
    "        else:\n",
    "            os.makedirs('.\\\\Test_{}{}'.format(testing,name_append))\n",
    "            os.chdir('.\\\\Test_{}{}'.format(testing,name_append))\n",
    "    else:\n",
    "        if os.path.isdir('./Test_{}{}'.format(testing,name_append)):\n",
    "            os.chdir('./Test_{}{}'.format(testing,name_append))\n",
    "        else:\n",
    "            os.makedirs('./Test_{}{}'.format(testing,name_append))\n",
    "            os.chdir('./Test_{}{}'.format(testing,name_append))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from folder:\n",
    "# Data is a set of random coefficients to apply to zernike polynomials through 4th order\n",
    "# The input data samples are 4 columns, x,y,dx,dy\n",
    "# The coefficients will be the targets/output\n",
    "\n",
    "def create_data(test_plan):    \n",
    "    \n",
    "    train_samp_num = int(test_plan.loc[0,'train_samp_num'])\n",
    "    test_samp_num = int(test_plan.loc[0,'test_samp_num'])\n",
    "    \n",
    "# Load data\n",
    "    train_coeff = np.load('../train_coeff.npy')[:train_samp_num]\n",
    "    \n",
    "    train_input_full = np.load('../train_input_normalized.npy',allow_pickle=True)[:train_samp_num]\n",
    "    test_input_full = np.load('../test_input_normalized.npy',allow_pickle=True)[:test_samp_num]\n",
    "    test_coeff = np.load('../test_coeff.npy')[:test_samp_num]\n",
    "    \n",
    "    train_input = np.array([train_input_full[n] for n in range(train_samp_num)])\n",
    "    test_input = np.array([test_input_full[n] for n in range(test_samp_num)])\n",
    "    \n",
    "    return train_coeff,train_input,test_coeff,test_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s3HwYQ5b-9L8"
   },
   "outputs": [],
   "source": [
    "def model_create(test_plan,sqr_grid_width=50):\n",
    "    model = models.Sequential()\n",
    "    input_length = sqr_grid_width**2\n",
    "\n",
    "    for i in range(int(test_plan['layer_num'].count())):\n",
    "        reg_rate = float(test_plan['kernel_regularizer'][i])\n",
    "       \n",
    "        if i == 0:\n",
    "            if test_plan['activation'][i] == 'LeakyReLU':\n",
    "                    model.add(layers.Dense(int(test_plan['num_nodes'][i]), input_shape=(input_length,4),\n",
    "                                   kernel_regularizer=regularizers.l2(reg_rate)))\n",
    "                    model.add(LeakyReLU())    \n",
    "            else:\n",
    "                try:\n",
    "                    model.add(layers.Dense(int(test_plan['num_nodes'][i]), activation=test_plan['activation'][i],\n",
    "                                input_shape=(input_length,4),\n",
    "                                   kernel_regularizer=regularizers.l2(reg_rate))) \n",
    "                except:\n",
    "                    print('Check the input_shape parameter. This is specified in a separate file used to create \\\n",
    "                          the data stored in .npy files')\n",
    "\n",
    "        \n",
    "        #Flatten the data to make compatible for output\n",
    "        elif test_plan['layer_type'][i] == 'Flatten':\n",
    "                model.add(Flatten())\n",
    "        \n",
    "        elif i == int(test_plan['layer_num'].count())-1:\n",
    "            model.add(layers.Dense(int(test_plan['num_nodes'][i]),\n",
    "                                   kernel_regularizer=regularizers.l2(reg_rate)))\n",
    "                \n",
    "        #Define typical layer\n",
    "        elif test_plan['activation'][i] == 'LeakyReLU':\n",
    "            model.add(layers.Dense(int(test_plan['num_nodes'][i]),\n",
    "                                   kernel_regularizer=regularizers.l2(reg_rate)))\n",
    "            model.add(LeakyReLU())\n",
    "                      \n",
    "        else:\n",
    "            model.add(layers.Dense(int(test_plan['num_nodes'][i]), activation=test_plan['activation'][i],\n",
    "                                   kernel_regularizer=regularizers.l2(reg_rate)))\n",
    "            \n",
    "    optimizer = getattr(optimizers,test_plan['optimizer'][0])\n",
    "    lr=test_plan['lr'][0]\n",
    "    \n",
    "    model.compile(optimizer=optimizer(lr=lr), loss=test_plan['loss'][0], metrics=['mae'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v7DgLaTJ-9L-"
   },
   "outputs": [],
   "source": [
    "# Simple scaling normalization\n",
    "def zernike_gen_ones(batch_size=16,sqr_grid_width=50):\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        train_coeff = np.transpose([[random.uniform(-1.,1.) for Z1 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z2 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z3 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z4 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z5 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z6 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z7 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z8 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z9 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z10 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z11 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z12 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z13 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z14 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z15 in range(batch_size)]])\n",
    "        \n",
    "#         Divide x and y by sqr_grid_width to normalize between 0 and 1\n",
    "        x = np.array([np.array([[i for i in range(sqr_grid_width)] for x in range(sqr_grid_width)]).flatten()\n",
    "                           for z in range(batch_size)])/float(sqr_grid_width)\n",
    "        y = np.array([np.array([[y for i in range(sqr_grid_width)] for y in range(sqr_grid_width)]).flatten()\n",
    "                           for z in range(batch_size)])/float(sqr_grid_width)\n",
    "\n",
    "\n",
    "        # z_grid to be used to create training data for dx and dy to create the gradient data\n",
    "        z_grid_train = np.array([np.array(opticspy.zernike.Coefficient(Z1=train_coeff[z][0],\n",
    "                            Z2=train_coeff[z][1], Z3=train_coeff[z][2], Z4=train_coeff[z][3], Z5=train_coeff[z][4],\n",
    "                            Z6=train_coeff[z][5], Z7=train_coeff[z][6], Z8=train_coeff[z][7], Z9=train_coeff[z][8],\n",
    "                            Z10=train_coeff[z][9], Z11=train_coeff[z][10], Z12=train_coeff[z][11], Z13=train_coeff[z][12],\n",
    "                            Z14=train_coeff[z][13], Z15=train_coeff[z][14]).zernikematrix(l=sqr_grid_width))\n",
    "                            for z in range(batch_size)])\n",
    "\n",
    "\n",
    "        gradient_train = [np.gradient(z_grid_train[i]) for i in range(batch_size)]\n",
    "        \n",
    "        dx_train = np.array([np.array([x for x in gradient_train[i][0]]).flatten() for i in range(batch_size)])\n",
    "        dy_train = np.array([np.array([y for y in gradient_train[i][1]]).flatten() for i in range(batch_size)])\n",
    "\n",
    "        train_input = np.array([np.transpose([x[i], y[i], dx_train[i], dy_train[i]])\n",
    "                      for i in range(batch_size)])\n",
    "\n",
    "        yield train_input,train_coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v7DgLaTJ-9L-"
   },
   "outputs": [],
   "source": [
    "# Simple scaling normalization\n",
    "def zernike_gen(batch_size=16,sqr_grid_width=50):\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        train_coeff = np.transpose([[random.uniform(-2.,3.) for Z1 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z2 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z3 in range(batch_size)],\n",
    "                                    [random.uniform(-9.,5.) for Z4 in range(batch_size)],\n",
    "                                    [random.uniform(-4.,2.) for Z5 in range(batch_size)],\n",
    "                                    [random.uniform(-3.,3.2) for Z6 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.4) for Z7 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z8 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z9 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z10 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z11 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z12 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z13 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z14 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z15 in range(batch_size)]])\n",
    "        \n",
    "#         Divide x and y by sqr_grid_width to normalize between 0 and 1\n",
    "        x = np.array([np.array([[i for i in range(sqr_grid_width)] for x in range(sqr_grid_width)]).flatten()\n",
    "                           for z in range(batch_size)])/float(sqr_grid_width)\n",
    "        y = np.array([np.array([[y for i in range(sqr_grid_width)] for y in range(sqr_grid_width)]).flatten()\n",
    "                           for z in range(batch_size)])/float(sqr_grid_width)\n",
    "\n",
    "\n",
    "        # z_grid to be used to create training data for dx and dy to create the gradient data\n",
    "        z_grid_train = np.array([np.array(opticspy.zernike.Coefficient(Z1=train_coeff[z][0],\n",
    "                            Z2=train_coeff[z][1], Z3=train_coeff[z][2], Z4=train_coeff[z][3], Z5=train_coeff[z][4],\n",
    "                            Z6=train_coeff[z][5], Z7=train_coeff[z][6], Z8=train_coeff[z][7], Z9=train_coeff[z][8],\n",
    "                            Z10=train_coeff[z][9], Z11=train_coeff[z][10], Z12=train_coeff[z][11], Z13=train_coeff[z][12],\n",
    "                            Z14=train_coeff[z][13], Z15=train_coeff[z][14]).zernikematrix(l=sqr_grid_width))\n",
    "                            for z in range(batch_size)])\n",
    "\n",
    "\n",
    "        gradient_train = [np.gradient(z_grid_train[i]) for i in range(batch_size)]\n",
    "        \n",
    "        dx_train = np.array([np.array([x for x in gradient_train[i][0]]).flatten() for i in range(batch_size)])\n",
    "        dy_train = np.array([np.array([y for y in gradient_train[i][1]]).flatten() for i in range(batch_size)])\n",
    "\n",
    "        train_input = np.array([np.transpose([x[i], y[i], dx_train[i], dy_train[i]])\n",
    "                      for i in range(batch_size)])\n",
    "\n",
    "        yield train_input,train_coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "va9qaeZv-9L_"
   },
   "outputs": [],
   "source": [
    "def block_gen(min_blk=2,max_blk=10,sqr_grid_width=50):\n",
    "    x_block_size = randrange(min_blk,max_blk+1) \n",
    "    x_block_start = randrange(sqr_grid_width-1-x_block_size) \n",
    "    x_block = np.array(range(x_block_start,x_block_start+x_block_size))/float(sqr_grid_width)\n",
    "\n",
    "    y_block_size = randrange(min_blk,max_blk+1) \n",
    "    y_block_start = randrange(sqr_grid_width-1-y_block_size) \n",
    "    y_block = np.array(range(y_block_start,y_block_start+y_block_size))/float(sqr_grid_width)\n",
    "    \n",
    "    return x_block,y_block\n",
    "\n",
    "y_mask = lambda y,batch,block: np.isin(y[batch][:,1],block)\n",
    "\n",
    "x_mask = lambda x,batch,block: np.isin(x[batch][:,0],block)\n",
    "\n",
    "def zernike_gen_block(batch_size=16,sqr_grid_width=50):\n",
    "   \n",
    "    while True:\n",
    "        \n",
    "        train_coeff = np.transpose([[random.uniform(-2.,3.) for Z1 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z2 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z3 in range(batch_size)],\n",
    "                                    [random.uniform(-9.,5.) for Z4 in range(batch_size)],\n",
    "                                    [random.uniform(-4.,2.) for Z5 in range(batch_size)],\n",
    "                                    [random.uniform(-3.,3.2) for Z6 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.4) for Z7 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z8 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z9 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z10 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z11 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z12 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z13 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z14 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z15 in range(batch_size)]])\n",
    "\n",
    "\n",
    "        x = np.array([np.array([[i for i in range(sqr_grid_width)] for x in range(sqr_grid_width)]).flatten()\n",
    "                           for z in range(batch_size)])/float(sqr_grid_width)\n",
    "\n",
    "        y = np.array([np.array([[y for i in range(sqr_grid_width)] for y in range(sqr_grid_width)]).flatten()\n",
    "                           for z in range(batch_size)])/float(sqr_grid_width)\n",
    "\n",
    "\n",
    "        # z_grid to be used to create training data for dx and dy to create the gradient data\n",
    "        z_grid_train = np.array([np.array(opticspy.zernike.Coefficient(Z1=train_coeff[z][0],\n",
    "                            Z2=train_coeff[z][1], Z3=train_coeff[z][2], Z4=train_coeff[z][3], Z5=train_coeff[z][4],\n",
    "                            Z6=train_coeff[z][5], Z7=train_coeff[z][6], Z8=train_coeff[z][7], Z9=train_coeff[z][8],\n",
    "                            Z10=train_coeff[z][9], Z11=train_coeff[z][10], Z12=train_coeff[z][11], Z13=train_coeff[z][12],\n",
    "                            Z14=train_coeff[z][13], Z15=train_coeff[z][14]).zernikematrix(l=sqr_grid_width))\n",
    "                            for z in range(batch_size)])\n",
    "\n",
    "\n",
    "        gradient_train = [np.gradient(z_grid_train[i]) for i in range(batch_size)]\n",
    "\n",
    "        dx_train = np.array([np.array([x for x in gradient_train[i][0]]).flatten() for i in range(batch_size)])\n",
    "        dy_train = np.array([np.array([y for y in gradient_train[i][1]]).flatten() for i in range(batch_size)])\n",
    "\n",
    "        train_input = np.array([np.transpose([x[i], y[i], dx_train[i], dy_train[i]])\n",
    "                            for i in range(batch_size)])\n",
    "\n",
    "    #     NOTE: every sample in batch has the same block removed with this set up\n",
    "    #     Generate random block\n",
    "        x_block,y_block = block_gen(min_blk=2,max_blk=10,sqr_grid_width=sqr_grid_width)\n",
    "\n",
    "    #     Remove random block\n",
    "        for i in range(batch_size):\n",
    "            train_input[i][(y_mask(train_input,i,y_block)) & (x_mask(train_input,i,x_block))] = 0\n",
    "\n",
    "\n",
    "        yield train_input,train_coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BHee32fS-9MB"
   },
   "outputs": [],
   "source": [
    "# Create random matrix to add to dx and dy\n",
    "def add_noise(original,batch_size=1,mu=0,sigma=.01):\n",
    "    noise = [[random.gauss(mu, sigma) for x in range(len(original[0]))] for i in range(batch_size)]\n",
    "    mod_data = original + noise\n",
    "    return np.array(mod_data)\n",
    "\n",
    "def zernike_gen_noise(batch_size=16,sqr_grid_width=50):\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        train_coeff = np.transpose([[random.uniform(-2.,3.) for Z1 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z2 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z3 in range(batch_size)],\n",
    "                                    [random.uniform(-9.,5.) for Z4 in range(batch_size)],\n",
    "                                    [random.uniform(-4.,2.) for Z5 in range(batch_size)],\n",
    "                                    [random.uniform(-3.,3.2) for Z6 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.4) for Z7 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z8 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z9 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z10 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z11 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z12 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z13 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z14 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z15 in range(batch_size)]])\n",
    "\n",
    "        x = np.array([np.array([[i for i in range(sqr_grid_width)] for x in range(sqr_grid_width)]).flatten()\n",
    "                           for z in range(batch_size)])/float(sqr_grid_width)\n",
    "        y = np.array([np.array([[y for i in range(sqr_grid_width)] for y in range(sqr_grid_width)]).flatten()\n",
    "                           for z in range(batch_size)])/float(sqr_grid_width)\n",
    "\n",
    "\n",
    "        # z_grid to be used to create training data for dx and dy to create the gradient data\n",
    "        z_grid_train = np.array([np.array(opticspy.zernike.Coefficient(Z1=train_coeff[z][0],\n",
    "                            Z2=train_coeff[z][1], Z3=train_coeff[z][2], Z4=train_coeff[z][3], Z5=train_coeff[z][4],\n",
    "                            Z6=train_coeff[z][5], Z7=train_coeff[z][6], Z8=train_coeff[z][7], Z9=train_coeff[z][8],\n",
    "                            Z10=train_coeff[z][9], Z11=train_coeff[z][10], Z12=train_coeff[z][11], Z13=train_coeff[z][12],\n",
    "                            Z14=train_coeff[z][13], Z15=train_coeff[z][14]).zernikematrix(l=sqr_grid_width))\n",
    "                            for z in range(batch_size)])\n",
    "\n",
    "\n",
    "        gradient_train = [np.gradient(z_grid_train[i]) for i in range(batch_size)]\n",
    "\n",
    "        dx_train = np.array([np.array([x for x in gradient_train[i][0]]).flatten() for i in range(batch_size)])\n",
    "        dx_noise = add_noise(dx_train,batch_size=batch_size)\n",
    "\n",
    "        dy_train = np.array([np.array([y for y in gradient_train[i][1]]).flatten() for i in range(batch_size)])\n",
    "        dy_noise = add_noise(dy_train,batch_size=batch_size)\n",
    "\n",
    "        train_input = np.array([np.transpose([x[i], y[i], dx_noise[i], dy_noise[i]])\n",
    "                      for i in range(batch_size)])\n",
    "\n",
    "        yield train_input,train_coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BHee32fS-9MB"
   },
   "outputs": [],
   "source": [
    "# Create random matrix to add to dx and dy and block region\n",
    "def zernike_gen_blocknoise(batch_size=16,sqr_grid_width=50):\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        train_coeff = np.transpose([[random.uniform(-2.,3.) for Z1 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z2 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z3 in range(batch_size)],\n",
    "                                    [random.uniform(-9.,5.) for Z4 in range(batch_size)],\n",
    "                                    [random.uniform(-4.,2.) for Z5 in range(batch_size)],\n",
    "                                    [random.uniform(-3.,3.2) for Z6 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.4) for Z7 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z8 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z9 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z10 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z11 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z12 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z13 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z14 in range(batch_size)],\n",
    "                                    [random.uniform(-1.,1.) for Z15 in range(batch_size)]])\n",
    "\n",
    "        x = np.array([np.array([[i for i in range(sqr_grid_width)] for x in range(sqr_grid_width)]).flatten()\n",
    "                           for z in range(batch_size)])/float(sqr_grid_width)\n",
    "        y = np.array([np.array([[y for i in range(sqr_grid_width)] for y in range(sqr_grid_width)]).flatten()\n",
    "                           for z in range(batch_size)])/float(sqr_grid_width)\n",
    "\n",
    "\n",
    "        # z_grid to be used to create training data for dx and dy to create the gradient data\n",
    "        z_grid_train = np.array([np.array(opticspy.zernike.Coefficient(Z1=train_coeff[z][0],\n",
    "                            Z2=train_coeff[z][1], Z3=train_coeff[z][2], Z4=train_coeff[z][3], Z5=train_coeff[z][4],\n",
    "                            Z6=train_coeff[z][5], Z7=train_coeff[z][6], Z8=train_coeff[z][7], Z9=train_coeff[z][8],\n",
    "                            Z10=train_coeff[z][9], Z11=train_coeff[z][10], Z12=train_coeff[z][11], Z13=train_coeff[z][12],\n",
    "                            Z14=train_coeff[z][13], Z15=train_coeff[z][14]).zernikematrix(l=sqr_grid_width))\n",
    "                            for z in range(batch_size)])\n",
    "\n",
    "\n",
    "        gradient_train = [np.gradient(z_grid_train[i]) for i in range(batch_size)]\n",
    "\n",
    "#     Add noise\n",
    "        dx_train = np.array([np.array([x for x in gradient_train[i][0]]).flatten() for i in range(batch_size)])\n",
    "        dx_noise = add_noise(dx_train,batch_size=batch_size)\n",
    "\n",
    "        dy_train = np.array([np.array([y for y in gradient_train[i][1]]).flatten() for i in range(batch_size)])\n",
    "        dy_noise = add_noise(dy_train,batch_size=batch_size)\n",
    "\n",
    "        train_input = np.array([np.transpose([x[i], y[i], dx_noise[i], dy_noise[i]])\n",
    "                      for i in range(batch_size)])\n",
    "        \n",
    "#     NOTE: every sample in batch has the same block removed with this set up?\n",
    "#     Generate random block\n",
    "        x_block,y_block = block_gen(min_blk=2,max_blk=10,sqr_grid_width=sqr_grid_width)\n",
    "\n",
    "#     Remove random block\n",
    "        for i in range(batch_size):\n",
    "            train_input[i][(y_mask(train_input,i,y_block)) & (x_mask(train_input,i,x_block))] = 0\n",
    "\n",
    "        yield train_input,train_coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IXlD3SC7-9MC"
   },
   "outputs": [],
   "source": [
    "def train_model(test_plan,model,test_num,gen=True,save_epochs=False,sqr_grid_width=50):\n",
    "    \n",
    "    epochs = int(test_plan['epochs'][0])\n",
    "    steps_per_epoch = int(test_plan['steps_per_epoch'][0])\n",
    "    batch_size = int(test_plan['batch_size'][0])\n",
    "\n",
    "    #Display info for this test\n",
    "    print(\"SETTINGS FOR THIS TEST:\\n{} \\n\".format(test_plan))\n",
    "\n",
    "    #log the info\n",
    "    csv_logger = CSVLogger('Training.log')\n",
    "    callbacks = [csv_logger]\n",
    "    \n",
    "    if save_epochs==True:\n",
    "        checkpoint = ModelCheckpoint(filepath='epoch.{epoch:02d}.hdf5',\n",
    "                                     monitor='val_loss',save_best_only=True,verbose=0, period=1) #\n",
    "        callbacks.append(checkpoint)\n",
    "    \n",
    "    #Train the model \n",
    "    if gen == True:\n",
    "        zern_gen = zernike_gen(batch_size=batch_size,sqr_grid_width=sqr_grid_width)\n",
    "        val_gen = zernike_gen(batch_size=batch_size,sqr_grid_width=sqr_grid_width)\n",
    "\n",
    "        history = model.fit_generator(generator=zern_gen,steps_per_epoch=steps_per_epoch,\n",
    "                                      epochs=epochs, callbacks=callbacks, validation_data=val_gen,\n",
    "                                      validation_steps=round(steps_per_epoch*.1)) \n",
    "    else:\n",
    "        # Load data\n",
    "        print(\"Loading Training data\")\n",
    "        train_coeff,train_input,test_coeff,test_input = create_data(test_plan)\n",
    "        \n",
    "        history = model.fit(train_input, train_coeff,callbacks=callbacks,\n",
    "                    epochs=epochs, batch_size=batch_size,\n",
    "                     validation_data=(test_input, test_coeff))\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IXlD3SC7-9MC"
   },
   "outputs": [],
   "source": [
    "def train_model_noise(test_plan,model,test_num,gen=True,save_epochs=False,sqr_grid_width=50):\n",
    "    \n",
    "    epochs = int(test_plan['epochs'][0])\n",
    "    steps_per_epoch = int(test_plan['steps_per_epoch'][0])\n",
    "    batch_size = int(test_plan['batch_size'][0])\n",
    "    # max_queue_size = int(test_plan['max_queue_size'][0])\n",
    "\n",
    "    #Display info for this test\n",
    "    print(\"SETTINGS FOR THIS TEST:\\n{} \\n\".format(test_plan))\n",
    "\n",
    "    #log the info\n",
    "    csv_logger = CSVLogger('Training.log')\n",
    "    \n",
    "    callbacks = [csv_logger]\n",
    "    \n",
    "    if save_epochs==True:\n",
    "        checkpoint = ModelCheckpoint(filepath='epoch.{epoch:02d}.hdf5',\n",
    "                                     monitor='val_loss',save_best_only=True,verbose=0, period=1) #\n",
    "        callbacks.append(checkpoint)\n",
    "    \n",
    "    #Train the model \n",
    "    if gen == True:\n",
    "        zern_gen = zernike_gen_noise(batch_size=batch_size,sqr_grid_width=sqr_grid_width)\n",
    "        val_gen = zernike_gen_noise(batch_size=batch_size,sqr_grid_width=sqr_grid_width)\n",
    "\n",
    "        history = model.fit_generator(generator=zern_gen,steps_per_epoch=steps_per_epoch,\n",
    "                                      epochs=epochs, callbacks=callbacks, validation_data=val_gen,\n",
    "                                      validation_steps=round(steps_per_epoch*.1)) \n",
    "    else:\n",
    "        # Load data\n",
    "        print(\"Loading Training data\")\n",
    "        train_coeff,train_input,test_coeff,test_input = create_data(test_plan)\n",
    "        \n",
    "        history = model.fit(train_input, train_coeff,callbacks=callbacks,\n",
    "                    epochs=epochs, batch_size=batch_size,\n",
    "                     validation_data=(test_input, test_coeff))\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IXlD3SC7-9MC"
   },
   "outputs": [],
   "source": [
    "def train_model_block(test_plan,model,test_num,gen=True,save_epochs=False,sqr_grid_width=50):\n",
    "    \n",
    "    epochs = int(test_plan['epochs'][0])\n",
    "    steps_per_epoch = int(test_plan['steps_per_epoch'][0])\n",
    "    batch_size = int(test_plan['batch_size'][0])\n",
    "    # max_queue_size = int(test_plan['max_queue_size'][0])\n",
    "\n",
    "    #Display info for this test\n",
    "    print(\"SETTINGS FOR THIS TEST:\\n{} \\n\".format(test_plan))\n",
    "\n",
    "    #log the info\n",
    "    csv_logger = CSVLogger('Training.log')\n",
    "    \n",
    "    callbacks = [csv_logger]\n",
    "    \n",
    "    if save_epochs==True:\n",
    "        checkpoint = ModelCheckpoint(filepath='epoch.{epoch:02d}.hdf5',\n",
    "                                     monitor='val_loss',save_best_only=True,verbose=0, period=1) #\n",
    "        callbacks.append(checkpoint)\n",
    "    \n",
    "    #Train the model \n",
    "    if gen == True:\n",
    "        zern_gen = zernike_gen_block(batch_size=batch_size,sqr_grid_width=sqr_grid_width)\n",
    "        val_gen = zernike_gen_block(batch_size=batch_size,sqr_grid_width=sqr_grid_width)\n",
    "\n",
    "        history = model.fit_generator(generator=zern_gen,steps_per_epoch=steps_per_epoch,\n",
    "                                      epochs=epochs, callbacks=callbacks, validation_data=val_gen,\n",
    "                                      validation_steps=round(steps_per_epoch*.1)) \n",
    "    else:\n",
    "        # Load data\n",
    "        print(\"Loading Training data\")\n",
    "        train_coeff,train_input,test_coeff,test_input = create_data(test_plan)\n",
    "        \n",
    "        history = model.fit(train_input, train_coeff,callbacks=callbacks,\n",
    "                    epochs=epochs, batch_size=batch_size,\n",
    "                     validation_data=(test_input, test_coeff))\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IXlD3SC7-9MC"
   },
   "outputs": [],
   "source": [
    "def train_model_blocknoise(test_plan,model,test_num,gen=True,save_epochs=False,sqr_grid_width=50):\n",
    "    \n",
    "    epochs = int(test_plan['epochs'][0])\n",
    "    steps_per_epoch = int(test_plan['steps_per_epoch'][0])\n",
    "    batch_size = int(test_plan['batch_size'][0])\n",
    "    # max_queue_size = int(test_plan['max_queue_size'][0])\n",
    "\n",
    "    #Display info for this test\n",
    "    print(\"SETTINGS FOR THIS TEST:\\n{} \\n\".format(test_plan))\n",
    "\n",
    "    #log the info\n",
    "    csv_logger = CSVLogger('Training.log')\n",
    "    \n",
    "    callbacks = [csv_logger]\n",
    "    \n",
    "    if save_epochs==True:\n",
    "        checkpoint = ModelCheckpoint(filepath='epoch.{epoch:02d}.hdf5',\n",
    "                                     monitor='val_loss',save_best_only=True,verbose=0, period=1) #\n",
    "        callbacks.append(checkpoint)\n",
    "    \n",
    "    #Train the model \n",
    "    if gen == True:\n",
    "        zern_gen = zernike_gen_blocknoise(batch_size=batch_size,sqr_grid_width=sqr_grid_width)\n",
    "        val_gen = zernike_gen_blocknoise(batch_size=batch_size,sqr_grid_width=sqr_grid_width)\n",
    "\n",
    "        history = model.fit_generator(generator=zern_gen,steps_per_epoch=steps_per_epoch,\n",
    "                                      epochs=epochs, callbacks=callbacks, validation_data=val_gen,\n",
    "                                      validation_steps=round(steps_per_epoch*.1)) \n",
    "    else:\n",
    "        # Load data\n",
    "        print(\"Loading Training data\")\n",
    "        train_coeff,train_input,test_coeff,test_input = create_data(test_plan)\n",
    "        \n",
    "        history = model.fit(train_input, train_coeff,callbacks=callbacks,\n",
    "                    epochs=epochs, batch_size=batch_size,\n",
    "                     validation_data=(test_input, test_coeff))\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A-CHgZfV-9MD"
   },
   "outputs": [],
   "source": [
    "def export_figs(history,testing):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history.history['mean_absolute_error'])\n",
    "    plt.plot(history.history['val_mean_absolute_error'])\n",
    "    plt.title('Model MAE - Test {}'.format(testing))\n",
    "    plt.ylabel('MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.savefig('MAE_Plot_Test_{}.png'.format(testing), dpi=300, bbox_inches='tight')\n",
    "    plt.clf()\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss - Test {}'.format(testing))\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.savefig('Loss_Plot_Test_{}.png'.format(testing), dpi=300, bbox_inches='tight')\n",
    "    plt.clf()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s2KZPk5l-9MF"
   },
   "outputs": [],
   "source": [
    "def export_settings(test_plan,testing,history): \n",
    "    test_plan['best_val_mae'] = \"\"\n",
    "    test_plan.loc[0,'best_val_mae'] = np.min(history.history['mean_absolute_error'])\n",
    "    test_plan\n",
    "    test_plan.to_csv('Test_{}_Settings.csv'.format(testing))\n",
    "    \n",
    "    \n",
    "def export_test_data(model,test_num):\n",
    "    model.save('model_test_{}.h5'.format(test_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "04oE5SlO-9MO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_results(start,stop,parent_dir,sqr_grid_width=50,gen=True):\n",
    "    for i in range(start,stop+1):\n",
    "        print(\"\\nBEGINNING TEST: {}\".format(i))\n",
    "        test_plan = extract_test_plan(test_num=i,file_path=parent_dir+'test_plan_generator.csv')\n",
    "\n",
    "        print(\"Creating storage directory\")\n",
    "        create_dir(i,parent_dir,linux=False)\n",
    "        \n",
    "        print(\"Creating the model\")\n",
    "        model = model_create(test_plan,sqr_grid_width=sqr_grid_width)\n",
    "        \n",
    "        print(model.summary())\n",
    "        \n",
    "        print(\"Fitting model\")\n",
    "        history = train_model(test_plan,model,i,gen=gen,save_epochs=True,\n",
    "                              sqr_grid_width=sqr_grid_width)\n",
    "        \n",
    "        print(\"Exporting plots\")\n",
    "        export_figs(history,i)\n",
    "\n",
    "        print(\"Exporting CSV with test parameters and best mean absolute error result\")\n",
    "        export_settings(test_plan,i,history)\n",
    "        \n",
    "        print('Exporting Test Data')\n",
    "        export_test_data(model,test_num=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "04oE5SlO-9MO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Added the load model option. Need to test.\n",
    "def get_results_block(start,stop,parent_dir,sqr_grid_width=50,gen=True,load=False,load_name_append='',name_append=''):\n",
    "    for i in range(start,stop+1):\n",
    "        print(\"\\nBEGINNING TEST: {}\".format(i))\n",
    "        test_plan = extract_test_plan(test_num=i,file_path=parent_dir+'test_plan_generator.csv')\n",
    "\n",
    "        print(\"Creating storage directory\")\n",
    "        create_dir(i,parent_dir,linux=False,name_append=name_append)\n",
    "        \n",
    "        if load == False:\n",
    "            print(\"Creating the model\")\n",
    "            model = model_create(test_plan,sqr_grid_width=sqr_grid_width)\n",
    "        else:\n",
    "            model = load_model('.\\\\Test_{}{}\\\\model_test_{}.h5'.format(i,load_name_append,i))\n",
    "        \n",
    "        print(model.summary())\n",
    "        \n",
    "        print(\"Fitting model\")\n",
    "        history = train_model_block(test_plan,model,i,gen=gen,save_epochs=True,\n",
    "                              sqr_grid_width=sqr_grid_width)\n",
    "        \n",
    "        print(\"Exporting plots\")\n",
    "        export_figs(history,i)\n",
    "\n",
    "        print(\"Exporting CSV with test parameters and best mean absolute error result\")\n",
    "        export_settings(test_plan,i,history)\n",
    "        \n",
    "        print('Exporting Test Data')\n",
    "        export_test_data(model,test_num=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "04oE5SlO-9MO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_results_noise(start,stop,parent_dir,sqr_grid_width=50,gen=True,load=False,load_name_append='',name_append=''):\n",
    "    for i in range(start,stop+1):\n",
    "        print(\"\\nBEGINNING TEST: {}\".format(i))\n",
    "        test_plan = extract_test_plan(test_num=i,file_path=parent_dir+'test_plan_generator.csv')\n",
    "\n",
    "        print(\"Creating storage directory\")\n",
    "        create_dir(i,parent_dir,linux=False,name_append=name_append)\n",
    "        \n",
    "        if load == False:\n",
    "            print(\"Creating the model\")\n",
    "            model = model_create(test_plan,sqr_grid_width=sqr_grid_width)\n",
    "        else:\n",
    "            model = load_model('.\\\\Test_{}{}\\\\model_test_{}.h5'.format(i,load_name_append,i))\n",
    "        \n",
    "        print(model.summary())\n",
    "        \n",
    "        print(\"Fitting model\")\n",
    "        history = train_model_noise(test_plan,model,i,gen=gen,save_epochs=True,\n",
    "                              sqr_grid_width=sqr_grid_width)\n",
    "        \n",
    "        print(\"Exporting plots\")\n",
    "        export_figs(history,i)\n",
    "\n",
    "        print(\"Exporting CSV with test parameters and best mean absolute error result\")\n",
    "        export_settings(test_plan,i,history)\n",
    "        \n",
    "        print('Exporting Test Data')\n",
    "        export_test_data(model,test_num=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "04oE5SlO-9MO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_results_blocknoise(start,stop,parent_dir,sqr_grid_width=50,gen=True,load=False,\n",
    "                           load_name_append='',name_append=''):\n",
    "    for i in range(start,stop+1):\n",
    "        print(\"\\nBEGINNING TEST: {}\".format(i))\n",
    "        test_plan = extract_test_plan(test_num=i,file_path=parent_dir+'test_plan_generator.csv')\n",
    "\n",
    "        print(\"Creating storage directory\")\n",
    "        create_dir(i,parent_dir,linux=False,name_append=name_append)\n",
    "        \n",
    "        if load == False:\n",
    "            print(\"Creating the model\")\n",
    "            model = model_create(test_plan,sqr_grid_width=sqr_grid_width)\n",
    "        else:\n",
    "            model = models.load_model('..\\\\Test_{}{}\\\\model_test_{}.h5'.format(i,load_name_append,i))\n",
    "        \n",
    "        print(model.summary())\n",
    "        \n",
    "        print(\"Fitting model\")\n",
    "        history = train_model_blocknoise(test_plan,model,i,gen=gen,save_epochs=True,\n",
    "                              sqr_grid_width=sqr_grid_width)\n",
    "        \n",
    "        print(\"Exporting plots\")\n",
    "        export_figs(history,i)\n",
    "\n",
    "        print(\"Exporting CSV with test parameters and best mean absolute error result\")\n",
    "        export_settings(test_plan,i,history)\n",
    "        \n",
    "        print('Exporting Test Data')\n",
    "        export_test_data(model,test_num=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "p77o91vL-9MP",
    "outputId": "f600a8be-bc21-4439-e876-20fae38d0834"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BEGINNING TEST: 1\n",
      "C:\\Users\\aambr\\OneDrive\\Documents\\UNH Fall 2019\\Research\\zernike_polynomials\\Data_and_Results_12-12-19\\test_plan_generator.csv\n",
      "Creating storage directory\n",
      "Creating the model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_45 (Dense)             (None, 2500, 32)          160       \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 80000)             0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 15)                1200015   \n",
      "=================================================================\n",
      "Total params: 1,202,287\n",
      "Trainable params: 1,202,287\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fitting model\n",
      "SETTINGS FOR THIS TEST:\n",
      "   test_num  layer_num layer_type  num_nodes activation optimizer loss  \\\n",
      "0         1          1      Dense       32.0       relu   RMSprop  mse   \n",
      "1         1          2      Dense       32.0       relu       NaN  NaN   \n",
      "2         1          3      Dense       32.0       relu       NaN  NaN   \n",
      "3         1          4    Flatten        NaN        NaN       NaN  NaN   \n",
      "4         1          5      Dense       15.0        NaN       NaN  NaN   \n",
      "\n",
      "   epochs  batch_size  steps_per_epoch       lr  max_queue_size  \\\n",
      "0    30.0         8.0           3200.0  0.00025           320.0   \n",
      "1     NaN         NaN              NaN      NaN             NaN   \n",
      "2     NaN         NaN              NaN      NaN             NaN   \n",
      "3     NaN         NaN              NaN      NaN             NaN   \n",
      "4     NaN         NaN              NaN      NaN             NaN   \n",
      "\n",
      "   kernel_regularizer  train_samp_num  test_samp_num  \n",
      "0                 0.0         20000.0         2000.0  \n",
      "1                 0.0             NaN            NaN  \n",
      "2                 0.0             NaN            NaN  \n",
      "3                 0.0             NaN            NaN  \n",
      "4                 0.0             NaN            NaN   \n",
      "\n",
      "Epoch 1/30\n",
      "3200/3200 [==============================] - 147s 46ms/step - loss: 0.0402 - mean_absolute_error: 0.1027 - val_loss: 0.0248 - val_mean_absolute_error: 0.0731\n",
      "Epoch 2/30\n",
      "3200/3200 [==============================] - 142s 45ms/step - loss: 0.0258 - mean_absolute_error: 0.0708 - val_loss: 0.0240 - val_mean_absolute_error: 0.0643\n",
      "Epoch 3/30\n",
      "3200/3200 [==============================] - 144s 45ms/step - loss: 0.0253 - mean_absolute_error: 0.0667 - val_loss: 0.0244 - val_mean_absolute_error: 0.0644\n",
      "Epoch 4/30\n",
      "3200/3200 [==============================] - 143s 45ms/step - loss: 0.0250 - mean_absolute_error: 0.0643 - val_loss: 0.0250 - val_mean_absolute_error: 0.0650\n",
      "Epoch 5/30\n",
      "3200/3200 [==============================] - 141s 44ms/step - loss: 0.0245 - mean_absolute_error: 0.0625 - val_loss: 0.0240 - val_mean_absolute_error: 0.0578\n",
      "Epoch 6/30\n",
      "3200/3200 [==============================] - 142s 44ms/step - loss: 0.0243 - mean_absolute_error: 0.0612 - val_loss: 0.0246 - val_mean_absolute_error: 0.0616\n",
      "Epoch 7/30\n",
      "3200/3200 [==============================] - 143s 45ms/step - loss: 0.0246 - mean_absolute_error: 0.0606 - val_loss: 0.0239 - val_mean_absolute_error: 0.0590\n",
      "Epoch 8/30\n",
      "3200/3200 [==============================] - 143s 45ms/step - loss: 0.0242 - mean_absolute_error: 0.0596 - val_loss: 0.0233 - val_mean_absolute_error: 0.0558\n",
      "Epoch 9/30\n",
      "3200/3200 [==============================] - 143s 45ms/step - loss: 0.0243 - mean_absolute_error: 0.0590 - val_loss: 0.0236 - val_mean_absolute_error: 0.0593\n",
      "Epoch 10/30\n",
      "3200/3200 [==============================] - 141s 44ms/step - loss: 0.0239 - mean_absolute_error: 0.0578 - val_loss: 0.0250 - val_mean_absolute_error: 0.0646\n",
      "Epoch 11/30\n",
      "3200/3200 [==============================] - 141s 44ms/step - loss: 0.0240 - mean_absolute_error: 0.0576 - val_loss: 0.0238 - val_mean_absolute_error: 0.0535\n",
      "Epoch 12/30\n",
      "3200/3200 [==============================] - 141s 44ms/step - loss: 0.0239 - mean_absolute_error: 0.0570 - val_loss: 0.0235 - val_mean_absolute_error: 0.0540\n",
      "Epoch 13/30\n",
      "3200/3200 [==============================] - 141s 44ms/step - loss: 0.0239 - mean_absolute_error: 0.0566 - val_loss: 0.0265 - val_mean_absolute_error: 0.0677\n",
      "Epoch 14/30\n",
      "3200/3200 [==============================] - 139s 44ms/step - loss: 0.0237 - mean_absolute_error: 0.0560 - val_loss: 0.0234 - val_mean_absolute_error: 0.0517\n",
      "Epoch 15/30\n",
      "3200/3200 [==============================] - 139s 44ms/step - loss: 0.0237 - mean_absolute_error: 0.0557 - val_loss: 0.0244 - val_mean_absolute_error: 0.0585\n",
      "Epoch 16/30\n",
      "3200/3200 [==============================] - 182s 57ms/step - loss: 0.0239 - mean_absolute_error: 0.0556 - val_loss: 0.0244 - val_mean_absolute_error: 0.0552\n",
      "Epoch 17/30\n",
      "3200/3200 [==============================] - 143s 45ms/step - loss: 0.0238 - mean_absolute_error: 0.0551 - val_loss: 0.0243 - val_mean_absolute_error: 0.0604\n",
      "Epoch 18/30\n",
      "3200/3200 [==============================] - 142s 44ms/step - loss: 0.0236 - mean_absolute_error: 0.0547 - val_loss: 0.0234 - val_mean_absolute_error: 0.0516\n",
      "Epoch 19/30\n",
      "3200/3200 [==============================] - 140s 44ms/step - loss: 0.0235 - mean_absolute_error: 0.0543 - val_loss: 0.0232 - val_mean_absolute_error: 0.0521\n",
      "Epoch 20/30\n",
      "3200/3200 [==============================] - 141s 44ms/step - loss: 0.0237 - mean_absolute_error: 0.0543 - val_loss: 0.0234 - val_mean_absolute_error: 0.0529\n",
      "Epoch 21/30\n",
      "3200/3200 [==============================] - 140s 44ms/step - loss: 0.0236 - mean_absolute_error: 0.0542 - val_loss: 0.0251 - val_mean_absolute_error: 0.0573\n",
      "Epoch 22/30\n",
      "3200/3200 [==============================] - 143s 45ms/step - loss: 0.0234 - mean_absolute_error: 0.0537 - val_loss: 0.0233 - val_mean_absolute_error: 0.0533\n",
      "Epoch 23/30\n",
      "3200/3200 [==============================] - 139s 43ms/step - loss: 0.0234 - mean_absolute_error: 0.0536 - val_loss: 0.0231 - val_mean_absolute_error: 0.0500\n",
      "Epoch 24/30\n",
      "3200/3200 [==============================] - 184s 58ms/step - loss: 0.0235 - mean_absolute_error: 0.0535 - val_loss: 0.0233 - val_mean_absolute_error: 0.0519\n",
      "Epoch 25/30\n",
      "3200/3200 [==============================] - 147s 46ms/step - loss: 0.0233 - mean_absolute_error: 0.0530 - val_loss: 0.0238 - val_mean_absolute_error: 0.0533\n",
      "Epoch 26/30\n",
      "3200/3200 [==============================] - 151s 47ms/step - loss: 0.0235 - mean_absolute_error: 0.0530 - val_loss: 0.0230 - val_mean_absolute_error: 0.0508\n",
      "Epoch 27/30\n",
      "3200/3200 [==============================] - 143s 45ms/step - loss: 0.0232 - mean_absolute_error: 0.0528 - val_loss: 0.0228 - val_mean_absolute_error: 0.0532\n",
      "Epoch 28/30\n",
      "3200/3200 [==============================] - 141s 44ms/step - loss: 0.0234 - mean_absolute_error: 0.0526 - val_loss: 0.0231 - val_mean_absolute_error: 0.0525\n",
      "Epoch 29/30\n",
      "3200/3200 [==============================] - 140s 44ms/step - loss: 0.0236 - mean_absolute_error: 0.0526 - val_loss: 0.0232 - val_mean_absolute_error: 0.0527\n",
      "Epoch 30/30\n",
      "3200/3200 [==============================] - 140s 44ms/step - loss: 0.0231 - mean_absolute_error: 0.0520 - val_loss: 0.0227 - val_mean_absolute_error: 0.0525\n",
      "Exporting plots\n",
      "Exporting CSV with test parameters and best mean absolute error result\n",
      "Exporting Test Data\n",
      "\n",
      "BEGINNING TEST: 2\n",
      "C:\\Users\\aambr\\OneDrive\\Documents\\UNH Fall 2019\\Research\\zernike_polynomials\\Data_and_Results_12-12-19\\test_plan_generator.csv\n",
      "Creating storage directory\n",
      "Creating the model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_49 (Dense)             (None, 2500, 32)          160       \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 80000)             0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 15)                1200015   \n",
      "=================================================================\n",
      "Total params: 1,202,287\n",
      "Trainable params: 1,202,287\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fitting model\n",
      "SETTINGS FOR THIS TEST:\n",
      "   test_num  layer_num layer_type  num_nodes activation optimizer loss  \\\n",
      "0         2          1      Dense       32.0       relu      Adam  mse   \n",
      "1         2          2      Dense       32.0       relu       NaN  NaN   \n",
      "2         2          3      Dense       32.0       relu       NaN  NaN   \n",
      "3         2          4    Flatten        NaN        NaN       NaN  NaN   \n",
      "4         2          5      Dense       15.0        NaN       NaN  NaN   \n",
      "\n",
      "   epochs  batch_size  steps_per_epoch       lr  max_queue_size  \\\n",
      "0    30.0         8.0           3200.0  0.00025           320.0   \n",
      "1     NaN         NaN              NaN      NaN             NaN   \n",
      "2     NaN         NaN              NaN      NaN             NaN   \n",
      "3     NaN         NaN              NaN      NaN             NaN   \n",
      "4     NaN         NaN              NaN      NaN             NaN   \n",
      "\n",
      "   kernel_regularizer  train_samp_num  test_samp_num  \n",
      "0                 0.0         20000.0         2000.0  \n",
      "1                 0.0             NaN            NaN  \n",
      "2                 0.0             NaN            NaN  \n",
      "3                 0.0             NaN            NaN  \n",
      "4                 0.0             NaN            NaN   \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "3200/3200 [==============================] - 150s 47ms/step - loss: 0.0303 - mean_absolute_error: 0.0709 - val_loss: 0.0247 - val_mean_absolute_error: 0.0574\n",
      "Epoch 2/30\n",
      "3200/3200 [==============================] - 148s 46ms/step - loss: 0.0242 - mean_absolute_error: 0.0563 - val_loss: 0.0229 - val_mean_absolute_error: 0.0562\n",
      "Epoch 3/30\n",
      "3200/3200 [==============================] - 149s 47ms/step - loss: 0.0238 - mean_absolute_error: 0.0544 - val_loss: 0.0234 - val_mean_absolute_error: 0.0542\n",
      "Epoch 4/30\n",
      "3200/3200 [==============================] - 148s 46ms/step - loss: 0.0233 - mean_absolute_error: 0.0530 - val_loss: 0.0234 - val_mean_absolute_error: 0.0505\n",
      "Epoch 5/30\n",
      "3200/3200 [==============================] - 149s 47ms/step - loss: 0.0234 - mean_absolute_error: 0.0524 - val_loss: 0.0231 - val_mean_absolute_error: 0.0511\n",
      "Epoch 6/30\n",
      "3200/3200 [==============================] - 149s 47ms/step - loss: 0.0232 - mean_absolute_error: 0.0512 - val_loss: 0.0229 - val_mean_absolute_error: 0.0492\n",
      "Epoch 7/30\n",
      "3200/3200 [==============================] - 154s 48ms/step - loss: 0.0233 - mean_absolute_error: 0.0511 - val_loss: 0.0230 - val_mean_absolute_error: 0.0505\n",
      "Epoch 8/30\n",
      "3200/3200 [==============================] - 147s 46ms/step - loss: 0.0232 - mean_absolute_error: 0.0508 - val_loss: 0.0233 - val_mean_absolute_error: 0.0502\n",
      "Epoch 9/30\n",
      "3200/3200 [==============================] - 150s 47ms/step - loss: 0.0230 - mean_absolute_error: 0.0501 - val_loss: 0.0227 - val_mean_absolute_error: 0.0478\n",
      "Epoch 10/30\n",
      "3200/3200 [==============================] - 149s 47ms/step - loss: 0.0229 - mean_absolute_error: 0.0495 - val_loss: 0.0226 - val_mean_absolute_error: 0.0471\n",
      "Epoch 11/30\n",
      "3200/3200 [==============================] - 150s 47ms/step - loss: 0.0231 - mean_absolute_error: 0.0494 - val_loss: 0.0232 - val_mean_absolute_error: 0.0494\n",
      "Epoch 12/30\n",
      "3200/3200 [==============================] - 149s 46ms/step - loss: 0.0230 - mean_absolute_error: 0.0487 - val_loss: 0.0228 - val_mean_absolute_error: 0.0492\n",
      "Epoch 13/30\n",
      "3200/3200 [==============================] - 151s 47ms/step - loss: 0.0228 - mean_absolute_error: 0.0484 - val_loss: 0.0224 - val_mean_absolute_error: 0.0478\n",
      "Epoch 14/30\n",
      "3200/3200 [==============================] - 149s 47ms/step - loss: 0.0229 - mean_absolute_error: 0.0484 - val_loss: 0.0241 - val_mean_absolute_error: 0.0525\n",
      "Epoch 15/30\n",
      "3200/3200 [==============================] - 150s 47ms/step - loss: 0.0229 - mean_absolute_error: 0.0479 - val_loss: 0.0231 - val_mean_absolute_error: 0.0490\n",
      "Epoch 16/30\n",
      "3200/3200 [==============================] - 149s 46ms/step - loss: 0.0227 - mean_absolute_error: 0.0474 - val_loss: 0.0220 - val_mean_absolute_error: 0.0461\n",
      "Epoch 17/30\n",
      "3200/3200 [==============================] - 148s 46ms/step - loss: 0.0228 - mean_absolute_error: 0.0472 - val_loss: 0.0226 - val_mean_absolute_error: 0.0458\n",
      "Epoch 18/30\n",
      "3200/3200 [==============================] - 150s 47ms/step - loss: 0.0228 - mean_absolute_error: 0.0471 - val_loss: 0.0231 - val_mean_absolute_error: 0.0492\n",
      "Epoch 19/30\n",
      "3200/3200 [==============================] - 150s 47ms/step - loss: 0.0230 - mean_absolute_error: 0.0470 - val_loss: 0.0230 - val_mean_absolute_error: 0.0504\n",
      "Epoch 20/30\n",
      "3200/3200 [==============================] - 150s 47ms/step - loss: 0.0227 - mean_absolute_error: 0.0465 - val_loss: 0.0227 - val_mean_absolute_error: 0.0456\n",
      "Epoch 21/30\n",
      "3200/3200 [==============================] - 149s 47ms/step - loss: 0.0227 - mean_absolute_error: 0.0463 - val_loss: 0.0229 - val_mean_absolute_error: 0.0484\n",
      "Epoch 22/30\n",
      "3200/3200 [==============================] - 148s 46ms/step - loss: 0.0227 - mean_absolute_error: 0.0459 - val_loss: 0.0231 - val_mean_absolute_error: 0.0452\n",
      "Epoch 23/30\n",
      "3200/3200 [==============================] - 147s 46ms/step - loss: 0.0227 - mean_absolute_error: 0.0456 - val_loss: 0.0229 - val_mean_absolute_error: 0.0453\n",
      "Epoch 24/30\n",
      "3200/3200 [==============================] - 147s 46ms/step - loss: 0.0226 - mean_absolute_error: 0.0455 - val_loss: 0.0236 - val_mean_absolute_error: 0.0445\n",
      "Epoch 25/30\n",
      "3200/3200 [==============================] - 147s 46ms/step - loss: 0.0225 - mean_absolute_error: 0.0446 - val_loss: 0.0227 - val_mean_absolute_error: 0.0437\n",
      "Epoch 26/30\n",
      "3200/3200 [==============================] - 148s 46ms/step - loss: 0.0225 - mean_absolute_error: 0.0444 - val_loss: 0.0228 - val_mean_absolute_error: 0.0453\n",
      "Epoch 27/30\n",
      "3200/3200 [==============================] - 147s 46ms/step - loss: 0.0226 - mean_absolute_error: 0.0442 - val_loss: 0.0220 - val_mean_absolute_error: 0.0436\n",
      "Epoch 28/30\n",
      "3200/3200 [==============================] - 147s 46ms/step - loss: 0.0225 - mean_absolute_error: 0.0437 - val_loss: 0.0224 - val_mean_absolute_error: 0.0432\n",
      "Epoch 29/30\n",
      "3200/3200 [==============================] - 148s 46ms/step - loss: 0.0225 - mean_absolute_error: 0.0437 - val_loss: 0.0231 - val_mean_absolute_error: 0.0434\n",
      "Epoch 30/30\n",
      "3200/3200 [==============================] - 146s 45ms/step - loss: 0.0227 - mean_absolute_error: 0.0435 - val_loss: 0.0229 - val_mean_absolute_error: 0.0425\n",
      "Exporting plots\n",
      "Exporting CSV with test parameters and best mean absolute error result\n",
      "Exporting Test Data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using zernike_gen_ones -- zernike coefficients only ranged from -1 to 1\n",
    "get_results(1,2,parent_dir,sqr_grid_width=50,gen=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BEGINNING TEST: 1\n",
      "C:\\Users\\aambr\\OneDrive\\Documents\\UNH Fall 2019\\Research\\zernike_polynomials\\Data_and_Results_12-12-19\\test_plan_generator.csv\n",
      "Creating storage directory\n",
      "Creating the model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_53 (Dense)             (None, 2500, 32)          160       \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 80000)             0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 15)                1200015   \n",
      "=================================================================\n",
      "Total params: 1,202,287\n",
      "Trainable params: 1,202,287\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fitting model\n",
      "SETTINGS FOR THIS TEST:\n",
      "   test_num  layer_num layer_type  num_nodes activation optimizer loss  \\\n",
      "0         1          1      Dense       32.0       relu   RMSprop  mse   \n",
      "1         1          2      Dense       32.0       relu       NaN  NaN   \n",
      "2         1          3      Dense       32.0       relu       NaN  NaN   \n",
      "3         1          4    Flatten        NaN        NaN       NaN  NaN   \n",
      "4         1          5      Dense       15.0        NaN       NaN  NaN   \n",
      "\n",
      "   epochs  batch_size  steps_per_epoch     lr  max_queue_size  \\\n",
      "0    30.0         8.0           3200.0  0.001           320.0   \n",
      "1     NaN         NaN              NaN    NaN             NaN   \n",
      "2     NaN         NaN              NaN    NaN             NaN   \n",
      "3     NaN         NaN              NaN    NaN             NaN   \n",
      "4     NaN         NaN              NaN    NaN             NaN   \n",
      "\n",
      "   kernel_regularizer  train_samp_num  test_samp_num  \n",
      "0                 0.0         20000.0         2000.0  \n",
      "1                 0.0             NaN            NaN  \n",
      "2                 0.0             NaN            NaN  \n",
      "3                 0.0             NaN            NaN  \n",
      "4                 0.0             NaN            NaN   \n",
      "\n",
      "Epoch 1/30\n",
      "3200/3200 [==============================] - 150s 47ms/step - loss: 0.3004 - mean_absolute_error: 0.2515 - val_loss: 0.1655 - val_mean_absolute_error: 0.1919\n",
      "Epoch 2/30\n",
      "3200/3200 [==============================] - 150s 47ms/step - loss: 0.1708 - mean_absolute_error: 0.1889 - val_loss: 0.1687 - val_mean_absolute_error: 0.1870\n",
      "Epoch 3/30\n",
      "3200/3200 [==============================] - 150s 47ms/step - loss: 0.1657 - mean_absolute_error: 0.1775 - val_loss: 0.1546 - val_mean_absolute_error: 0.1616\n",
      "Epoch 4/30\n",
      "3200/3200 [==============================] - 146s 46ms/step - loss: 0.1625 - mean_absolute_error: 0.1709 - val_loss: 0.1668 - val_mean_absolute_error: 0.1820\n",
      "Epoch 5/30\n",
      "3200/3200 [==============================] - 145s 45ms/step - loss: 0.1606 - mean_absolute_error: 0.1667 - val_loss: 0.1646 - val_mean_absolute_error: 0.1629\n",
      "Epoch 6/30\n",
      "3200/3200 [==============================] - 141s 44ms/step - loss: 0.1600 - mean_absolute_error: 0.1630 - val_loss: 0.1602 - val_mean_absolute_error: 0.1597\n",
      "Epoch 7/30\n",
      "3200/3200 [==============================] - 140s 44ms/step - loss: 0.1575 - mean_absolute_error: 0.1598 - val_loss: 0.1486 - val_mean_absolute_error: 0.1382\n",
      "Epoch 8/30\n",
      "3200/3200 [==============================] - 143s 45ms/step - loss: 0.1565 - mean_absolute_error: 0.1574 - val_loss: 0.1561 - val_mean_absolute_error: 0.1611\n",
      "Epoch 9/30\n",
      "3200/3200 [==============================] - 142s 44ms/step - loss: 0.1557 - mean_absolute_error: 0.1552 - val_loss: 0.1566 - val_mean_absolute_error: 0.1699\n",
      "Epoch 10/30\n",
      "3200/3200 [==============================] - 142s 44ms/step - loss: 0.1544 - mean_absolute_error: 0.1535 - val_loss: 0.1467 - val_mean_absolute_error: 0.1431\n",
      "Epoch 11/30\n",
      "3200/3200 [==============================] - 143s 45ms/step - loss: 0.1534 - mean_absolute_error: 0.1522 - val_loss: 0.1536 - val_mean_absolute_error: 0.1504\n",
      "Epoch 12/30\n",
      "3200/3200 [==============================] - 155s 49ms/step - loss: 0.1526 - mean_absolute_error: 0.1502 - val_loss: 0.1535 - val_mean_absolute_error: 0.1396\n",
      "Epoch 13/30\n",
      "3200/3200 [==============================] - 147s 46ms/step - loss: 0.1519 - mean_absolute_error: 0.1487 - val_loss: 0.1533 - val_mean_absolute_error: 0.1521\n",
      "Epoch 14/30\n",
      "3200/3200 [==============================] - 144s 45ms/step - loss: 0.1500 - mean_absolute_error: 0.1464 - val_loss: 0.1475 - val_mean_absolute_error: 0.1313\n",
      "Epoch 15/30\n",
      "3200/3200 [==============================] - 145s 45ms/step - loss: 0.1504 - mean_absolute_error: 0.1455 - val_loss: 0.1499 - val_mean_absolute_error: 0.1384\n",
      "Epoch 16/30\n",
      "3200/3200 [==============================] - 146s 45ms/step - loss: 0.1499 - mean_absolute_error: 0.1441 - val_loss: 0.1493 - val_mean_absolute_error: 0.1512\n",
      "Epoch 17/30\n",
      "3200/3200 [==============================] - 148s 46ms/step - loss: 0.1483 - mean_absolute_error: 0.1423 - val_loss: 0.1515 - val_mean_absolute_error: 0.1661\n",
      "Epoch 18/30\n",
      "3200/3200 [==============================] - 147s 46ms/step - loss: 0.1483 - mean_absolute_error: 0.1410 - val_loss: 0.1473 - val_mean_absolute_error: 0.1503\n",
      "Epoch 19/30\n",
      "3200/3200 [==============================] - 152s 47ms/step - loss: 0.1486 - mean_absolute_error: 0.1404 - val_loss: 0.1475 - val_mean_absolute_error: 0.1344\n",
      "Epoch 20/30\n",
      "3200/3200 [==============================] - 155s 48ms/step - loss: 0.1478 - mean_absolute_error: 0.1394 - val_loss: 0.1483 - val_mean_absolute_error: 0.1552\n",
      "Epoch 21/30\n",
      "3200/3200 [==============================] - 148s 46ms/step - loss: 0.1476 - mean_absolute_error: 0.1382 - val_loss: 0.1533 - val_mean_absolute_error: 0.1461\n",
      "Epoch 22/30\n",
      "3200/3200 [==============================] - 152s 47ms/step - loss: 0.1491 - mean_absolute_error: 0.1384 - val_loss: 0.1554 - val_mean_absolute_error: 0.1487\n",
      "Epoch 23/30\n",
      "3200/3200 [==============================] - 141s 44ms/step - loss: 0.1484 - mean_absolute_error: 0.1372 - val_loss: 0.1466 - val_mean_absolute_error: 0.1471\n",
      "Epoch 24/30\n",
      "3200/3200 [==============================] - 143s 45ms/step - loss: 0.1473 - mean_absolute_error: 0.1360 - val_loss: 0.1456 - val_mean_absolute_error: 0.1355\n",
      "Epoch 25/30\n",
      "3200/3200 [==============================] - 144s 45ms/step - loss: 0.1478 - mean_absolute_error: 0.1360 - val_loss: 0.1477 - val_mean_absolute_error: 0.1387\n",
      "Epoch 26/30\n",
      "3200/3200 [==============================] - 142s 44ms/step - loss: 0.1471 - mean_absolute_error: 0.1355 - val_loss: 0.1469 - val_mean_absolute_error: 0.1335\n",
      "Epoch 27/30\n",
      "3200/3200 [==============================] - 142s 45ms/step - loss: 0.1458 - mean_absolute_error: 0.1342 - val_loss: 0.1427 - val_mean_absolute_error: 0.1246\n",
      "Epoch 28/30\n",
      "3200/3200 [==============================] - 142s 44ms/step - loss: 0.1458 - mean_absolute_error: 0.1338 - val_loss: 0.1480 - val_mean_absolute_error: 0.1494\n",
      "Epoch 29/30\n",
      "3200/3200 [==============================] - 141s 44ms/step - loss: 0.1478 - mean_absolute_error: 0.1339 - val_loss: 0.1455 - val_mean_absolute_error: 0.1302\n",
      "Epoch 30/30\n",
      "3200/3200 [==============================] - 143s 45ms/step - loss: 0.1462 - mean_absolute_error: 0.1325 - val_loss: 0.1429 - val_mean_absolute_error: 0.1185\n",
      "Exporting plots\n",
      "Exporting CSV with test parameters and best mean absolute error result\n",
      "Exporting Test Data\n",
      "\n",
      "BEGINNING TEST: 2\n",
      "C:\\Users\\aambr\\OneDrive\\Documents\\UNH Fall 2019\\Research\\zernike_polynomials\\Data_and_Results_12-12-19\\test_plan_generator.csv\n",
      "Creating storage directory\n",
      "Creating the model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_57 (Dense)             (None, 2500, 32)          160       \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 80000)             0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 15)                1200015   \n",
      "=================================================================\n",
      "Total params: 1,202,287\n",
      "Trainable params: 1,202,287\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fitting model\n",
      "SETTINGS FOR THIS TEST:\n",
      "   test_num  layer_num layer_type  num_nodes activation optimizer loss  \\\n",
      "0         2          1      Dense       32.0       relu      Adam  mse   \n",
      "1         2          2      Dense       32.0       relu       NaN  NaN   \n",
      "2         2          3      Dense       32.0       relu       NaN  NaN   \n",
      "3         2          4    Flatten        NaN        NaN       NaN  NaN   \n",
      "4         2          5      Dense       15.0        NaN       NaN  NaN   \n",
      "\n",
      "   epochs  batch_size  steps_per_epoch     lr  max_queue_size  \\\n",
      "0    30.0         8.0           3200.0  0.001           320.0   \n",
      "1     NaN         NaN              NaN    NaN             NaN   \n",
      "2     NaN         NaN              NaN    NaN             NaN   \n",
      "3     NaN         NaN              NaN    NaN             NaN   \n",
      "4     NaN         NaN              NaN    NaN             NaN   \n",
      "\n",
      "   kernel_regularizer  train_samp_num  test_samp_num  \n",
      "0                 0.0         20000.0         2000.0  \n",
      "1                 0.0             NaN            NaN  \n",
      "2                 0.0             NaN            NaN  \n",
      "3                 0.0             NaN            NaN  \n",
      "4                 0.0             NaN            NaN   \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "3200/3200 [==============================] - 144s 45ms/step - loss: 0.1808 - mean_absolute_error: 0.1699 - val_loss: 0.1595 - val_mean_absolute_error: 0.1525\n",
      "Epoch 2/30\n",
      "3200/3200 [==============================] - 148s 46ms/step - loss: 0.1529 - mean_absolute_error: 0.1476 - val_loss: 0.1547 - val_mean_absolute_error: 0.1455\n",
      "Epoch 3/30\n",
      "3200/3200 [==============================] - 146s 46ms/step - loss: 0.1490 - mean_absolute_error: 0.1397 - val_loss: 0.1520 - val_mean_absolute_error: 0.1374\n",
      "Epoch 4/30\n",
      "3200/3200 [==============================] - 144s 45ms/step - loss: 0.1484 - mean_absolute_error: 0.1355 - val_loss: 0.1423 - val_mean_absolute_error: 0.1275\n",
      "Epoch 5/30\n",
      "3200/3200 [==============================] - 146s 46ms/step - loss: 0.1460 - mean_absolute_error: 0.1295 - val_loss: 0.1503 - val_mean_absolute_error: 0.1388\n",
      "Epoch 6/30\n",
      "3200/3200 [==============================] - 144s 45ms/step - loss: 0.1444 - mean_absolute_error: 0.1248 - val_loss: 0.1460 - val_mean_absolute_error: 0.1299\n",
      "Epoch 7/30\n",
      "3200/3200 [==============================] - 142s 44ms/step - loss: 0.1446 - mean_absolute_error: 0.1205 - val_loss: 0.1384 - val_mean_absolute_error: 0.1112\n",
      "Epoch 8/30\n",
      "3200/3200 [==============================] - 143s 45ms/step - loss: 0.1428 - mean_absolute_error: 0.1162 - val_loss: 0.1417 - val_mean_absolute_error: 0.1048\n",
      "Epoch 9/30\n",
      "3200/3200 [==============================] - 141s 44ms/step - loss: 0.1426 - mean_absolute_error: 0.1136 - val_loss: 0.1402 - val_mean_absolute_error: 0.1163\n",
      "Epoch 10/30\n",
      "3200/3200 [==============================] - 145s 45ms/step - loss: 0.1424 - mean_absolute_error: 0.1129 - val_loss: 0.1399 - val_mean_absolute_error: 0.1065\n",
      "Epoch 11/30\n",
      "3200/3200 [==============================] - 145s 45ms/step - loss: 0.1425 - mean_absolute_error: 0.1113 - val_loss: 0.1440 - val_mean_absolute_error: 0.1141\n",
      "Epoch 12/30\n",
      "3200/3200 [==============================] - 146s 46ms/step - loss: 0.1431 - mean_absolute_error: 0.1098 - val_loss: 0.1457 - val_mean_absolute_error: 0.1041\n",
      "Epoch 13/30\n",
      "3200/3200 [==============================] - 147s 46ms/step - loss: 0.1421 - mean_absolute_error: 0.1101 - val_loss: 0.1470 - val_mean_absolute_error: 0.1210\n",
      "Epoch 14/30\n",
      "3200/3200 [==============================] - 146s 46ms/step - loss: 0.1405 - mean_absolute_error: 0.1085 - val_loss: 0.1475 - val_mean_absolute_error: 0.1143\n",
      "Epoch 15/30\n",
      "3200/3200 [==============================] - 147s 46ms/step - loss: 0.1424 - mean_absolute_error: 0.1089 - val_loss: 0.1443 - val_mean_absolute_error: 0.1021\n",
      "Epoch 16/30\n",
      "3200/3200 [==============================] - 147s 46ms/step - loss: 0.1417 - mean_absolute_error: 0.1077 - val_loss: 0.1418 - val_mean_absolute_error: 0.1026\n",
      "Epoch 17/30\n",
      "3200/3200 [==============================] - 147s 46ms/step - loss: 0.1418 - mean_absolute_error: 0.1079 - val_loss: 0.1385 - val_mean_absolute_error: 0.1057\n",
      "Epoch 18/30\n",
      "3200/3200 [==============================] - 146s 46ms/step - loss: 0.1414 - mean_absolute_error: 0.1078 - val_loss: 0.1430 - val_mean_absolute_error: 0.1001\n",
      "Epoch 19/30\n",
      "3200/3200 [==============================] - 147s 46ms/step - loss: 0.1412 - mean_absolute_error: 0.1070 - val_loss: 0.1431 - val_mean_absolute_error: 0.1052\n",
      "Epoch 20/30\n",
      "3200/3200 [==============================] - 147s 46ms/step - loss: 0.1407 - mean_absolute_error: 0.1065 - val_loss: 0.1476 - val_mean_absolute_error: 0.1180\n",
      "Epoch 21/30\n",
      "3200/3200 [==============================] - 147s 46ms/step - loss: 0.1424 - mean_absolute_error: 0.1075 - val_loss: 0.1422 - val_mean_absolute_error: 0.1005\n",
      "Epoch 22/30\n",
      "3200/3200 [==============================] - 147s 46ms/step - loss: 0.1415 - mean_absolute_error: 0.1068 - val_loss: 0.1412 - val_mean_absolute_error: 0.1007\n",
      "Epoch 23/30\n",
      "3200/3200 [==============================] - 146s 46ms/step - loss: 0.1422 - mean_absolute_error: 0.1073 - val_loss: 0.1374 - val_mean_absolute_error: 0.1005\n",
      "Epoch 24/30\n",
      "3200/3200 [==============================] - 145s 45ms/step - loss: 0.1413 - mean_absolute_error: 0.1073 - val_loss: 0.1376 - val_mean_absolute_error: 0.1023\n",
      "Epoch 25/30\n",
      "3200/3200 [==============================] - 143s 45ms/step - loss: 0.1419 - mean_absolute_error: 0.1068 - val_loss: 0.1464 - val_mean_absolute_error: 0.1215\n",
      "Epoch 26/30\n",
      "3200/3200 [==============================] - 142s 44ms/step - loss: 0.1413 - mean_absolute_error: 0.1074 - val_loss: 0.1418 - val_mean_absolute_error: 0.1174\n",
      "Epoch 27/30\n",
      "3200/3200 [==============================] - 143s 45ms/step - loss: 0.1427 - mean_absolute_error: 0.1077 - val_loss: 0.1425 - val_mean_absolute_error: 0.1043\n",
      "Epoch 28/30\n",
      "3200/3200 [==============================] - 143s 45ms/step - loss: 0.1408 - mean_absolute_error: 0.1066 - val_loss: 0.1439 - val_mean_absolute_error: 0.1124\n",
      "Epoch 29/30\n",
      "3200/3200 [==============================] - 144s 45ms/step - loss: 0.1408 - mean_absolute_error: 0.1065 - val_loss: 0.1439 - val_mean_absolute_error: 0.1179\n",
      "Epoch 30/30\n",
      "3200/3200 [==============================] - 142s 44ms/step - loss: 0.1408 - mean_absolute_error: 0.1058 - val_loss: 0.1427 - val_mean_absolute_error: 0.1043\n",
      "Exporting plots\n",
      "Exporting CSV with test parameters and best mean absolute error result\n",
      "Exporting Test Data\n",
      "\n",
      "BEGINNING TEST: 3\n",
      "C:\\Users\\aambr\\OneDrive\\Documents\\UNH Fall 2019\\Research\\zernike_polynomials\\Data_and_Results_12-12-19\\test_plan_generator.csv\n",
      "Creating storage directory\n",
      "Creating the model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_61 (Dense)             (None, 2500, 32)          160       \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 80000)             0         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 15)                1200015   \n",
      "=================================================================\n",
      "Total params: 1,202,287\n",
      "Trainable params: 1,202,287\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fitting model\n",
      "SETTINGS FOR THIS TEST:\n",
      "   test_num  layer_num layer_type  num_nodes activation optimizer loss  \\\n",
      "0         3          1      Dense       32.0       relu      Adam  mse   \n",
      "1         3          2      Dense       32.0       relu       NaN  NaN   \n",
      "2         3          3      Dense       32.0       relu       NaN  NaN   \n",
      "3         3          4    Flatten        NaN        NaN       NaN  NaN   \n",
      "4         3          5      Dense       15.0        NaN       NaN  NaN   \n",
      "\n",
      "   epochs  batch_size  steps_per_epoch     lr  max_queue_size  \\\n",
      "0    30.0        64.0           3200.0  0.001           320.0   \n",
      "1     NaN         NaN              NaN    NaN             NaN   \n",
      "2     NaN         NaN              NaN    NaN             NaN   \n",
      "3     NaN         NaN              NaN    NaN             NaN   \n",
      "4     NaN         NaN              NaN    NaN             NaN   \n",
      "\n",
      "   kernel_regularizer  train_samp_num  test_samp_num  \n",
      "0                 0.0         20000.0         2000.0  \n",
      "1                 0.0             NaN            NaN  \n",
      "2                 0.0             NaN            NaN  \n",
      "3                 0.0             NaN            NaN  \n",
      "4                 0.0             NaN            NaN   \n",
      "\n",
      "Epoch 1/30\n",
      "3200/3200 [==============================] - 907s 283ms/step - loss: 0.1706 - mean_absolute_error: 0.1357 - val_loss: 0.1437 - val_mean_absolute_error: 0.1127\n",
      "Epoch 2/30\n",
      "3200/3200 [==============================] - 904s 282ms/step - loss: 0.1429 - mean_absolute_error: 0.1124 - val_loss: 0.1416 - val_mean_absolute_error: 0.1095\n",
      "Epoch 3/30\n",
      "3200/3200 [==============================] - 903s 282ms/step - loss: 0.1421 - mean_absolute_error: 0.1113 - val_loss: 0.1405 - val_mean_absolute_error: 0.1069\n",
      "Epoch 4/30\n",
      "3200/3200 [==============================] - 898s 281ms/step - loss: 0.1415 - mean_absolute_error: 0.1097 - val_loss: 0.1412 - val_mean_absolute_error: 0.1099\n",
      "Epoch 5/30\n",
      "3200/3200 [==============================] - 894s 279ms/step - loss: 0.1424 - mean_absolute_error: 0.1091 - val_loss: 0.1409 - val_mean_absolute_error: 0.1069\n",
      "Epoch 6/30\n",
      "3200/3200 [==============================] - 897s 280ms/step - loss: 0.1413 - mean_absolute_error: 0.1077 - val_loss: 0.1417 - val_mean_absolute_error: 0.1078\n",
      "Epoch 7/30\n",
      "3200/3200 [==============================] - 896s 280ms/step - loss: 0.1408 - mean_absolute_error: 0.1068 - val_loss: 0.1417 - val_mean_absolute_error: 0.1083\n",
      "Epoch 8/30\n",
      "3200/3200 [==============================] - 897s 280ms/step - loss: 0.1410 - mean_absolute_error: 0.1057 - val_loss: 0.1404 - val_mean_absolute_error: 0.1039\n",
      "Epoch 9/30\n",
      "3200/3200 [==============================] - 896s 280ms/step - loss: 0.1404 - mean_absolute_error: 0.1041 - val_loss: 0.1405 - val_mean_absolute_error: 0.1073\n",
      "Epoch 10/30\n",
      "3200/3200 [==============================] - 897s 280ms/step - loss: 0.1407 - mean_absolute_error: 0.1035 - val_loss: 0.1403 - val_mean_absolute_error: 0.1005\n",
      "Epoch 11/30\n",
      "3200/3200 [==============================] - 895s 280ms/step - loss: 0.1407 - mean_absolute_error: 0.1026 - val_loss: 0.1413 - val_mean_absolute_error: 0.1095\n",
      "Epoch 12/30\n",
      "3200/3200 [==============================] - 886s 277ms/step - loss: 0.1399 - mean_absolute_error: 0.1016 - val_loss: 0.1403 - val_mean_absolute_error: 0.1005\n",
      "Epoch 13/30\n",
      "3200/3200 [==============================] - 898s 281ms/step - loss: 0.1396 - mean_absolute_error: 0.1010 - val_loss: 0.1377 - val_mean_absolute_error: 0.0967\n",
      "Epoch 14/30\n",
      "3200/3200 [==============================] - 903s 282ms/step - loss: 0.1401 - mean_absolute_error: 0.1009 - val_loss: 0.1432 - val_mean_absolute_error: 0.1059\n",
      "Epoch 15/30\n",
      "3200/3200 [==============================] - 896s 280ms/step - loss: 0.1401 - mean_absolute_error: 0.1003 - val_loss: 0.1394 - val_mean_absolute_error: 0.0986\n",
      "Epoch 16/30\n",
      "3200/3200 [==============================] - 894s 279ms/step - loss: 0.1402 - mean_absolute_error: 0.1001 - val_loss: 0.1400 - val_mean_absolute_error: 0.1020\n",
      "Epoch 17/30\n",
      "3200/3200 [==============================] - 887s 277ms/step - loss: 0.1401 - mean_absolute_error: 0.0996 - val_loss: 0.1407 - val_mean_absolute_error: 0.0978\n",
      "Epoch 18/30\n",
      "3200/3200 [==============================] - 899s 281ms/step - loss: 0.1403 - mean_absolute_error: 0.0994 - val_loss: 0.1394 - val_mean_absolute_error: 0.1007\n",
      "Epoch 19/30\n",
      "3200/3200 [==============================] - 893s 279ms/step - loss: 0.1394 - mean_absolute_error: 0.0985 - val_loss: 0.1413 - val_mean_absolute_error: 0.0964\n",
      "Epoch 20/30\n",
      "3200/3200 [==============================] - 896s 280ms/step - loss: 0.1396 - mean_absolute_error: 0.0984 - val_loss: 0.1410 - val_mean_absolute_error: 0.1003\n",
      "Epoch 21/30\n",
      "3200/3200 [==============================] - 894s 279ms/step - loss: 0.1401 - mean_absolute_error: 0.0983 - val_loss: 0.1413 - val_mean_absolute_error: 0.0993\n",
      "Epoch 22/30\n",
      "3200/3200 [==============================] - 901s 282ms/step - loss: 0.1398 - mean_absolute_error: 0.0980 - val_loss: 0.1387 - val_mean_absolute_error: 0.0977\n",
      "Epoch 23/30\n",
      "3200/3200 [==============================] - 899s 281ms/step - loss: 0.1399 - mean_absolute_error: 0.0978 - val_loss: 0.1410 - val_mean_absolute_error: 0.0994\n",
      "Epoch 24/30\n",
      "3200/3200 [==============================] - 897s 280ms/step - loss: 0.1399 - mean_absolute_error: 0.0975 - val_loss: 0.1385 - val_mean_absolute_error: 0.0952\n",
      "Epoch 25/30\n",
      "3200/3200 [==============================] - 896s 280ms/step - loss: 0.1403 - mean_absolute_error: 0.0974 - val_loss: 0.1387 - val_mean_absolute_error: 0.0923\n",
      "Epoch 26/30\n",
      "3200/3200 [==============================] - 902s 282ms/step - loss: 0.1403 - mean_absolute_error: 0.0971 - val_loss: 0.1399 - val_mean_absolute_error: 0.0958\n",
      "Epoch 27/30\n",
      "3200/3200 [==============================] - 905s 283ms/step - loss: 0.1393 - mean_absolute_error: 0.0966 - val_loss: 0.1409 - val_mean_absolute_error: 0.0946\n",
      "Epoch 28/30\n",
      "3200/3200 [==============================] - 895s 280ms/step - loss: 0.1398 - mean_absolute_error: 0.0966 - val_loss: 0.1392 - val_mean_absolute_error: 0.0992\n",
      "Epoch 29/30\n",
      "3200/3200 [==============================] - 900s 281ms/step - loss: 0.1397 - mean_absolute_error: 0.0961 - val_loss: 0.1400 - val_mean_absolute_error: 0.0934\n",
      "Epoch 30/30\n",
      "3200/3200 [==============================] - 890s 278ms/step - loss: 0.1395 - mean_absolute_error: 0.0961 - val_loss: 0.1401 - val_mean_absolute_error: 0.0963\n",
      "Exporting plots\n",
      "Exporting CSV with test parameters and best mean absolute error result\n",
      "Exporting Test Data\n",
      "\n",
      "BEGINNING TEST: 4\n",
      "C:\\Users\\aambr\\OneDrive\\Documents\\UNH Fall 2019\\Research\\zernike_polynomials\\Data_and_Results_12-12-19\\test_plan_generator.csv\n",
      "Creating storage directory\n",
      "Creating the model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_65 (Dense)             (None, 2500, 64)          320       \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 2500, 64)          4160      \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 2500, 64)          4160      \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 160000)            0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 15)                2400015   \n",
      "=================================================================\n",
      "Total params: 2,408,655\n",
      "Trainable params: 2,408,655\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fitting model\n",
      "SETTINGS FOR THIS TEST:\n",
      "   test_num  layer_num layer_type  num_nodes activation optimizer loss  \\\n",
      "0         4          1      Dense       64.0       relu      Adam  mse   \n",
      "1         4          2      Dense       64.0       relu       NaN  NaN   \n",
      "2         4          3      Dense       64.0       relu       NaN  NaN   \n",
      "3         4          4    Flatten        NaN        NaN       NaN  NaN   \n",
      "4         4          5      Dense       15.0        NaN       NaN  NaN   \n",
      "\n",
      "   epochs  batch_size  steps_per_epoch     lr  max_queue_size  \\\n",
      "0    30.0         8.0           3200.0  0.001           320.0   \n",
      "1     NaN         NaN              NaN    NaN             NaN   \n",
      "2     NaN         NaN              NaN    NaN             NaN   \n",
      "3     NaN         NaN              NaN    NaN             NaN   \n",
      "4     NaN         NaN              NaN    NaN             NaN   \n",
      "\n",
      "   kernel_regularizer  train_samp_num  test_samp_num  \n",
      "0                 0.0             NaN            NaN  \n",
      "1                 0.0             NaN            NaN  \n",
      "2                 0.0             NaN            NaN  \n",
      "3                 0.0             NaN            NaN  \n",
      "4                 0.0             NaN            NaN   \n",
      "\n",
      "Epoch 1/30\n",
      "3200/3200 [==============================] - 192s 60ms/step - loss: 0.2014 - mean_absolute_error: 0.1848 - val_loss: 0.1591 - val_mean_absolute_error: 0.1590\n",
      "Epoch 2/30\n",
      "3200/3200 [==============================] - 189s 59ms/step - loss: 0.1550 - mean_absolute_error: 0.1507 - val_loss: 0.1490 - val_mean_absolute_error: 0.1476\n",
      "Epoch 3/30\n",
      "3200/3200 [==============================] - 189s 59ms/step - loss: 0.1500 - mean_absolute_error: 0.1409 - val_loss: 0.1410 - val_mean_absolute_error: 0.1281\n",
      "Epoch 4/30\n",
      "3200/3200 [==============================] - 189s 59ms/step - loss: 0.1461 - mean_absolute_error: 0.1325 - val_loss: 0.1485 - val_mean_absolute_error: 0.1375\n",
      "Epoch 5/30\n",
      "3200/3200 [==============================] - 189s 59ms/step - loss: 0.1451 - mean_absolute_error: 0.1264 - val_loss: 0.1423 - val_mean_absolute_error: 0.1199\n",
      "Epoch 6/30\n",
      "3200/3200 [==============================] - 189s 59ms/step - loss: 0.1461 - mean_absolute_error: 0.1235 - val_loss: 0.1374 - val_mean_absolute_error: 0.1117\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3200 [==============================] - 184s 57ms/step - loss: 0.1444 - mean_absolute_error: 0.1214 - val_loss: 0.1466 - val_mean_absolute_error: 0.1169\n",
      "Epoch 8/30\n",
      "3200/3200 [==============================] - 185s 58ms/step - loss: 0.1454 - mean_absolute_error: 0.1204 - val_loss: 0.1384 - val_mean_absolute_error: 0.1144\n",
      "Epoch 9/30\n",
      "3200/3200 [==============================] - 183s 57ms/step - loss: 0.1436 - mean_absolute_error: 0.1188 - val_loss: 0.1438 - val_mean_absolute_error: 0.1163\n",
      "Epoch 10/30\n",
      "3200/3200 [==============================] - 184s 57ms/step - loss: 0.1430 - mean_absolute_error: 0.1177 - val_loss: 0.1422 - val_mean_absolute_error: 0.1133\n",
      "Epoch 11/30\n",
      "3200/3200 [==============================] - 183s 57ms/step - loss: 0.1425 - mean_absolute_error: 0.1160 - val_loss: 0.1439 - val_mean_absolute_error: 0.1155\n",
      "Epoch 12/30\n",
      "3200/3200 [==============================] - 183s 57ms/step - loss: 0.1414 - mean_absolute_error: 0.1161 - val_loss: 0.1409 - val_mean_absolute_error: 0.1193\n",
      "Epoch 13/30\n",
      "3200/3200 [==============================] - 183s 57ms/step - loss: 0.1434 - mean_absolute_error: 0.1163 - val_loss: 0.1439 - val_mean_absolute_error: 0.1165\n",
      "Epoch 14/30\n",
      "3200/3200 [==============================] - 184s 57ms/step - loss: 0.1401 - mean_absolute_error: 0.1134 - val_loss: 0.1430 - val_mean_absolute_error: 0.1176\n",
      "Epoch 15/30\n",
      "3200/3200 [==============================] - 183s 57ms/step - loss: 0.1420 - mean_absolute_error: 0.1132 - val_loss: 0.1413 - val_mean_absolute_error: 0.1135\n",
      "Epoch 16/30\n",
      "3200/3200 [==============================] - 183s 57ms/step - loss: 0.1419 - mean_absolute_error: 0.1127 - val_loss: 0.1439 - val_mean_absolute_error: 0.1184\n",
      "Epoch 17/30\n",
      "3200/3200 [==============================] - 188s 59ms/step - loss: 0.1407 - mean_absolute_error: 0.1114 - val_loss: 0.1448 - val_mean_absolute_error: 0.1151\n",
      "Epoch 18/30\n",
      "3200/3200 [==============================] - 189s 59ms/step - loss: 0.1416 - mean_absolute_error: 0.1119 - val_loss: 0.1420 - val_mean_absolute_error: 0.1169\n",
      "Epoch 19/30\n",
      "3200/3200 [==============================] - 188s 59ms/step - loss: 0.1420 - mean_absolute_error: 0.1113 - val_loss: 0.1427 - val_mean_absolute_error: 0.1062\n",
      "Epoch 20/30\n",
      "3200/3200 [==============================] - 188s 59ms/step - loss: 0.1425 - mean_absolute_error: 0.1110 - val_loss: 0.1409 - val_mean_absolute_error: 0.1042\n",
      "Epoch 21/30\n",
      "3200/3200 [==============================] - 188s 59ms/step - loss: 0.1412 - mean_absolute_error: 0.1097 - val_loss: 0.1391 - val_mean_absolute_error: 0.1101\n",
      "Epoch 22/30\n",
      "3200/3200 [==============================] - 187s 58ms/step - loss: 0.1417 - mean_absolute_error: 0.1088 - val_loss: 0.1420 - val_mean_absolute_error: 0.1097\n",
      "Epoch 23/30\n",
      "3200/3200 [==============================] - 188s 59ms/step - loss: 0.1398 - mean_absolute_error: 0.1086 - val_loss: 0.1410 - val_mean_absolute_error: 0.1028\n",
      "Epoch 24/30\n",
      "3200/3200 [==============================] - 186s 58ms/step - loss: 0.1415 - mean_absolute_error: 0.1075 - val_loss: 0.1431 - val_mean_absolute_error: 0.1032\n",
      "Epoch 25/30\n",
      "3200/3200 [==============================] - 188s 59ms/step - loss: 0.1407 - mean_absolute_error: 0.1065 - val_loss: 0.1389 - val_mean_absolute_error: 0.1042\n",
      "Epoch 26/30\n",
      "3200/3200 [==============================] - 185s 58ms/step - loss: 0.1416 - mean_absolute_error: 0.1073 - val_loss: 0.1450 - val_mean_absolute_error: 0.1021\n",
      "Epoch 27/30\n",
      "3200/3200 [==============================] - 182s 57ms/step - loss: 0.1405 - mean_absolute_error: 0.1056 - val_loss: 0.1347 - val_mean_absolute_error: 0.1025\n",
      "Epoch 28/30\n",
      "3200/3200 [==============================] - 182s 57ms/step - loss: 0.1407 - mean_absolute_error: 0.1056 - val_loss: 0.1408 - val_mean_absolute_error: 0.1053\n",
      "Epoch 29/30\n",
      "3200/3200 [==============================] - 181s 57ms/step - loss: 0.1413 - mean_absolute_error: 0.1063 - val_loss: 0.1321 - val_mean_absolute_error: 0.0977\n",
      "Epoch 30/30\n",
      "3200/3200 [==============================] - 183s 57ms/step - loss: 0.1406 - mean_absolute_error: 0.1056 - val_loss: 0.1422 - val_mean_absolute_error: 0.1024\n",
      "Exporting plots\n",
      "Exporting CSV with test parameters and best mean absolute error result\n",
      "Exporting Test Data\n",
      "\n",
      "BEGINNING TEST: 5\n",
      "C:\\Users\\aambr\\OneDrive\\Documents\\UNH Fall 2019\\Research\\zernike_polynomials\\Data_and_Results_12-12-19\\test_plan_generator.csv\n",
      "Creating storage directory\n",
      "Creating the model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_69 (Dense)             (None, 2500, 32)          160       \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 80000)             0         \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 15)                1200015   \n",
      "=================================================================\n",
      "Total params: 1,204,399\n",
      "Trainable params: 1,204,399\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fitting model\n",
      "SETTINGS FOR THIS TEST:\n",
      "   test_num  layer_num layer_type  num_nodes activation optimizer loss  \\\n",
      "0         5          1      Dense       32.0       relu      Adam  mse   \n",
      "1         5          2      Dense       32.0       relu       NaN  NaN   \n",
      "2         5          3      Dense       32.0       relu       NaN  NaN   \n",
      "3         5          4      Dense       32.0       relu       NaN  NaN   \n",
      "4         5          5      Dense       32.0       relu       NaN  NaN   \n",
      "5         5          6    Flatten        NaN        NaN       NaN  NaN   \n",
      "6         5          7      Dense       15.0        NaN       NaN  NaN   \n",
      "\n",
      "   epochs  batch_size  steps_per_epoch     lr  max_queue_size  \\\n",
      "0    30.0         8.0           3200.0  0.001           320.0   \n",
      "1     NaN         NaN              NaN    NaN             NaN   \n",
      "2     NaN         NaN              NaN    NaN             NaN   \n",
      "3     NaN         NaN              NaN    NaN             NaN   \n",
      "4     NaN         NaN              NaN    NaN             NaN   \n",
      "5     NaN         NaN              NaN    NaN             NaN   \n",
      "6     NaN         NaN              NaN    NaN             NaN   \n",
      "\n",
      "   kernel_regularizer  train_samp_num  test_samp_num  \n",
      "0                 0.0             NaN            NaN  \n",
      "1                 0.0             NaN            NaN  \n",
      "2                 0.0             NaN            NaN  \n",
      "3                 0.0             NaN            NaN  \n",
      "4                 0.0             NaN            NaN  \n",
      "5                 0.0             NaN            NaN  \n",
      "6                 0.0             NaN            NaN   \n",
      "\n",
      "Epoch 1/30\n",
      "3200/3200 [==============================] - 158s 49ms/step - loss: 0.1763 - mean_absolute_error: 0.1699 - val_loss: 0.1472 - val_mean_absolute_error: 0.1449\n",
      "Epoch 2/30\n",
      "3200/3200 [==============================] - 156s 49ms/step - loss: 0.1494 - mean_absolute_error: 0.1405 - val_loss: 0.1435 - val_mean_absolute_error: 0.1316\n",
      "Epoch 3/30\n",
      "3200/3200 [==============================] - 157s 49ms/step - loss: 0.1478 - mean_absolute_error: 0.1360 - val_loss: 0.1535 - val_mean_absolute_error: 0.1360\n",
      "Epoch 4/30\n",
      "3200/3200 [==============================] - 156s 49ms/step - loss: 0.1466 - mean_absolute_error: 0.1304 - val_loss: 0.1462 - val_mean_absolute_error: 0.1317\n",
      "Epoch 5/30\n",
      "3200/3200 [==============================] - 156s 49ms/step - loss: 0.1449 - mean_absolute_error: 0.1275 - val_loss: 0.1450 - val_mean_absolute_error: 0.1151\n",
      "Epoch 6/30\n",
      "3200/3200 [==============================] - 156s 49ms/step - loss: 0.1446 - mean_absolute_error: 0.1233 - val_loss: 0.1434 - val_mean_absolute_error: 0.1173\n",
      "Epoch 7/30\n",
      "3200/3200 [==============================] - 160s 50ms/step - loss: 0.1435 - mean_absolute_error: 0.1204 - val_loss: 0.1481 - val_mean_absolute_error: 0.1255\n",
      "Epoch 8/30\n",
      "3200/3200 [==============================] - 160s 50ms/step - loss: 0.1445 - mean_absolute_error: 0.1186 - val_loss: 0.1441 - val_mean_absolute_error: 0.1147\n",
      "Epoch 9/30\n",
      "3200/3200 [==============================] - 160s 50ms/step - loss: 0.1436 - mean_absolute_error: 0.1176 - val_loss: 0.1427 - val_mean_absolute_error: 0.1159\n",
      "Epoch 10/30\n",
      "3200/3200 [==============================] - 161s 50ms/step - loss: 0.1417 - mean_absolute_error: 0.1153 - val_loss: 0.1434 - val_mean_absolute_error: 0.1119\n",
      "Epoch 11/30\n",
      "3200/3200 [==============================] - 160s 50ms/step - loss: 0.1423 - mean_absolute_error: 0.1144 - val_loss: 0.1401 - val_mean_absolute_error: 0.1132\n",
      "Epoch 12/30\n",
      "3200/3200 [==============================] - 160s 50ms/step - loss: 0.1422 - mean_absolute_error: 0.1134 - val_loss: 0.1410 - val_mean_absolute_error: 0.1176\n",
      "Epoch 13/30\n",
      "3200/3200 [==============================] - 161s 50ms/step - loss: 0.1421 - mean_absolute_error: 0.1123 - val_loss: 0.1439 - val_mean_absolute_error: 0.1162\n",
      "Epoch 14/30\n",
      "3200/3200 [==============================] - 161s 50ms/step - loss: 0.1412 - mean_absolute_error: 0.1114 - val_loss: 0.1397 - val_mean_absolute_error: 0.1121\n",
      "Epoch 15/30\n",
      "3200/3200 [==============================] - 161s 50ms/step - loss: 0.1430 - mean_absolute_error: 0.1114 - val_loss: 0.1428 - val_mean_absolute_error: 0.1193\n",
      "Epoch 16/30\n",
      "3200/3200 [==============================] - 160s 50ms/step - loss: 0.1418 - mean_absolute_error: 0.1107 - val_loss: 0.1402 - val_mean_absolute_error: 0.1089\n",
      "Epoch 17/30\n",
      "3200/3200 [==============================] - 160s 50ms/step - loss: 0.1415 - mean_absolute_error: 0.1107 - val_loss: 0.1412 - val_mean_absolute_error: 0.1055\n",
      "Epoch 18/30\n",
      "3200/3200 [==============================] - 159s 50ms/step - loss: 0.1421 - mean_absolute_error: 0.1105 - val_loss: 0.1400 - val_mean_absolute_error: 0.1121\n",
      "Epoch 19/30\n",
      "3200/3200 [==============================] - 157s 49ms/step - loss: 0.1409 - mean_absolute_error: 0.1098 - val_loss: 0.1422 - val_mean_absolute_error: 0.1134\n",
      "Epoch 20/30\n",
      "3200/3200 [==============================] - 158s 49ms/step - loss: 0.1418 - mean_absolute_error: 0.1093 - val_loss: 0.1414 - val_mean_absolute_error: 0.1095\n",
      "Epoch 21/30\n",
      "3200/3200 [==============================] - 156s 49ms/step - loss: 0.1423 - mean_absolute_error: 0.1101 - val_loss: 0.1410 - val_mean_absolute_error: 0.1081\n",
      "Epoch 22/30\n",
      "3200/3200 [==============================] - 158s 49ms/step - loss: 0.1421 - mean_absolute_error: 0.1094 - val_loss: 0.1409 - val_mean_absolute_error: 0.1031\n",
      "Epoch 23/30\n",
      "3200/3200 [==============================] - 156s 49ms/step - loss: 0.1422 - mean_absolute_error: 0.1096 - val_loss: 0.1401 - val_mean_absolute_error: 0.1017\n",
      "Epoch 24/30\n",
      "3200/3200 [==============================] - 157s 49ms/step - loss: 0.1411 - mean_absolute_error: 0.1094 - val_loss: 0.1430 - val_mean_absolute_error: 0.1082\n",
      "Epoch 25/30\n",
      "3200/3200 [==============================] - 158s 49ms/step - loss: 0.1416 - mean_absolute_error: 0.1093 - val_loss: 0.1447 - val_mean_absolute_error: 0.1036\n",
      "Epoch 26/30\n",
      "3200/3200 [==============================] - 157s 49ms/step - loss: 0.1418 - mean_absolute_error: 0.1092 - val_loss: 0.1431 - val_mean_absolute_error: 0.1088\n",
      "Epoch 27/30\n",
      "3200/3200 [==============================] - 157s 49ms/step - loss: 0.1418 - mean_absolute_error: 0.1087 - val_loss: 0.1481 - val_mean_absolute_error: 0.1155\n",
      "Epoch 28/30\n",
      "3200/3200 [==============================] - 158s 49ms/step - loss: 0.1413 - mean_absolute_error: 0.1089 - val_loss: 0.1453 - val_mean_absolute_error: 0.1122\n",
      "Epoch 29/30\n",
      "3200/3200 [==============================] - 156s 49ms/step - loss: 0.1417 - mean_absolute_error: 0.1096 - val_loss: 0.1410 - val_mean_absolute_error: 0.1052\n",
      "Epoch 30/30\n",
      "3200/3200 [==============================] - 161s 50ms/step - loss: 0.1414 - mean_absolute_error: 0.1089 - val_loss: 0.1419 - val_mean_absolute_error: 0.1114\n",
      "Exporting plots\n",
      "Exporting CSV with test parameters and best mean absolute error result\n",
      "Exporting Test Data\n",
      "\n",
      "BEGINNING TEST: 6\n",
      "C:\\Users\\aambr\\OneDrive\\Documents\\UNH Fall 2019\\Research\\zernike_polynomials\\Data_and_Results_12-12-19\\test_plan_generator.csv\n",
      "Creating storage directory\n",
      "Creating the model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_75 (Dense)             (None, 2500, 32)          160       \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 80000)             0         \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 15)                1200015   \n",
      "=================================================================\n",
      "Total params: 1,202,287\n",
      "Trainable params: 1,202,287\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fitting model\n",
      "SETTINGS FOR THIS TEST:\n",
      "   test_num  layer_num layer_type  num_nodes activation optimizer loss  \\\n",
      "0         6          1      Dense       32.0    sigmoid      Adam  mse   \n",
      "1         6          2      Dense       32.0    sigmoid       NaN  NaN   \n",
      "2         6          3      Dense       32.0    sigmoid       NaN  NaN   \n",
      "3         6          4    Flatten        NaN        NaN       NaN  NaN   \n",
      "4         6          5      Dense       15.0        NaN       NaN  NaN   \n",
      "\n",
      "   epochs  batch_size  steps_per_epoch     lr  max_queue_size  \\\n",
      "0    30.0         8.0           3200.0  0.001           320.0   \n",
      "1     NaN         NaN              NaN    NaN             NaN   \n",
      "2     NaN         NaN              NaN    NaN             NaN   \n",
      "3     NaN         NaN              NaN    NaN             NaN   \n",
      "4     NaN         NaN              NaN    NaN             NaN   \n",
      "\n",
      "   kernel_regularizer  train_samp_num  test_samp_num  \n",
      "0                 0.0             NaN            NaN  \n",
      "1                 0.0             NaN            NaN  \n",
      "2                 0.0             NaN            NaN  \n",
      "3                 0.0             NaN            NaN  \n",
      "4                 0.0             NaN            NaN   \n",
      "\n",
      "Epoch 1/30\n",
      "3200/3200 [==============================] - 149s 47ms/step - loss: 2.3000 - mean_absolute_error: 0.6166 - val_loss: 0.2273 - val_mean_absolute_error: 0.2593\n",
      "Epoch 2/30\n",
      "3200/3200 [==============================] - 148s 46ms/step - loss: 0.2150 - mean_absolute_error: 0.2175 - val_loss: 0.2650 - val_mean_absolute_error: 0.1865\n",
      "Epoch 3/30\n",
      "3200/3200 [==============================] - 147s 46ms/step - loss: 0.1668 - mean_absolute_error: 0.1560 - val_loss: 0.1807 - val_mean_absolute_error: 0.1482\n",
      "Epoch 4/30\n",
      "3200/3200 [==============================] - 147s 46ms/step - loss: 0.1553 - mean_absolute_error: 0.1370 - val_loss: 0.1447 - val_mean_absolute_error: 0.1244\n",
      "Epoch 5/30\n",
      "3200/3200 [==============================] - 147s 46ms/step - loss: 0.1479 - mean_absolute_error: 0.1257 - val_loss: 0.1419 - val_mean_absolute_error: 0.1194\n",
      "Epoch 6/30\n",
      "3200/3200 [==============================] - 148s 46ms/step - loss: 0.1457 - mean_absolute_error: 0.1183 - val_loss: 0.1428 - val_mean_absolute_error: 0.1158\n",
      "Epoch 7/30\n",
      "3200/3200 [==============================] - 147s 46ms/step - loss: 0.1442 - mean_absolute_error: 0.1117 - val_loss: 0.1394 - val_mean_absolute_error: 0.1058\n",
      "Epoch 8/30\n",
      "3200/3200 [==============================] - 147s 46ms/step - loss: 0.1426 - mean_absolute_error: 0.1067 - val_loss: 0.1404 - val_mean_absolute_error: 0.1006\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3200 [==============================] - 147s 46ms/step - loss: 0.1429 - mean_absolute_error: 0.1049 - val_loss: 0.1410 - val_mean_absolute_error: 0.1032\n",
      "Epoch 10/30\n",
      "3200/3200 [==============================] - 146s 46ms/step - loss: 0.1419 - mean_absolute_error: 0.1026 - val_loss: 0.1379 - val_mean_absolute_error: 0.1012\n",
      "Epoch 11/30\n",
      "3200/3200 [==============================] - 146s 46ms/step - loss: 0.1398 - mean_absolute_error: 0.1004 - val_loss: 0.1456 - val_mean_absolute_error: 0.1017\n",
      "Epoch 12/30\n",
      "3200/3200 [==============================] - 152s 47ms/step - loss: 0.1399 - mean_absolute_error: 0.0996 - val_loss: 0.1419 - val_mean_absolute_error: 0.0987\n",
      "Epoch 13/30\n",
      "3200/3200 [==============================] - 144s 45ms/step - loss: 0.1399 - mean_absolute_error: 0.0985 - val_loss: 0.1424 - val_mean_absolute_error: 0.0993\n",
      "Epoch 14/30\n",
      "3200/3200 [==============================] - 145s 45ms/step - loss: 0.1401 - mean_absolute_error: 0.0978 - val_loss: 0.1427 - val_mean_absolute_error: 0.0990\n",
      "Epoch 15/30\n",
      "3200/3200 [==============================] - 145s 45ms/step - loss: 0.1395 - mean_absolute_error: 0.0972 - val_loss: 0.1419 - val_mean_absolute_error: 0.0978\n",
      "Epoch 16/30\n",
      "3200/3200 [==============================] - 144s 45ms/step - loss: 0.1401 - mean_absolute_error: 0.0967 - val_loss: 0.1431 - val_mean_absolute_error: 0.0963\n",
      "Epoch 17/30\n",
      "3200/3200 [==============================] - 144s 45ms/step - loss: 0.1411 - mean_absolute_error: 0.0970 - val_loss: 0.1403 - val_mean_absolute_error: 0.0979\n",
      "Epoch 18/30\n",
      "3200/3200 [==============================] - 144s 45ms/step - loss: 0.1395 - mean_absolute_error: 0.0959 - val_loss: 0.1389 - val_mean_absolute_error: 0.0965\n",
      "Epoch 19/30\n",
      "3200/3200 [==============================] - 144s 45ms/step - loss: 0.1398 - mean_absolute_error: 0.0958 - val_loss: 0.1418 - val_mean_absolute_error: 0.1003\n",
      "Epoch 20/30\n",
      "3200/3200 [==============================] - 146s 46ms/step - loss: 0.1409 - mean_absolute_error: 0.0958 - val_loss: 0.1462 - val_mean_absolute_error: 0.1019\n",
      "Epoch 21/30\n",
      "3200/3200 [==============================] - 144s 45ms/step - loss: 0.1398 - mean_absolute_error: 0.0954 - val_loss: 0.1384 - val_mean_absolute_error: 0.0960\n",
      "Epoch 22/30\n",
      "3200/3200 [==============================] - 144s 45ms/step - loss: 0.1373 - mean_absolute_error: 0.0940 - val_loss: 0.1388 - val_mean_absolute_error: 0.0973\n",
      "Epoch 23/30\n",
      "3200/3200 [==============================] - 144s 45ms/step - loss: 0.1386 - mean_absolute_error: 0.0942 - val_loss: 0.1338 - val_mean_absolute_error: 0.0929\n",
      "Epoch 24/30\n",
      "3200/3200 [==============================] - 146s 46ms/step - loss: 0.1402 - mean_absolute_error: 0.0947 - val_loss: 0.1438 - val_mean_absolute_error: 0.0981\n",
      "Epoch 25/30\n",
      "3200/3200 [==============================] - 148s 46ms/step - loss: 0.1404 - mean_absolute_error: 0.0943 - val_loss: 0.1421 - val_mean_absolute_error: 0.0952\n",
      "Epoch 26/30\n",
      "3200/3200 [==============================] - 147s 46ms/step - loss: 0.1407 - mean_absolute_error: 0.0944 - val_loss: 0.1389 - val_mean_absolute_error: 0.0925\n",
      "Epoch 27/30\n",
      "3200/3200 [==============================] - 147s 46ms/step - loss: 0.1378 - mean_absolute_error: 0.0930 - val_loss: 0.1350 - val_mean_absolute_error: 0.0932\n",
      "Epoch 28/30\n",
      "3200/3200 [==============================] - 147s 46ms/step - loss: 0.1400 - mean_absolute_error: 0.0940 - val_loss: 0.1396 - val_mean_absolute_error: 0.0915\n",
      "Epoch 29/30\n",
      "3200/3200 [==============================] - 148s 46ms/step - loss: 0.1400 - mean_absolute_error: 0.0939 - val_loss: 0.1442 - val_mean_absolute_error: 0.0984\n",
      "Epoch 30/30\n",
      "3200/3200 [==============================] - 147s 46ms/step - loss: 0.1390 - mean_absolute_error: 0.0931 - val_loss: 0.1408 - val_mean_absolute_error: 0.0924\n",
      "Exporting plots\n",
      "Exporting CSV with test parameters and best mean absolute error result\n",
      "Exporting Test Data\n",
      "\n",
      "BEGINNING TEST: 7\n",
      "C:\\Users\\aambr\\OneDrive\\Documents\\UNH Fall 2019\\Research\\zernike_polynomials\\Data_and_Results_12-12-19\\test_plan_generator.csv\n",
      "Creating storage directory\n",
      "Creating the model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_79 (Dense)             (None, 2500, 32)          160       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 2500, 32)          0         \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 2500, 32)          0         \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 2500, 32)          0         \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 2500, 32)          0         \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 2500, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 80000)             0         \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 15)                1200015   \n",
      "=================================================================\n",
      "Total params: 1,204,399\n",
      "Trainable params: 1,204,399\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fitting model\n",
      "SETTINGS FOR THIS TEST:\n",
      "   test_num  layer_num layer_type  num_nodes activation optimizer loss  \\\n",
      "0         7          1      Dense       32.0  LeakyReLU      Adam  mse   \n",
      "1         7          2      Dense       32.0  LeakyReLU       NaN  NaN   \n",
      "2         7          3      Dense       32.0  LeakyReLU       NaN  NaN   \n",
      "3         7          4      Dense       32.0  LeakyReLU       NaN  NaN   \n",
      "4         7          5      Dense       32.0  LeakyReLU       NaN  NaN   \n",
      "5         7          6    Flatten        NaN        NaN       NaN  NaN   \n",
      "6         7          7      Dense       15.0        NaN       NaN  NaN   \n",
      "\n",
      "   epochs  batch_size  steps_per_epoch     lr  max_queue_size  \\\n",
      "0    30.0         8.0           3200.0  0.001           320.0   \n",
      "1     NaN         NaN              NaN    NaN             NaN   \n",
      "2     NaN         NaN              NaN    NaN             NaN   \n",
      "3     NaN         NaN              NaN    NaN             NaN   \n",
      "4     NaN         NaN              NaN    NaN             NaN   \n",
      "5     NaN         NaN              NaN    NaN             NaN   \n",
      "6     NaN         NaN              NaN    NaN             NaN   \n",
      "\n",
      "   kernel_regularizer  train_samp_num  test_samp_num  \n",
      "0                 0.0             NaN            NaN  \n",
      "1                 0.0             NaN            NaN  \n",
      "2                 0.0             NaN            NaN  \n",
      "3                 0.0             NaN            NaN  \n",
      "4                 0.0             NaN            NaN  \n",
      "5                 0.0             NaN            NaN  \n",
      "6                 0.0             NaN            NaN   \n",
      "\n",
      "Epoch 1/30\n",
      "3200/3200 [==============================] - 162s 51ms/step - loss: 0.1713 - mean_absolute_error: 0.1594 - val_loss: 0.1530 - val_mean_absolute_error: 0.1344\n",
      "Epoch 2/30\n",
      "3200/3200 [==============================] - 165s 52ms/step - loss: 0.1510 - mean_absolute_error: 0.1367 - val_loss: 0.1514 - val_mean_absolute_error: 0.1421\n",
      "Epoch 3/30\n",
      "3200/3200 [==============================] - 161s 50ms/step - loss: 0.1487 - mean_absolute_error: 0.1325 - val_loss: 0.1438 - val_mean_absolute_error: 0.1307\n",
      "Epoch 4/30\n",
      "3200/3200 [==============================] - 161s 50ms/step - loss: 0.1472 - mean_absolute_error: 0.1296 - val_loss: 0.1459 - val_mean_absolute_error: 0.1231\n",
      "Epoch 5/30\n",
      "3200/3200 [==============================] - 160s 50ms/step - loss: 0.1463 - mean_absolute_error: 0.1256 - val_loss: 0.1425 - val_mean_absolute_error: 0.1194\n",
      "Epoch 6/30\n",
      "3200/3200 [==============================] - 158s 49ms/step - loss: 0.1450 - mean_absolute_error: 0.1235 - val_loss: 0.1466 - val_mean_absolute_error: 0.1144\n",
      "Epoch 7/30\n",
      "3200/3200 [==============================] - 156s 49ms/step - loss: 0.1443 - mean_absolute_error: 0.1197 - val_loss: 0.1360 - val_mean_absolute_error: 0.1115\n",
      "Epoch 8/30\n",
      "3200/3200 [==============================] - 158s 49ms/step - loss: 0.1451 - mean_absolute_error: 0.1184 - val_loss: 0.1462 - val_mean_absolute_error: 0.1305\n",
      "Epoch 9/30\n",
      "3200/3200 [==============================] - 156s 49ms/step - loss: 0.1440 - mean_absolute_error: 0.1165 - val_loss: 0.1419 - val_mean_absolute_error: 0.1201\n",
      "Epoch 10/30\n",
      "3200/3200 [==============================] - 156s 49ms/step - loss: 0.1443 - mean_absolute_error: 0.1150 - val_loss: 0.1443 - val_mean_absolute_error: 0.1126\n",
      "Epoch 11/30\n",
      "3200/3200 [==============================] - 156s 49ms/step - loss: 0.1441 - mean_absolute_error: 0.1142 - val_loss: 0.1410 - val_mean_absolute_error: 0.1170\n",
      "Epoch 12/30\n",
      "3200/3200 [==============================] - 157s 49ms/step - loss: 0.1424 - mean_absolute_error: 0.1133 - val_loss: 0.1381 - val_mean_absolute_error: 0.1010\n",
      "Epoch 13/30\n",
      "3200/3200 [==============================] - 157s 49ms/step - loss: 0.1411 - mean_absolute_error: 0.1118 - val_loss: 0.1403 - val_mean_absolute_error: 0.0978\n",
      "Epoch 14/30\n",
      "3200/3200 [==============================] - 156s 49ms/step - loss: 0.1420 - mean_absolute_error: 0.1105 - val_loss: 0.1427 - val_mean_absolute_error: 0.1029\n",
      "Epoch 15/30\n",
      "3200/3200 [==============================] - 157s 49ms/step - loss: 0.1417 - mean_absolute_error: 0.1101 - val_loss: 0.1501 - val_mean_absolute_error: 0.1253\n",
      "Epoch 16/30\n",
      "3200/3200 [==============================] - 156s 49ms/step - loss: 0.1426 - mean_absolute_error: 0.1109 - val_loss: 0.1402 - val_mean_absolute_error: 0.1038\n",
      "Epoch 17/30\n",
      "3200/3200 [==============================] - 157s 49ms/step - loss: 0.1422 - mean_absolute_error: 0.1084 - val_loss: 0.1494 - val_mean_absolute_error: 0.1101\n",
      "Epoch 18/30\n",
      "3200/3200 [==============================] - 161s 50ms/step - loss: 0.1435 - mean_absolute_error: 0.1088 - val_loss: 0.1400 - val_mean_absolute_error: 0.1025\n",
      "Epoch 19/30\n",
      "3200/3200 [==============================] - 160s 50ms/step - loss: 0.1416 - mean_absolute_error: 0.1070 - val_loss: 0.1396 - val_mean_absolute_error: 0.0960\n",
      "Epoch 20/30\n",
      "3200/3200 [==============================] - 160s 50ms/step - loss: 0.1421 - mean_absolute_error: 0.1076 - val_loss: 0.1418 - val_mean_absolute_error: 0.1071\n",
      "Epoch 21/30\n",
      "3200/3200 [==============================] - 160s 50ms/step - loss: 0.1411 - mean_absolute_error: 0.1069 - val_loss: 0.1396 - val_mean_absolute_error: 0.1023\n",
      "Epoch 22/30\n",
      "3200/3200 [==============================] - 160s 50ms/step - loss: 0.1427 - mean_absolute_error: 0.1076 - val_loss: 0.1513 - val_mean_absolute_error: 0.1381\n",
      "Epoch 23/30\n",
      "3200/3200 [==============================] - 160s 50ms/step - loss: 0.1425 - mean_absolute_error: 0.1064 - val_loss: 0.1498 - val_mean_absolute_error: 0.1146\n",
      "Epoch 24/30\n",
      "3200/3200 [==============================] - 161s 50ms/step - loss: 0.1424 - mean_absolute_error: 0.1077 - val_loss: 0.1458 - val_mean_absolute_error: 0.1180\n",
      "Epoch 25/30\n",
      "3200/3200 [==============================] - 161s 50ms/step - loss: 0.1406 - mean_absolute_error: 0.1052 - val_loss: 0.1428 - val_mean_absolute_error: 0.1086\n",
      "Epoch 26/30\n",
      "3200/3200 [==============================] - 160s 50ms/step - loss: 0.1406 - mean_absolute_error: 0.1069 - val_loss: 0.1394 - val_mean_absolute_error: 0.1123\n",
      "Epoch 27/30\n",
      "3200/3200 [==============================] - 164s 51ms/step - loss: 0.1416 - mean_absolute_error: 0.1051 - val_loss: 0.1443 - val_mean_absolute_error: 0.1098\n",
      "Epoch 28/30\n",
      "3200/3200 [==============================] - 160s 50ms/step - loss: 0.1423 - mean_absolute_error: 0.1072 - val_loss: 0.1366 - val_mean_absolute_error: 0.0935\n",
      "Epoch 29/30\n",
      "3200/3200 [==============================] - 157s 49ms/step - loss: 0.1424 - mean_absolute_error: 0.1060 - val_loss: 0.1404 - val_mean_absolute_error: 0.0963\n",
      "Epoch 30/30\n",
      "3200/3200 [==============================] - 158s 49ms/step - loss: 0.1420 - mean_absolute_error: 0.1054 - val_loss: 0.1451 - val_mean_absolute_error: 0.1081\n",
      "Exporting plots\n",
      "Exporting CSV with test parameters and best mean absolute error result\n",
      "Exporting Test Data\n",
      "\n",
      "BEGINNING TEST: 8\n",
      "C:\\Users\\aambr\\OneDrive\\Documents\\UNH Fall 2019\\Research\\zernike_polynomials\\Data_and_Results_12-12-19\\test_plan_generator.csv\n",
      "Creating storage directory\n",
      "Creating the model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_85 (Dense)             (None, 2500, 32)          160       \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "flatten_21 (Flatten)         (None, 80000)             0         \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 15)                1200015   \n",
      "=================================================================\n",
      "Total params: 1,204,399\n",
      "Trainable params: 1,204,399\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fitting model\n",
      "SETTINGS FOR THIS TEST:\n",
      "   test_num  layer_num layer_type  num_nodes activation optimizer loss  \\\n",
      "0         8          1      Dense       32.0       relu      Adam  mse   \n",
      "1         8          2      Dense       32.0       relu       NaN  NaN   \n",
      "2         8          3      Dense       32.0       relu       NaN  NaN   \n",
      "3         8          4      Dense       32.0       relu       NaN  NaN   \n",
      "4         8          5      Dense       32.0       relu       NaN  NaN   \n",
      "5         8          6    Flatten        NaN        NaN       NaN  NaN   \n",
      "6         8          7      Dense       15.0        NaN       NaN  NaN   \n",
      "\n",
      "   epochs  batch_size  steps_per_epoch     lr  max_queue_size  \\\n",
      "0    30.0         8.0           3200.0  0.001           320.0   \n",
      "1     NaN         NaN              NaN    NaN             NaN   \n",
      "2     NaN         NaN              NaN    NaN             NaN   \n",
      "3     NaN         NaN              NaN    NaN             NaN   \n",
      "4     NaN         NaN              NaN    NaN             NaN   \n",
      "5     NaN         NaN              NaN    NaN             NaN   \n",
      "6     NaN         NaN              NaN    NaN             NaN   \n",
      "\n",
      "   kernel_regularizer  train_samp_num  test_samp_num  \n",
      "0                 0.0             NaN            NaN  \n",
      "1                 0.0             NaN            NaN  \n",
      "2                 0.0             NaN            NaN  \n",
      "3                 0.0             NaN            NaN  \n",
      "4                 0.0             NaN            NaN  \n",
      "5                 0.0             NaN            NaN  \n",
      "6                 0.0             NaN            NaN   \n",
      "\n",
      "Epoch 1/30\n",
      "3200/3200 [==============================] - 158s 49ms/step - loss: 0.1734 - mean_absolute_error: 0.1689 - val_loss: 0.1475 - val_mean_absolute_error: 0.1488\n",
      "Epoch 2/30\n",
      "3200/3200 [==============================] - 158s 49ms/step - loss: 0.1505 - mean_absolute_error: 0.1411 - val_loss: 0.1476 - val_mean_absolute_error: 0.1404\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3200 [==============================] - 157s 49ms/step - loss: 0.1489 - mean_absolute_error: 0.1376 - val_loss: 0.1419 - val_mean_absolute_error: 0.1233\n",
      "Epoch 4/30\n",
      "3200/3200 [==============================] - 157s 49ms/step - loss: 0.1475 - mean_absolute_error: 0.1317 - val_loss: 0.1504 - val_mean_absolute_error: 0.1442\n",
      "Epoch 5/30\n",
      "3200/3200 [==============================] - 157s 49ms/step - loss: 0.1473 - mean_absolute_error: 0.1291 - val_loss: 0.1418 - val_mean_absolute_error: 0.1248\n",
      "Epoch 6/30\n",
      "3200/3200 [==============================] - 158s 49ms/step - loss: 0.1438 - mean_absolute_error: 0.1244 - val_loss: 0.1464 - val_mean_absolute_error: 0.1265\n",
      "Epoch 7/30\n",
      "3200/3200 [==============================] - 159s 50ms/step - loss: 0.1460 - mean_absolute_error: 0.1230 - val_loss: 0.1444 - val_mean_absolute_error: 0.1261\n",
      "Epoch 8/30\n",
      "3200/3200 [==============================] - 157s 49ms/step - loss: 0.1426 - mean_absolute_error: 0.1198 - val_loss: 0.1506 - val_mean_absolute_error: 0.1191\n",
      "Epoch 9/30\n",
      "3200/3200 [==============================] - 157s 49ms/step - loss: 0.1429 - mean_absolute_error: 0.1189 - val_loss: 0.1371 - val_mean_absolute_error: 0.1082\n",
      "Epoch 10/30\n",
      "3200/3200 [==============================] - 161s 50ms/step - loss: 0.1448 - mean_absolute_error: 0.1191 - val_loss: 0.1422 - val_mean_absolute_error: 0.1159\n",
      "Epoch 11/30\n",
      "3200/3200 [==============================] - 161s 50ms/step - loss: 0.1448 - mean_absolute_error: 0.1185 - val_loss: 0.1457 - val_mean_absolute_error: 0.1186\n",
      "Epoch 12/30\n",
      "3200/3200 [==============================] - 162s 50ms/step - loss: 0.1435 - mean_absolute_error: 0.1162 - val_loss: 0.1414 - val_mean_absolute_error: 0.1105\n",
      "Epoch 13/30\n",
      "3200/3200 [==============================] - 162s 51ms/step - loss: 0.1428 - mean_absolute_error: 0.1152 - val_loss: 0.1404 - val_mean_absolute_error: 0.1083\n",
      "Epoch 14/30\n",
      "3200/3200 [==============================] - 161s 50ms/step - loss: 0.1431 - mean_absolute_error: 0.1149 - val_loss: 0.1433 - val_mean_absolute_error: 0.1198\n",
      "Epoch 15/30\n",
      "3200/3200 [==============================] - 161s 50ms/step - loss: 0.1433 - mean_absolute_error: 0.1138 - val_loss: 0.1390 - val_mean_absolute_error: 0.1067\n",
      "Epoch 16/30\n",
      "3200/3200 [==============================] - 163s 51ms/step - loss: 0.1431 - mean_absolute_error: 0.1131 - val_loss: 0.1399 - val_mean_absolute_error: 0.1092\n",
      "Epoch 17/30\n",
      "3200/3200 [==============================] - 161s 50ms/step - loss: 0.1415 - mean_absolute_error: 0.1119 - val_loss: 0.1441 - val_mean_absolute_error: 0.1177\n",
      "Epoch 18/30\n",
      "3200/3200 [==============================] - 165s 51ms/step - loss: 0.1414 - mean_absolute_error: 0.1107 - val_loss: 0.1417 - val_mean_absolute_error: 0.1075\n",
      "Epoch 19/30\n",
      "3200/3200 [==============================] - 166s 52ms/step - loss: 0.1418 - mean_absolute_error: 0.1094 - val_loss: 0.1395 - val_mean_absolute_error: 0.1024\n",
      "Epoch 20/30\n",
      "3200/3200 [==============================] - 170s 53ms/step - loss: 0.1414 - mean_absolute_error: 0.1089 - val_loss: 0.1396 - val_mean_absolute_error: 0.1102\n",
      "Epoch 21/30\n",
      "3200/3200 [==============================] - 168s 52ms/step - loss: 0.1413 - mean_absolute_error: 0.1088 - val_loss: 0.1379 - val_mean_absolute_error: 0.1050\n",
      "Epoch 22/30\n",
      "3200/3200 [==============================] - 176s 55ms/step - loss: 0.1431 - mean_absolute_error: 0.1095 - val_loss: 0.1424 - val_mean_absolute_error: 0.1128\n",
      "Epoch 23/30\n",
      "3200/3200 [==============================] - 172s 54ms/step - loss: 0.1415 - mean_absolute_error: 0.1087 - val_loss: 0.1434 - val_mean_absolute_error: 0.1079\n",
      "Epoch 24/30\n",
      "3200/3200 [==============================] - 176s 55ms/step - loss: 0.1416 - mean_absolute_error: 0.1076 - val_loss: 0.1446 - val_mean_absolute_error: 0.1061\n",
      "Epoch 25/30\n",
      "3200/3200 [==============================] - 159s 50ms/step - loss: 0.1419 - mean_absolute_error: 0.1092 - val_loss: 0.1448 - val_mean_absolute_error: 0.1097\n",
      "Epoch 26/30\n",
      "3200/3200 [==============================] - 160s 50ms/step - loss: 0.1407 - mean_absolute_error: 0.1078 - val_loss: 0.1442 - val_mean_absolute_error: 0.1026\n",
      "Epoch 27/30\n",
      "3200/3200 [==============================] - 158s 49ms/step - loss: 0.1421 - mean_absolute_error: 0.1089 - val_loss: 0.1387 - val_mean_absolute_error: 0.1036\n",
      "Epoch 28/30\n",
      "3200/3200 [==============================] - 160s 50ms/step - loss: 0.1413 - mean_absolute_error: 0.1091 - val_loss: 0.1419 - val_mean_absolute_error: 0.1130\n",
      "Epoch 29/30\n",
      "3200/3200 [==============================] - 180s 56ms/step - loss: 0.1415 - mean_absolute_error: 0.1082 - val_loss: 0.1441 - val_mean_absolute_error: 0.1099\n",
      "Epoch 30/30\n",
      "3200/3200 [==============================] - 170s 53ms/step - loss: 0.1414 - mean_absolute_error: 0.1081 - val_loss: 0.1396 - val_mean_absolute_error: 0.1067\n",
      "Exporting plots\n",
      "Exporting CSV with test parameters and best mean absolute error result\n",
      "Exporting Test Data\n",
      "\n",
      "BEGINNING TEST: 9\n",
      "C:\\Users\\aambr\\OneDrive\\Documents\\UNH Fall 2019\\Research\\zernike_polynomials\\Data_and_Results_12-12-19\\test_plan_generator.csv\n",
      "Creating storage directory\n",
      "Creating the model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_91 (Dense)             (None, 2500, 32)          160       \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "dense_93 (Dense)             (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "flatten_22 (Flatten)         (None, 80000)             0         \n",
      "_________________________________________________________________\n",
      "dense_94 (Dense)             (None, 15)                1200015   \n",
      "=================================================================\n",
      "Total params: 1,202,287\n",
      "Trainable params: 1,202,287\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fitting model\n",
      "SETTINGS FOR THIS TEST:\n",
      "   test_num  layer_num layer_type  num_nodes activation optimizer loss  \\\n",
      "0         9          1      Dense       32.0       relu      Adam  mse   \n",
      "1         9          2      Dense       32.0       relu       NaN  NaN   \n",
      "2         9          3      Dense       32.0       relu       NaN  NaN   \n",
      "3         9          4    Flatten        NaN        NaN       NaN  NaN   \n",
      "4         9          5      Dense       15.0        NaN       NaN  NaN   \n",
      "\n",
      "   epochs  batch_size  steps_per_epoch     lr  max_queue_size  \\\n",
      "0    30.0         8.0           3200.0  0.001           320.0   \n",
      "1     NaN         NaN              NaN    NaN             NaN   \n",
      "2     NaN         NaN              NaN    NaN             NaN   \n",
      "3     NaN         NaN              NaN    NaN             NaN   \n",
      "4     NaN         NaN              NaN    NaN             NaN   \n",
      "\n",
      "   kernel_regularizer  train_samp_num  test_samp_num  \n",
      "0                0.01             NaN            NaN  \n",
      "1                0.01             NaN            NaN  \n",
      "2                0.01             NaN            NaN  \n",
      "3                0.00             NaN            NaN  \n",
      "4                0.01             NaN            NaN   \n",
      "\n",
      "Epoch 1/30\n",
      "3200/3200 [==============================] - 170s 53ms/step - loss: 0.4682 - mean_absolute_error: 0.2445 - val_loss: 0.3133 - val_mean_absolute_error: 0.2319\n",
      "Epoch 2/30\n",
      "3200/3200 [==============================] - 164s 51ms/step - loss: 0.3126 - mean_absolute_error: 0.2575 - val_loss: 0.2940 - val_mean_absolute_error: 0.2468\n",
      "Epoch 3/30\n",
      "3200/3200 [==============================] - 165s 52ms/step - loss: 0.3120 - mean_absolute_error: 0.2620 - val_loss: 0.3136 - val_mean_absolute_error: 0.2813\n",
      "Epoch 4/30\n",
      "3200/3200 [==============================] - 167s 52ms/step - loss: 0.3084 - mean_absolute_error: 0.2642 - val_loss: 0.3263 - val_mean_absolute_error: 0.2912\n",
      "Epoch 5/30\n",
      "3200/3200 [==============================] - 161s 50ms/step - loss: 0.3083 - mean_absolute_error: 0.2645 - val_loss: 0.3109 - val_mean_absolute_error: 0.2508\n",
      "Epoch 6/30\n",
      "3200/3200 [==============================] - 163s 51ms/step - loss: 0.3045 - mean_absolute_error: 0.2649 - val_loss: 0.3027 - val_mean_absolute_error: 0.2647\n",
      "Epoch 7/30\n",
      "3200/3200 [==============================] - 165s 52ms/step - loss: 0.3028 - mean_absolute_error: 0.2634 - val_loss: 0.2996 - val_mean_absolute_error: 0.2760\n",
      "Epoch 8/30\n",
      "3200/3200 [==============================] - 169s 53ms/step - loss: 0.3029 - mean_absolute_error: 0.2631 - val_loss: 0.2945 - val_mean_absolute_error: 0.2578\n",
      "Epoch 9/30\n",
      "3200/3200 [==============================] - 165s 52ms/step - loss: 0.3010 - mean_absolute_error: 0.2619 - val_loss: 0.3248 - val_mean_absolute_error: 0.2626\n",
      "Epoch 10/30\n",
      "3200/3200 [==============================] - 165s 52ms/step - loss: 0.3034 - mean_absolute_error: 0.2632 - val_loss: 0.3049 - val_mean_absolute_error: 0.2760\n",
      "Epoch 11/30\n",
      "3200/3200 [==============================] - 175s 55ms/step - loss: 0.3000 - mean_absolute_error: 0.2610 - val_loss: 0.2999 - val_mean_absolute_error: 0.2425\n",
      "Epoch 12/30\n",
      "3200/3200 [==============================] - 175s 55ms/step - loss: 0.2994 - mean_absolute_error: 0.2617 - val_loss: 0.2935 - val_mean_absolute_error: 0.2687\n",
      "Epoch 13/30\n",
      "3200/3200 [==============================] - 165s 52ms/step - loss: 0.3007 - mean_absolute_error: 0.2602 - val_loss: 0.3126 - val_mean_absolute_error: 0.2742\n",
      "Epoch 14/30\n",
      "3200/3200 [==============================] - 169s 53ms/step - loss: 0.3009 - mean_absolute_error: 0.2618 - val_loss: 0.3029 - val_mean_absolute_error: 0.2626\n",
      "Epoch 15/30\n",
      "3200/3200 [==============================] - 177s 55ms/step - loss: 0.2998 - mean_absolute_error: 0.2609 - val_loss: 0.3278 - val_mean_absolute_error: 0.2490\n",
      "Epoch 16/30\n",
      "3200/3200 [==============================] - 220s 69ms/step - loss: 0.2994 - mean_absolute_error: 0.2608 - val_loss: 0.2823 - val_mean_absolute_error: 0.2540\n",
      "Epoch 17/30\n",
      "3200/3200 [==============================] - 193s 60ms/step - loss: 0.2977 - mean_absolute_error: 0.2601 - val_loss: 0.2848 - val_mean_absolute_error: 0.2388\n",
      "Epoch 18/30\n",
      "3200/3200 [==============================] - 183s 57ms/step - loss: 0.2979 - mean_absolute_error: 0.2592 - val_loss: 0.2931 - val_mean_absolute_error: 0.2464\n",
      "Epoch 19/30\n",
      "3200/3200 [==============================] - 176s 55ms/step - loss: 0.2979 - mean_absolute_error: 0.2593 - val_loss: 0.2809 - val_mean_absolute_error: 0.2398\n",
      "Epoch 20/30\n",
      "3200/3200 [==============================] - 176s 55ms/step - loss: 0.2963 - mean_absolute_error: 0.2574 - val_loss: 0.2791 - val_mean_absolute_error: 0.2483\n",
      "Epoch 21/30\n",
      "3200/3200 [==============================] - 180s 56ms/step - loss: 0.2989 - mean_absolute_error: 0.2594 - val_loss: 0.3282 - val_mean_absolute_error: 0.2929\n",
      "Epoch 22/30\n",
      "3200/3200 [==============================] - 181s 57ms/step - loss: 0.2993 - mean_absolute_error: 0.2587 - val_loss: 0.3226 - val_mean_absolute_error: 0.3096\n",
      "Epoch 23/30\n",
      "3200/3200 [==============================] - 186s 58ms/step - loss: 0.2963 - mean_absolute_error: 0.2600 - val_loss: 0.2888 - val_mean_absolute_error: 0.2444\n",
      "Epoch 24/30\n",
      "3200/3200 [==============================] - 187s 58ms/step - loss: 0.2928 - mean_absolute_error: 0.2559 - val_loss: 0.3055 - val_mean_absolute_error: 0.2638\n",
      "Epoch 25/30\n",
      "3200/3200 [==============================] - 190s 59ms/step - loss: 0.2974 - mean_absolute_error: 0.2582 - val_loss: 0.3084 - val_mean_absolute_error: 0.2691\n",
      "Epoch 26/30\n",
      "3200/3200 [==============================] - 191s 60ms/step - loss: 0.2973 - mean_absolute_error: 0.2582 - val_loss: 0.3043 - val_mean_absolute_error: 0.2609\n",
      "Epoch 27/30\n",
      "3200/3200 [==============================] - 192s 60ms/step - loss: 0.2951 - mean_absolute_error: 0.2578 - val_loss: 0.3195 - val_mean_absolute_error: 0.2725\n",
      "Epoch 28/30\n",
      "3200/3200 [==============================] - 196s 61ms/step - loss: 0.2961 - mean_absolute_error: 0.2579 - val_loss: 0.2939 - val_mean_absolute_error: 0.2499\n",
      "Epoch 29/30\n",
      "3200/3200 [==============================] - 197s 62ms/step - loss: 0.2951 - mean_absolute_error: 0.2567 - val_loss: 0.2772 - val_mean_absolute_error: 0.2436\n",
      "Epoch 30/30\n",
      "3200/3200 [==============================] - 199s 62ms/step - loss: 0.2956 - mean_absolute_error: 0.2565 - val_loss: 0.3235 - val_mean_absolute_error: 0.3025\n",
      "Exporting plots\n",
      "Exporting CSV with test parameters and best mean absolute error result\n",
      "Exporting Test Data\n",
      "\n",
      "BEGINNING TEST: 10\n",
      "C:\\Users\\aambr\\OneDrive\\Documents\\UNH Fall 2019\\Research\\zernike_polynomials\\Data_and_Results_12-12-19\\test_plan_generator.csv\n",
      "Creating storage directory\n",
      "Creating the model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_95 (Dense)             (None, 2500, 32)          160       \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "flatten_23 (Flatten)         (None, 80000)             0         \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 15)                1200015   \n",
      "=================================================================\n",
      "Total params: 1,202,287\n",
      "Trainable params: 1,202,287\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fitting model\n",
      "SETTINGS FOR THIS TEST:\n",
      "   test_num  layer_num layer_type  num_nodes activation optimizer loss  \\\n",
      "0        10          1      Dense       32.0       relu      Adam  mse   \n",
      "1        10          2      Dense       32.0       relu       NaN  NaN   \n",
      "2        10          3      Dense       32.0       relu       NaN  NaN   \n",
      "3        10          4    Flatten        NaN        NaN       NaN  NaN   \n",
      "4        10          5      Dense       15.0        NaN       NaN  NaN   \n",
      "\n",
      "   epochs  batch_size  steps_per_epoch    lr  max_queue_size  \\\n",
      "0    30.0         8.0           3200.0  0.01           320.0   \n",
      "1     NaN         NaN              NaN   NaN             NaN   \n",
      "2     NaN         NaN              NaN   NaN             NaN   \n",
      "3     NaN         NaN              NaN   NaN             NaN   \n",
      "4     NaN         NaN              NaN   NaN             NaN   \n",
      "\n",
      "   kernel_regularizer  train_samp_num  test_samp_num  \n",
      "0                0.01             NaN            NaN  \n",
      "1                0.01             NaN            NaN  \n",
      "2                0.01             NaN            NaN  \n",
      "3                0.00             NaN            NaN  \n",
      "4                0.01             NaN            NaN   \n",
      "\n",
      "Epoch 1/30\n",
      "3200/3200 [==============================] - 205s 64ms/step - loss: 1.6807 - mean_absolute_error: 0.4815 - val_loss: 0.6840 - val_mean_absolute_error: 0.4435\n",
      "Epoch 2/30\n",
      "3200/3200 [==============================] - 203s 63ms/step - loss: 0.7490 - mean_absolute_error: 0.4939 - val_loss: 0.7188 - val_mean_absolute_error: 0.5105\n",
      "Epoch 3/30\n",
      "3200/3200 [==============================] - 191s 60ms/step - loss: 0.7552 - mean_absolute_error: 0.5306 - val_loss: 0.9120 - val_mean_absolute_error: 0.6066\n",
      "Epoch 4/30\n",
      "3200/3200 [==============================] - 197s 61ms/step - loss: 0.7533 - mean_absolute_error: 0.5324 - val_loss: 0.7416 - val_mean_absolute_error: 0.5416\n",
      "Epoch 5/30\n",
      "3200/3200 [==============================] - 203s 63ms/step - loss: 0.7588 - mean_absolute_error: 0.5357 - val_loss: 0.6906 - val_mean_absolute_error: 0.4989\n",
      "Epoch 6/30\n",
      "3200/3200 [==============================] - 204s 64ms/step - loss: 0.7595 - mean_absolute_error: 0.5350 - val_loss: 0.7845 - val_mean_absolute_error: 0.5490\n",
      "Epoch 7/30\n",
      "3200/3200 [==============================] - 200s 62ms/step - loss: 0.7526 - mean_absolute_error: 0.5315 - val_loss: 0.7645 - val_mean_absolute_error: 0.5316\n",
      "Epoch 8/30\n",
      "3200/3200 [==============================] - 200s 62ms/step - loss: 0.7526 - mean_absolute_error: 0.5306 - val_loss: 0.7095 - val_mean_absolute_error: 0.5186\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3200 [==============================] - 197s 62ms/step - loss: 0.7547 - mean_absolute_error: 0.5312 - val_loss: 0.7891 - val_mean_absolute_error: 0.5457\n",
      "Epoch 10/30\n",
      "3200/3200 [==============================] - 199s 62ms/step - loss: 0.7433 - mean_absolute_error: 0.5264 - val_loss: 0.7208 - val_mean_absolute_error: 0.5157\n",
      "Epoch 11/30\n",
      "3200/3200 [==============================] - 198s 62ms/step - loss: 0.7526 - mean_absolute_error: 0.5309 - val_loss: 0.7303 - val_mean_absolute_error: 0.5278\n",
      "Epoch 12/30\n",
      "3200/3200 [==============================] - 203s 63ms/step - loss: 0.7457 - mean_absolute_error: 0.5287 - val_loss: 0.8447 - val_mean_absolute_error: 0.5805\n",
      "Epoch 13/30\n",
      "3200/3200 [==============================] - 332s 104ms/step - loss: 0.7434 - mean_absolute_error: 0.5270 - val_loss: 0.8317 - val_mean_absolute_error: 0.5701\n",
      "Epoch 14/30\n",
      "3200/3200 [==============================] - 308s 96ms/step - loss: 0.7452 - mean_absolute_error: 0.5280 - val_loss: 0.7998 - val_mean_absolute_error: 0.5567\n",
      "Epoch 15/30\n",
      "3200/3200 [==============================] - 274s 86ms/step - loss: 0.7453 - mean_absolute_error: 0.5284 - val_loss: 0.7017 - val_mean_absolute_error: 0.5039\n",
      "Epoch 16/30\n",
      "3200/3200 [==============================] - 228s 71ms/step - loss: 0.7497 - mean_absolute_error: 0.5291 - val_loss: 0.7357 - val_mean_absolute_error: 0.5257\n",
      "Epoch 17/30\n",
      "3200/3200 [==============================] - 203s 64ms/step - loss: 0.7536 - mean_absolute_error: 0.5312 - val_loss: 0.8190 - val_mean_absolute_error: 0.5847\n",
      "Epoch 18/30\n",
      "3200/3200 [==============================] - 216s 67ms/step - loss: 0.7508 - mean_absolute_error: 0.5291 - val_loss: 0.8128 - val_mean_absolute_error: 0.5466\n",
      "Epoch 19/30\n",
      "3200/3200 [==============================] - 215s 67ms/step - loss: 0.7514 - mean_absolute_error: 0.5301 - val_loss: 0.8267 - val_mean_absolute_error: 0.5654\n",
      "Epoch 20/30\n",
      "3200/3200 [==============================] - 215s 67ms/step - loss: 0.7506 - mean_absolute_error: 0.5282 - val_loss: 0.7447 - val_mean_absolute_error: 0.5234\n",
      "Epoch 21/30\n",
      "3200/3200 [==============================] - 216s 68ms/step - loss: 0.7441 - mean_absolute_error: 0.5277 - val_loss: 0.7474 - val_mean_absolute_error: 0.5323\n",
      "Epoch 22/30\n",
      "3200/3200 [==============================] - 216s 67ms/step - loss: 0.7508 - mean_absolute_error: 0.5290 - val_loss: 0.7739 - val_mean_absolute_error: 0.5314\n",
      "Epoch 23/30\n",
      "3200/3200 [==============================] - 217s 68ms/step - loss: 0.7531 - mean_absolute_error: 0.5295 - val_loss: 0.7019 - val_mean_absolute_error: 0.5123\n",
      "Epoch 24/30\n",
      "3200/3200 [==============================] - 211s 66ms/step - loss: 0.7454 - mean_absolute_error: 0.5277 - val_loss: 0.7321 - val_mean_absolute_error: 0.5103\n",
      "Epoch 25/30\n",
      "3200/3200 [==============================] - 208s 65ms/step - loss: 0.7478 - mean_absolute_error: 0.5275 - val_loss: 0.7708 - val_mean_absolute_error: 0.5347\n",
      "Epoch 26/30\n",
      "3200/3200 [==============================] - 211s 66ms/step - loss: 0.7498 - mean_absolute_error: 0.5299 - val_loss: 0.7882 - val_mean_absolute_error: 0.5216\n",
      "Epoch 27/30\n",
      "3200/3200 [==============================] - 211s 66ms/step - loss: 0.7492 - mean_absolute_error: 0.5279 - val_loss: 0.7933 - val_mean_absolute_error: 0.5547\n",
      "Epoch 28/30\n",
      "3200/3200 [==============================] - 212s 66ms/step - loss: 0.7477 - mean_absolute_error: 0.5286 - val_loss: 0.6709 - val_mean_absolute_error: 0.4798\n",
      "Epoch 29/30\n",
      "3200/3200 [==============================] - 213s 67ms/step - loss: 0.7490 - mean_absolute_error: 0.5297 - val_loss: 0.7574 - val_mean_absolute_error: 0.5303\n",
      "Epoch 30/30\n",
      "3200/3200 [==============================] - 216s 68ms/step - loss: 0.7555 - mean_absolute_error: 0.5311 - val_loss: 0.8153 - val_mean_absolute_error: 0.5573\n",
      "Exporting plots\n",
      "Exporting CSV with test parameters and best mean absolute error result\n",
      "Exporting Test Data\n",
      "\n",
      "BEGINNING TEST: 11\n",
      "C:\\Users\\aambr\\OneDrive\\Documents\\UNH Fall 2019\\Research\\zernike_polynomials\\Data_and_Results_12-12-19\\test_plan_generator.csv\n",
      "Creating storage directory\n",
      "Creating the model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_99 (Dense)             (None, 2500, 32)          160       \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "flatten_24 (Flatten)         (None, 80000)             0         \n",
      "_________________________________________________________________\n",
      "dense_102 (Dense)            (None, 15)                1200015   \n",
      "=================================================================\n",
      "Total params: 1,202,287\n",
      "Trainable params: 1,202,287\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fitting model\n",
      "SETTINGS FOR THIS TEST:\n",
      "   test_num  layer_num layer_type  num_nodes activation optimizer loss  \\\n",
      "0        11          1      Dense       32.0       relu      Adam  mse   \n",
      "1        11          2      Dense       32.0       relu       NaN  NaN   \n",
      "2        11          3      Dense       32.0       relu       NaN  NaN   \n",
      "3        11          4    Flatten        NaN        NaN       NaN  NaN   \n",
      "4        11          5      Dense       15.0        NaN       NaN  NaN   \n",
      "\n",
      "   epochs  batch_size  steps_per_epoch      lr  max_queue_size  \\\n",
      "0    30.0         8.0           3200.0  0.0002           320.0   \n",
      "1     NaN         NaN              NaN     NaN             NaN   \n",
      "2     NaN         NaN              NaN     NaN             NaN   \n",
      "3     NaN         NaN              NaN     NaN             NaN   \n",
      "4     NaN         NaN              NaN     NaN             NaN   \n",
      "\n",
      "   kernel_regularizer  train_samp_num  test_samp_num  \n",
      "0                0.01             NaN            NaN  \n",
      "1                0.01             NaN            NaN  \n",
      "2                0.01             NaN            NaN  \n",
      "3                0.00             NaN            NaN  \n",
      "4                0.00             NaN            NaN   \n",
      "\n",
      "Epoch 1/30\n",
      "3200/3200 [==============================] - 214s 67ms/step - loss: 0.4347 - mean_absolute_error: 0.1409 - val_loss: 0.2319 - val_mean_absolute_error: 0.1111\n",
      "Epoch 2/30\n",
      "3200/3200 [==============================] - 211s 66ms/step - loss: 0.2011 - mean_absolute_error: 0.1138 - val_loss: 0.1843 - val_mean_absolute_error: 0.1148\n",
      "Epoch 3/30\n",
      "3200/3200 [==============================] - 214s 67ms/step - loss: 0.1694 - mean_absolute_error: 0.1099 - val_loss: 0.1690 - val_mean_absolute_error: 0.1081\n",
      "Epoch 4/30\n",
      "3200/3200 [==============================] - 218s 68ms/step - loss: 0.1578 - mean_absolute_error: 0.1074 - val_loss: 0.1603 - val_mean_absolute_error: 0.1083\n",
      "Epoch 5/30\n",
      "3200/3200 [==============================] - 210s 66ms/step - loss: 0.1530 - mean_absolute_error: 0.1056 - val_loss: 0.1550 - val_mean_absolute_error: 0.1072\n",
      "Epoch 6/30\n",
      "3200/3200 [==============================] - 211s 66ms/step - loss: 0.1502 - mean_absolute_error: 0.1043 - val_loss: 0.1488 - val_mean_absolute_error: 0.1013\n",
      "Epoch 7/30\n",
      "3200/3200 [==============================] - 210s 66ms/step - loss: 0.1496 - mean_absolute_error: 0.1038 - val_loss: 0.1467 - val_mean_absolute_error: 0.1054\n",
      "Epoch 8/30\n",
      "3200/3200 [==============================] - 211s 66ms/step - loss: 0.1468 - mean_absolute_error: 0.1028 - val_loss: 0.1499 - val_mean_absolute_error: 0.1049\n",
      "Epoch 9/30\n",
      "3200/3200 [==============================] - 209s 65ms/step - loss: 0.1470 - mean_absolute_error: 0.1026 - val_loss: 0.1418 - val_mean_absolute_error: 0.0987\n",
      "Epoch 10/30\n",
      "3200/3200 [==============================] - 212s 66ms/step - loss: 0.1456 - mean_absolute_error: 0.1016 - val_loss: 0.1437 - val_mean_absolute_error: 0.1021\n",
      "Epoch 11/30\n",
      "3200/3200 [==============================] - 212s 66ms/step - loss: 0.1454 - mean_absolute_error: 0.1015 - val_loss: 0.1453 - val_mean_absolute_error: 0.1025\n",
      "Epoch 12/30\n",
      "3200/3200 [==============================] - 212s 66ms/step - loss: 0.1478 - mean_absolute_error: 0.1019 - val_loss: 0.1455 - val_mean_absolute_error: 0.0992\n",
      "Epoch 13/30\n",
      "3200/3200 [==============================] - 213s 67ms/step - loss: 0.1455 - mean_absolute_error: 0.1005 - val_loss: 0.1471 - val_mean_absolute_error: 0.0990\n",
      "Epoch 14/30\n",
      "3200/3200 [==============================] - 215s 67ms/step - loss: 0.1446 - mean_absolute_error: 0.1000 - val_loss: 0.1457 - val_mean_absolute_error: 0.0973\n",
      "Epoch 15/30\n",
      "3200/3200 [==============================] - 217s 68ms/step - loss: 0.1444 - mean_absolute_error: 0.0999 - val_loss: 0.1446 - val_mean_absolute_error: 0.0974\n",
      "Epoch 16/30\n",
      "3200/3200 [==============================] - 217s 68ms/step - loss: 0.1457 - mean_absolute_error: 0.1002 - val_loss: 0.1430 - val_mean_absolute_error: 0.0962\n",
      "Epoch 17/30\n",
      "3200/3200 [==============================] - 218s 68ms/step - loss: 0.1444 - mean_absolute_error: 0.0997 - val_loss: 0.1462 - val_mean_absolute_error: 0.1008\n",
      "Epoch 18/30\n",
      "3200/3200 [==============================] - 218s 68ms/step - loss: 0.1447 - mean_absolute_error: 0.1001 - val_loss: 0.1422 - val_mean_absolute_error: 0.1032\n",
      "Epoch 19/30\n",
      "3200/3200 [==============================] - 222s 69ms/step - loss: 0.1438 - mean_absolute_error: 0.0998 - val_loss: 0.1412 - val_mean_absolute_error: 0.1027\n",
      "Epoch 20/30\n",
      "3200/3200 [==============================] - 230s 72ms/step - loss: 0.1445 - mean_absolute_error: 0.1004 - val_loss: 0.1443 - val_mean_absolute_error: 0.1009\n",
      "Epoch 21/30\n",
      "3200/3200 [==============================] - 242s 76ms/step - loss: 0.1441 - mean_absolute_error: 0.1001 - val_loss: 0.1419 - val_mean_absolute_error: 0.1062\n",
      "Epoch 22/30\n",
      "3200/3200 [==============================] - 245s 77ms/step - loss: 0.1432 - mean_absolute_error: 0.0998 - val_loss: 0.1404 - val_mean_absolute_error: 0.0959\n",
      "Epoch 23/30\n",
      "3200/3200 [==============================] - 240s 75ms/step - loss: 0.1445 - mean_absolute_error: 0.1003 - val_loss: 0.1444 - val_mean_absolute_error: 0.0968\n",
      "Epoch 24/30\n",
      "3200/3200 [==============================] - 245s 77ms/step - loss: 0.1442 - mean_absolute_error: 0.1000 - val_loss: 0.1469 - val_mean_absolute_error: 0.1071\n",
      "Epoch 25/30\n",
      "3200/3200 [==============================] - 248s 78ms/step - loss: 0.1442 - mean_absolute_error: 0.1006 - val_loss: 0.1442 - val_mean_absolute_error: 0.1005\n",
      "Epoch 26/30\n",
      "3200/3200 [==============================] - 249s 78ms/step - loss: 0.1432 - mean_absolute_error: 0.0998 - val_loss: 0.1461 - val_mean_absolute_error: 0.0989\n",
      "Epoch 27/30\n",
      "3200/3200 [==============================] - 253s 79ms/step - loss: 0.1440 - mean_absolute_error: 0.1003 - val_loss: 0.1474 - val_mean_absolute_error: 0.1017\n",
      "Epoch 28/30\n",
      "3200/3200 [==============================] - 259s 81ms/step - loss: 0.1417 - mean_absolute_error: 0.0994 - val_loss: 0.1404 - val_mean_absolute_error: 0.1060\n",
      "Epoch 29/30\n",
      "3200/3200 [==============================] - 253s 79ms/step - loss: 0.1426 - mean_absolute_error: 0.0992 - val_loss: 0.1485 - val_mean_absolute_error: 0.1005\n",
      "Epoch 30/30\n",
      "3200/3200 [==============================] - 253s 79ms/step - loss: 0.1425 - mean_absolute_error: 0.0993 - val_loss: 0.1440 - val_mean_absolute_error: 0.0975\n",
      "Exporting plots\n",
      "Exporting CSV with test parameters and best mean absolute error result\n",
      "Exporting Test Data\n",
      "\n",
      "BEGINNING TEST: 12\n",
      "C:\\Users\\aambr\\OneDrive\\Documents\\UNH Fall 2019\\Research\\zernike_polynomials\\Data_and_Results_12-12-19\\test_plan_generator.csv\n",
      "Creating storage directory\n",
      "Creating the model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_103 (Dense)            (None, 2500, 32)          160       \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "dense_105 (Dense)            (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "flatten_25 (Flatten)         (None, 80000)             0         \n",
      "_________________________________________________________________\n",
      "dense_106 (Dense)            (None, 15)                1200015   \n",
      "=================================================================\n",
      "Total params: 1,202,287\n",
      "Trainable params: 1,202,287\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fitting model\n",
      "SETTINGS FOR THIS TEST:\n",
      "   test_num  layer_num layer_type  num_nodes activation optimizer loss  \\\n",
      "0        12          1      Dense       32.0       relu      Adam  mse   \n",
      "1        12          2      Dense       32.0       relu       NaN  NaN   \n",
      "2        12          3      Dense       32.0       relu       NaN  NaN   \n",
      "3        12          4    Flatten        NaN        NaN       NaN  NaN   \n",
      "4        12          5      Dense       15.0        NaN       NaN  NaN   \n",
      "\n",
      "   epochs  batch_size  steps_per_epoch      lr  max_queue_size  \\\n",
      "0    30.0         8.0           3200.0  0.0002           320.0   \n",
      "1     NaN         NaN              NaN     NaN             NaN   \n",
      "2     NaN         NaN              NaN     NaN             NaN   \n",
      "3     NaN         NaN              NaN     NaN             NaN   \n",
      "4     NaN         NaN              NaN     NaN             NaN   \n",
      "\n",
      "   kernel_regularizer  train_samp_num  test_samp_num  \n",
      "0               0.001             NaN            NaN  \n",
      "1               0.001             NaN            NaN  \n",
      "2               0.001             NaN            NaN  \n",
      "3               0.000             NaN            NaN  \n",
      "4               0.000             NaN            NaN   \n",
      "\n",
      "Epoch 1/30\n",
      "3200/3200 [==============================] - 265s 83ms/step - loss: 0.2124 - mean_absolute_error: 0.1430 - val_loss: 0.1787 - val_mean_absolute_error: 0.1133\n",
      "Epoch 2/30\n",
      "3200/3200 [==============================] - 259s 81ms/step - loss: 0.1741 - mean_absolute_error: 0.1136 - val_loss: 0.1675 - val_mean_absolute_error: 0.1100\n",
      "Epoch 3/30\n",
      "3200/3200 [==============================] - 351s 110ms/step - loss: 0.1634 - mean_absolute_error: 0.1104 - val_loss: 0.1602 - val_mean_absolute_error: 0.1062\n",
      "Epoch 4/30\n",
      "3200/3200 [==============================] - 256s 80ms/step - loss: 0.1572 - mean_absolute_error: 0.1087 - val_loss: 0.1567 - val_mean_absolute_error: 0.1060\n",
      "Epoch 5/30\n",
      "3200/3200 [==============================] - 255s 80ms/step - loss: 0.1523 - mean_absolute_error: 0.1056 - val_loss: 0.1500 - val_mean_absolute_error: 0.1027\n",
      "Epoch 6/30\n",
      "3200/3200 [==============================] - 255s 80ms/step - loss: 0.1527 - mean_absolute_error: 0.1054 - val_loss: 0.1478 - val_mean_absolute_error: 0.0986\n",
      "Epoch 7/30\n",
      "3200/3200 [==============================] - 254s 79ms/step - loss: 0.1495 - mean_absolute_error: 0.1042 - val_loss: 0.1492 - val_mean_absolute_error: 0.1039\n",
      "Epoch 8/30\n",
      "3200/3200 [==============================] - 256s 80ms/step - loss: 0.1464 - mean_absolute_error: 0.1018 - val_loss: 0.1444 - val_mean_absolute_error: 0.0984\n",
      "Epoch 9/30\n",
      "3200/3200 [==============================] - 260s 81ms/step - loss: 0.1464 - mean_absolute_error: 0.1017 - val_loss: 0.1424 - val_mean_absolute_error: 0.1033\n",
      "Epoch 10/30\n",
      "3200/3200 [==============================] - 254s 80ms/step - loss: 0.1448 - mean_absolute_error: 0.1007 - val_loss: 0.1462 - val_mean_absolute_error: 0.0992\n",
      "Epoch 11/30\n",
      "3200/3200 [==============================] - 254s 79ms/step - loss: 0.1440 - mean_absolute_error: 0.1001 - val_loss: 0.1444 - val_mean_absolute_error: 0.1005\n",
      "Epoch 12/30\n",
      "3200/3200 [==============================] - 254s 79ms/step - loss: 0.1453 - mean_absolute_error: 0.1001 - val_loss: 0.1451 - val_mean_absolute_error: 0.0999\n",
      "Epoch 13/30\n",
      "3200/3200 [==============================] - 255s 80ms/step - loss: 0.1426 - mean_absolute_error: 0.0993 - val_loss: 0.1408 - val_mean_absolute_error: 0.1009\n",
      "Epoch 14/30\n",
      "3200/3200 [==============================] - 259s 81ms/step - loss: 0.1430 - mean_absolute_error: 0.0991 - val_loss: 0.1425 - val_mean_absolute_error: 0.0973\n",
      "Epoch 15/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3200 [==============================] - 261s 82ms/step - loss: 0.1424 - mean_absolute_error: 0.0983 - val_loss: 0.1396 - val_mean_absolute_error: 0.0957\n",
      "Epoch 16/30\n",
      "3200/3200 [==============================] - 260s 81ms/step - loss: 0.1428 - mean_absolute_error: 0.0978 - val_loss: 0.1390 - val_mean_absolute_error: 0.0948\n",
      "Epoch 17/30\n",
      "3200/3200 [==============================] - 265s 83ms/step - loss: 0.1418 - mean_absolute_error: 0.0976 - val_loss: 0.1453 - val_mean_absolute_error: 0.1049\n",
      "Epoch 18/30\n",
      "3200/3200 [==============================] - 265s 83ms/step - loss: 0.1422 - mean_absolute_error: 0.0976 - val_loss: 0.1394 - val_mean_absolute_error: 0.0955\n",
      "Epoch 19/30\n",
      "3200/3200 [==============================] - 265s 83ms/step - loss: 0.1416 - mean_absolute_error: 0.0970 - val_loss: 0.1414 - val_mean_absolute_error: 0.0944\n",
      "Epoch 20/30\n",
      "3200/3200 [==============================] - 265s 83ms/step - loss: 0.1411 - mean_absolute_error: 0.0965 - val_loss: 0.1427 - val_mean_absolute_error: 0.0944\n",
      "Epoch 21/30\n",
      "3200/3200 [==============================] - 283s 88ms/step - loss: 0.1410 - mean_absolute_error: 0.0966 - val_loss: 0.1466 - val_mean_absolute_error: 0.1056\n",
      "Epoch 22/30\n",
      "3200/3200 [==============================] - 289s 90ms/step - loss: 0.1399 - mean_absolute_error: 0.0955 - val_loss: 0.1414 - val_mean_absolute_error: 0.0938\n",
      "Epoch 23/30\n",
      "3200/3200 [==============================] - 287s 90ms/step - loss: 0.1403 - mean_absolute_error: 0.0957 - val_loss: 0.1412 - val_mean_absolute_error: 0.0964\n",
      "Epoch 24/30\n",
      "3200/3200 [==============================] - 276s 86ms/step - loss: 0.1416 - mean_absolute_error: 0.0961 - val_loss: 0.1417 - val_mean_absolute_error: 0.0950\n",
      "Epoch 25/30\n",
      "3200/3200 [==============================] - 279s 87ms/step - loss: 0.1407 - mean_absolute_error: 0.0960 - val_loss: 0.1400 - val_mean_absolute_error: 0.0932\n",
      "Epoch 26/30\n",
      "3200/3200 [==============================] - 286s 89ms/step - loss: 0.1407 - mean_absolute_error: 0.0963 - val_loss: 0.1406 - val_mean_absolute_error: 0.1008\n",
      "Epoch 27/30\n",
      "3200/3200 [==============================] - 283s 88ms/step - loss: 0.1407 - mean_absolute_error: 0.0954 - val_loss: 0.1428 - val_mean_absolute_error: 0.0940\n",
      "Epoch 28/30\n",
      "3200/3200 [==============================] - 289s 90ms/step - loss: 0.1397 - mean_absolute_error: 0.0950 - val_loss: 0.1418 - val_mean_absolute_error: 0.0967\n",
      "Epoch 29/30\n",
      "3200/3200 [==============================] - 284s 89ms/step - loss: 0.1409 - mean_absolute_error: 0.0955 - val_loss: 0.1417 - val_mean_absolute_error: 0.0956\n",
      "Epoch 30/30\n",
      "3200/3200 [==============================] - 287s 90ms/step - loss: 0.1399 - mean_absolute_error: 0.0948 - val_loss: 0.1385 - val_mean_absolute_error: 0.0932\n",
      "Exporting plots\n",
      "Exporting CSV with test parameters and best mean absolute error result\n",
      "Exporting Test Data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_results(1,12,parent_dir,sqr_grid_width=50,gen=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BEGINNING TEST: 13\n",
      "C:\\Users\\aambr\\OneDrive\\Documents\\UNH Fall 2019\\Research\\zernike_polynomials\\Data_and_Results_12-12-19\\test_plan_generator.csv\n",
      "Creating storage directory\n",
      "Creating the model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_107 (Dense)            (None, 2500, 32)          160       \n",
      "_________________________________________________________________\n",
      "dense_108 (Dense)            (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "dense_109 (Dense)            (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "dense_110 (Dense)            (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "flatten_26 (Flatten)         (None, 80000)             0         \n",
      "_________________________________________________________________\n",
      "dense_111 (Dense)            (None, 15)                1200015   \n",
      "=================================================================\n",
      "Total params: 1,203,343\n",
      "Trainable params: 1,203,343\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fitting model\n",
      "SETTINGS FOR THIS TEST:\n",
      "   test_num  layer_num layer_type  num_nodes activation optimizer loss  \\\n",
      "0        13          1      Dense       32.0    sigmoid      Adam  mse   \n",
      "1        13          2      Dense       32.0    sigmoid       NaN  NaN   \n",
      "2        13          3      Dense       32.0    sigmoid       NaN  NaN   \n",
      "3        13          4      Dense       32.0    sigmoid       NaN  NaN   \n",
      "4        13          5    Flatten        NaN        NaN       NaN  NaN   \n",
      "5        13          6      Dense       15.0        NaN       NaN  NaN   \n",
      "\n",
      "   epochs  batch_size  steps_per_epoch      lr  max_queue_size  \\\n",
      "0    30.0         8.0           3200.0  0.0002           320.0   \n",
      "1     NaN         NaN              NaN     NaN             NaN   \n",
      "2     NaN         NaN              NaN     NaN             NaN   \n",
      "3     NaN         NaN              NaN     NaN             NaN   \n",
      "4     NaN         NaN              NaN     NaN             NaN   \n",
      "5     NaN         NaN              NaN     NaN             NaN   \n",
      "\n",
      "   kernel_regularizer  train_samp_num  test_samp_num  \n",
      "0                 0.0             NaN            NaN  \n",
      "1                 0.0             NaN            NaN  \n",
      "2                 0.0             NaN            NaN  \n",
      "3                 0.0             NaN            NaN  \n",
      "4                 0.0             NaN            NaN  \n",
      "5                 0.0             NaN            NaN   \n",
      "\n",
      "Epoch 1/30\n",
      "3200/3200 [==============================] - 327s 102ms/step - loss: 1.1025 - mean_absolute_error: 0.6033 - val_loss: 0.2261 - val_mean_absolute_error: 0.2755\n",
      "Epoch 2/30\n",
      "3200/3200 [==============================] - 342s 107ms/step - loss: 0.2021 - mean_absolute_error: 0.2312 - val_loss: 0.2102 - val_mean_absolute_error: 0.2218\n",
      "Epoch 3/30\n",
      "3200/3200 [==============================] - 321s 100ms/step - loss: 0.1743 - mean_absolute_error: 0.1887 - val_loss: 0.1584 - val_mean_absolute_error: 0.1656\n",
      "Epoch 4/30\n",
      "3200/3200 [==============================] - 334s 105ms/step - loss: 0.1604 - mean_absolute_error: 0.1634 - val_loss: 0.1574 - val_mean_absolute_error: 0.1588\n",
      "Epoch 5/30\n",
      "3200/3200 [==============================] - 331s 103ms/step - loss: 0.1554 - mean_absolute_error: 0.1480 - val_loss: 0.1441 - val_mean_absolute_error: 0.1319\n",
      "Epoch 6/30\n",
      "3200/3200 [==============================] - 327s 102ms/step - loss: 0.1506 - mean_absolute_error: 0.1344 - val_loss: 0.1453 - val_mean_absolute_error: 0.1335\n",
      "Epoch 7/30\n",
      "3200/3200 [==============================] - 323s 101ms/step - loss: 0.1487 - mean_absolute_error: 0.1254 - val_loss: 0.1425 - val_mean_absolute_error: 0.1234\n",
      "Epoch 8/30\n",
      "3200/3200 [==============================] - 311s 97ms/step - loss: 0.1466 - mean_absolute_error: 0.1188 - val_loss: 0.1429 - val_mean_absolute_error: 0.1201\n",
      "Epoch 9/30\n",
      "3200/3200 [==============================] - 345s 108ms/step - loss: 0.1446 - mean_absolute_error: 0.1138 - val_loss: 0.1427 - val_mean_absolute_error: 0.1084\n",
      "Epoch 10/30\n",
      "3200/3200 [==============================] - 315s 98ms/step - loss: 0.1441 - mean_absolute_error: 0.1111 - val_loss: 0.1374 - val_mean_absolute_error: 0.1059\n",
      "Epoch 11/30\n",
      "3200/3200 [==============================] - 318s 99ms/step - loss: 0.1444 - mean_absolute_error: 0.1094 - val_loss: 0.1441 - val_mean_absolute_error: 0.1054\n",
      "Epoch 12/30\n",
      "3200/3200 [==============================] - 318s 99ms/step - loss: 0.1433 - mean_absolute_error: 0.1081 - val_loss: 0.1397 - val_mean_absolute_error: 0.1050\n",
      "Epoch 13/30\n",
      "3200/3200 [==============================] - 311s 97ms/step - loss: 0.1421 - mean_absolute_error: 0.1068 - val_loss: 0.1406 - val_mean_absolute_error: 0.1046\n",
      "Epoch 14/30\n",
      "3200/3200 [==============================] - 310s 97ms/step - loss: 0.1426 - mean_absolute_error: 0.1058 - val_loss: 0.1416 - val_mean_absolute_error: 0.1041\n",
      "Epoch 15/30\n",
      "3200/3200 [==============================] - 322s 101ms/step - loss: 0.1417 - mean_absolute_error: 0.1048 - val_loss: 0.1386 - val_mean_absolute_error: 0.1006\n",
      "Epoch 16/30\n",
      "3200/3200 [==============================] - 310s 97ms/step - loss: 0.1414 - mean_absolute_error: 0.1040 - val_loss: 0.1408 - val_mean_absolute_error: 0.1027\n",
      "Epoch 17/30\n",
      "3200/3200 [==============================] - 309s 96ms/step - loss: 0.1418 - mean_absolute_error: 0.1039 - val_loss: 0.1403 - val_mean_absolute_error: 0.1017\n",
      "Epoch 18/30\n",
      "3200/3200 [==============================] - 368s 115ms/step - loss: 0.1425 - mean_absolute_error: 0.1035 - val_loss: 0.1441 - val_mean_absolute_error: 0.1046\n",
      "Epoch 19/30\n",
      "3200/3200 [==============================] - 314s 98ms/step - loss: 0.1415 - mean_absolute_error: 0.1023 - val_loss: 0.1412 - val_mean_absolute_error: 0.0992\n",
      "Epoch 20/30\n",
      "3200/3200 [==============================] - 308s 96ms/step - loss: 0.1418 - mean_absolute_error: 0.1023 - val_loss: 0.1427 - val_mean_absolute_error: 0.1021\n",
      "Epoch 21/30\n",
      "3200/3200 [==============================] - 368s 115ms/step - loss: 0.1405 - mean_absolute_error: 0.1011 - val_loss: 0.1435 - val_mean_absolute_error: 0.1005\n",
      "Epoch 22/30\n",
      "3200/3200 [==============================] - 370s 116ms/step - loss: 0.1414 - mean_absolute_error: 0.1007 - val_loss: 0.1401 - val_mean_absolute_error: 0.0986\n",
      "Epoch 23/30\n",
      "3200/3200 [==============================] - 370s 116ms/step - loss: 0.1391 - mean_absolute_error: 0.0995 - val_loss: 0.1430 - val_mean_absolute_error: 0.1046\n",
      "Epoch 24/30\n",
      "3200/3200 [==============================] - 325s 101ms/step - loss: 0.1421 - mean_absolute_error: 0.1001 - val_loss: 0.1377 - val_mean_absolute_error: 0.0952\n",
      "Epoch 25/30\n",
      "3200/3200 [==============================] - 315s 98ms/step - loss: 0.1408 - mean_absolute_error: 0.0991 - val_loss: 0.1404 - val_mean_absolute_error: 0.0971\n",
      "Epoch 26/30\n",
      "3200/3200 [==============================] - 315s 98ms/step - loss: 0.1401 - mean_absolute_error: 0.0982 - val_loss: 0.1424 - val_mean_absolute_error: 0.0979\n",
      "Epoch 27/30\n",
      "3200/3200 [==============================] - 313s 98ms/step - loss: 0.1402 - mean_absolute_error: 0.0980 - val_loss: 0.1388 - val_mean_absolute_error: 0.0964\n",
      "Epoch 28/30\n",
      "3200/3200 [==============================] - 316s 99ms/step - loss: 0.1388 - mean_absolute_error: 0.0971 - val_loss: 0.1344 - val_mean_absolute_error: 0.0951\n",
      "Epoch 29/30\n",
      "3200/3200 [==============================] - 313s 98ms/step - loss: 0.1399 - mean_absolute_error: 0.0970 - val_loss: 0.1413 - val_mean_absolute_error: 0.0979\n",
      "Epoch 30/30\n",
      "3200/3200 [==============================] - 314s 98ms/step - loss: 0.1404 - mean_absolute_error: 0.0972 - val_loss: 0.1402 - val_mean_absolute_error: 0.0959\n",
      "Exporting plots\n",
      "Exporting CSV with test parameters and best mean absolute error result\n",
      "Exporting Test Data\n",
      "\n",
      "BEGINNING TEST: 14\n",
      "C:\\Users\\aambr\\OneDrive\\Documents\\UNH Fall 2019\\Research\\zernike_polynomials\\Data_and_Results_12-12-19\\test_plan_generator.csv\n",
      "Creating storage directory\n",
      "Creating the model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_112 (Dense)            (None, 2500, 32)          160       \n",
      "_________________________________________________________________\n",
      "dense_113 (Dense)            (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "dense_114 (Dense)            (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "flatten_27 (Flatten)         (None, 80000)             0         \n",
      "_________________________________________________________________\n",
      "dense_115 (Dense)            (None, 15)                1200015   \n",
      "=================================================================\n",
      "Total params: 1,202,287\n",
      "Trainable params: 1,202,287\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fitting model\n",
      "SETTINGS FOR THIS TEST:\n",
      "   test_num  layer_num layer_type  num_nodes activation optimizer loss  \\\n",
      "0        14          1      Dense       32.0    sigmoid      Adam  mse   \n",
      "1        14          2      Dense       32.0    sigmoid       NaN  NaN   \n",
      "2        14          3      Dense       32.0    sigmoid       NaN  NaN   \n",
      "3        14          4    Flatten        NaN        NaN       NaN  NaN   \n",
      "4        14          5      Dense       15.0        NaN       NaN  NaN   \n",
      "\n",
      "   epochs  batch_size  steps_per_epoch      lr  max_queue_size  \\\n",
      "0    30.0         8.0           3200.0  0.0002           320.0   \n",
      "1     NaN         NaN              NaN     NaN             NaN   \n",
      "2     NaN         NaN              NaN     NaN             NaN   \n",
      "3     NaN         NaN              NaN     NaN             NaN   \n",
      "4     NaN         NaN              NaN     NaN             NaN   \n",
      "\n",
      "   kernel_regularizer  train_samp_num  test_samp_num  \n",
      "0                 0.0             NaN            NaN  \n",
      "1                 0.0             NaN            NaN  \n",
      "2                 0.0             NaN            NaN  \n",
      "3                 0.0             NaN            NaN  \n",
      "4                 0.0             NaN            NaN   \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "3200/3200 [==============================] - 289s 90ms/step - loss: 0.7294 - mean_absolute_error: 0.4644 - val_loss: 0.1795 - val_mean_absolute_error: 0.2082\n",
      "Epoch 2/30\n",
      "3200/3200 [==============================] - 287s 90ms/step - loss: 0.1884 - mean_absolute_error: 0.1971 - val_loss: 0.1645 - val_mean_absolute_error: 0.1819\n",
      "Epoch 3/30\n",
      "3200/3200 [==============================] - 287s 90ms/step - loss: 0.1664 - mean_absolute_error: 0.1652 - val_loss: 0.1834 - val_mean_absolute_error: 0.1631\n",
      "Epoch 4/30\n",
      "3200/3200 [==============================] - 288s 90ms/step - loss: 0.1565 - mean_absolute_error: 0.1439 - val_loss: 0.1454 - val_mean_absolute_error: 0.1325\n",
      "Epoch 5/30\n",
      "3200/3200 [==============================] - 287s 90ms/step - loss: 0.1494 - mean_absolute_error: 0.1288 - val_loss: 0.1388 - val_mean_absolute_error: 0.1275\n",
      "Epoch 6/30\n",
      "3200/3200 [==============================] - 288s 90ms/step - loss: 0.1489 - mean_absolute_error: 0.1197 - val_loss: 0.1546 - val_mean_absolute_error: 0.1177\n",
      "Epoch 7/30\n",
      "3200/3200 [==============================] - 295s 92ms/step - loss: 0.1448 - mean_absolute_error: 0.1121 - val_loss: 0.1366 - val_mean_absolute_error: 0.1110\n",
      "Epoch 8/30\n",
      "3200/3200 [==============================] - 292s 91ms/step - loss: 0.1436 - mean_absolute_error: 0.1086 - val_loss: 0.1439 - val_mean_absolute_error: 0.1036\n",
      "Epoch 9/30\n",
      "3200/3200 [==============================] - 292s 91ms/step - loss: 0.1436 - mean_absolute_error: 0.1070 - val_loss: 0.1407 - val_mean_absolute_error: 0.1042\n",
      "Epoch 10/30\n",
      "3200/3200 [==============================] - 292s 91ms/step - loss: 0.1419 - mean_absolute_error: 0.1052 - val_loss: 0.1383 - val_mean_absolute_error: 0.1040\n",
      "Epoch 11/30\n",
      "3200/3200 [==============================] - 292s 91ms/step - loss: 0.1424 - mean_absolute_error: 0.1045 - val_loss: 0.1391 - val_mean_absolute_error: 0.1013\n",
      "Epoch 12/30\n",
      "3200/3200 [==============================] - 291s 91ms/step - loss: 0.1419 - mean_absolute_error: 0.1035 - val_loss: 0.1376 - val_mean_absolute_error: 0.1019\n",
      "Epoch 13/30\n",
      "3200/3200 [==============================] - 300s 94ms/step - loss: 0.1414 - mean_absolute_error: 0.1030 - val_loss: 0.1370 - val_mean_absolute_error: 0.0998\n",
      "Epoch 14/30\n",
      "3200/3200 [==============================] - 294s 92ms/step - loss: 0.1415 - mean_absolute_error: 0.1024 - val_loss: 0.1425 - val_mean_absolute_error: 0.1006\n",
      "Epoch 15/30\n",
      "3200/3200 [==============================] - 296s 92ms/step - loss: 0.1413 - mean_absolute_error: 0.1018 - val_loss: 0.1395 - val_mean_absolute_error: 0.1021\n",
      "Epoch 16/30\n",
      "3200/3200 [==============================] - 326s 102ms/step - loss: 0.1399 - mean_absolute_error: 0.1007 - val_loss: 0.1449 - val_mean_absolute_error: 0.1002\n",
      "Epoch 17/30\n",
      "3200/3200 [==============================] - 310s 97ms/step - loss: 0.1409 - mean_absolute_error: 0.1003 - val_loss: 0.1442 - val_mean_absolute_error: 0.1019\n",
      "Epoch 18/30\n",
      "3200/3200 [==============================] - 303s 95ms/step - loss: 0.1405 - mean_absolute_error: 0.0996 - val_loss: 0.1361 - val_mean_absolute_error: 0.0981\n",
      "Epoch 19/30\n",
      "3200/3200 [==============================] - 330s 103ms/step - loss: 0.1406 - mean_absolute_error: 0.0993 - val_loss: 0.1397 - val_mean_absolute_error: 0.1006\n",
      "Epoch 20/30\n",
      "3200/3200 [==============================] - 321s 100ms/step - loss: 0.1418 - mean_absolute_error: 0.0991 - val_loss: 0.1426 - val_mean_absolute_error: 0.1020\n",
      "Epoch 21/30\n",
      "3200/3200 [==============================] - 322s 101ms/step - loss: 0.1411 - mean_absolute_error: 0.0983 - val_loss: 0.1393 - val_mean_absolute_error: 0.0958\n",
      "Epoch 22/30\n",
      "3200/3200 [==============================] - 325s 101ms/step - loss: 0.1395 - mean_absolute_error: 0.0973 - val_loss: 0.1383 - val_mean_absolute_error: 0.0979\n",
      "Epoch 23/30\n",
      "3200/3200 [==============================] - 293s 91ms/step - loss: 0.1408 - mean_absolute_error: 0.0973 - val_loss: 0.1407 - val_mean_absolute_error: 0.0962\n",
      "Epoch 24/30\n",
      "3200/3200 [==============================] - 291s 91ms/step - loss: 0.1394 - mean_absolute_error: 0.0961 - val_loss: 0.1420 - val_mean_absolute_error: 0.0983\n",
      "Epoch 25/30\n",
      "3200/3200 [==============================] - 291s 91ms/step - loss: 0.1395 - mean_absolute_error: 0.0958 - val_loss: 0.1395 - val_mean_absolute_error: 0.0936\n",
      "Epoch 26/30\n",
      "3200/3200 [==============================] - 291s 91ms/step - loss: 0.1409 - mean_absolute_error: 0.0958 - val_loss: 0.1380 - val_mean_absolute_error: 0.0932\n",
      "Epoch 27/30\n",
      "3200/3200 [==============================] - 290s 91ms/step - loss: 0.1407 - mean_absolute_error: 0.0951 - val_loss: 0.1381 - val_mean_absolute_error: 0.0960\n",
      "Epoch 28/30\n",
      "3200/3200 [==============================] - 292s 91ms/step - loss: 0.1408 - mean_absolute_error: 0.0951 - val_loss: 0.1380 - val_mean_absolute_error: 0.0927\n",
      "Epoch 29/30\n",
      "3200/3200 [==============================] - 290s 90ms/step - loss: 0.1405 - mean_absolute_error: 0.0943 - val_loss: 0.1420 - val_mean_absolute_error: 0.0984\n",
      "Epoch 30/30\n",
      "3200/3200 [==============================] - 292s 91ms/step - loss: 0.1401 - mean_absolute_error: 0.0940 - val_loss: 0.1360 - val_mean_absolute_error: 0.0930\n",
      "Exporting plots\n",
      "Exporting CSV with test parameters and best mean absolute error result\n",
      "Exporting Test Data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_results(13,14,parent_dir,sqr_grid_width=50,gen=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BEGINNING TEST: 15\n",
      "C:\\Users\\aambr\\OneDrive\\Documents\\UNH Fall 2019\\Research\\zernike_polynomials\\Data_and_Results_12-12-19\\test_plan_generator.csv\n",
      "Creating storage directory\n",
      "Creating the model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_116 (Dense)            (None, 2500, 32)          160       \n",
      "_________________________________________________________________\n",
      "dense_117 (Dense)            (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "dense_118 (Dense)            (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "flatten_28 (Flatten)         (None, 80000)             0         \n",
      "_________________________________________________________________\n",
      "dense_119 (Dense)            (None, 15)                1200015   \n",
      "=================================================================\n",
      "Total params: 1,202,287\n",
      "Trainable params: 1,202,287\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fitting model\n",
      "SETTINGS FOR THIS TEST:\n",
      "   test_num  layer_num layer_type  num_nodes activation optimizer loss  \\\n",
      "0        15          1      Dense       32.0       tanh      Adam  mse   \n",
      "1        15          2      Dense       32.0       tanh       NaN  NaN   \n",
      "2        15          3      Dense       32.0       tanh       NaN  NaN   \n",
      "3        15          4    Flatten        NaN        NaN       NaN  NaN   \n",
      "4        15          5      Dense       15.0        NaN       NaN  NaN   \n",
      "\n",
      "   epochs  batch_size  steps_per_epoch      lr  max_queue_size  \\\n",
      "0    30.0         8.0           3200.0  0.0002           320.0   \n",
      "1     NaN         NaN              NaN     NaN             NaN   \n",
      "2     NaN         NaN              NaN     NaN             NaN   \n",
      "3     NaN         NaN              NaN     NaN             NaN   \n",
      "4     NaN         NaN              NaN     NaN             NaN   \n",
      "\n",
      "   kernel_regularizer  train_samp_num  test_samp_num  \n",
      "0                 0.0             NaN            NaN  \n",
      "1                 0.0             NaN            NaN  \n",
      "2                 0.0             NaN            NaN  \n",
      "3                 0.0             NaN            NaN  \n",
      "4                 0.0             NaN            NaN   \n",
      "\n",
      "Epoch 1/30\n",
      "3200/3200 [==============================] - 294s 92ms/step - loss: 0.1685 - mean_absolute_error: 0.1400 - val_loss: 0.1527 - val_mean_absolute_error: 0.1267\n",
      "Epoch 2/30\n",
      "3200/3200 [==============================] - 292s 91ms/step - loss: 0.1484 - mean_absolute_error: 0.1188 - val_loss: 0.1595 - val_mean_absolute_error: 0.1213\n",
      "Epoch 3/30\n",
      "3200/3200 [==============================] - 292s 91ms/step - loss: 0.1464 - mean_absolute_error: 0.1219 - val_loss: 0.1488 - val_mean_absolute_error: 0.1231\n",
      "Epoch 4/30\n",
      "3200/3200 [==============================] - 291s 91ms/step - loss: 0.1461 - mean_absolute_error: 0.1202 - val_loss: 0.1466 - val_mean_absolute_error: 0.1229\n",
      "Epoch 5/30\n",
      "3200/3200 [==============================] - 292s 91ms/step - loss: 0.1457 - mean_absolute_error: 0.1192 - val_loss: 0.1445 - val_mean_absolute_error: 0.1187\n",
      "Epoch 6/30\n",
      "3200/3200 [==============================] - 291s 91ms/step - loss: 0.1447 - mean_absolute_error: 0.1182 - val_loss: 0.1434 - val_mean_absolute_error: 0.1168\n",
      "Epoch 7/30\n",
      "3200/3200 [==============================] - 292s 91ms/step - loss: 0.1444 - mean_absolute_error: 0.1170 - val_loss: 0.1406 - val_mean_absolute_error: 0.1097\n",
      "Epoch 8/30\n",
      "3200/3200 [==============================] - 291s 91ms/step - loss: 0.1429 - mean_absolute_error: 0.1162 - val_loss: 0.1437 - val_mean_absolute_error: 0.1133\n",
      "Epoch 9/30\n",
      "3200/3200 [==============================] - 287s 90ms/step - loss: 0.1419 - mean_absolute_error: 0.1145 - val_loss: 0.1416 - val_mean_absolute_error: 0.1061\n",
      "Epoch 10/30\n",
      "3200/3200 [==============================] - 289s 90ms/step - loss: 0.1435 - mean_absolute_error: 0.1144 - val_loss: 0.1452 - val_mean_absolute_error: 0.1184\n",
      "Epoch 11/30\n",
      "3200/3200 [==============================] - 287s 90ms/step - loss: 0.1429 - mean_absolute_error: 0.1147 - val_loss: 0.1473 - val_mean_absolute_error: 0.1226\n",
      "Epoch 12/30\n",
      "3200/3200 [==============================] - 287s 90ms/step - loss: 0.1439 - mean_absolute_error: 0.1144 - val_loss: 0.1477 - val_mean_absolute_error: 0.1206\n",
      "Epoch 13/30\n",
      "3200/3200 [==============================] - 288s 90ms/step - loss: 0.1423 - mean_absolute_error: 0.1128 - val_loss: 0.1412 - val_mean_absolute_error: 0.1209\n",
      "Epoch 14/30\n",
      "3200/3200 [==============================] - 285s 89ms/step - loss: 0.1415 - mean_absolute_error: 0.1117 - val_loss: 0.1432 - val_mean_absolute_error: 0.1175\n",
      "Epoch 15/30\n",
      "3200/3200 [==============================] - 286s 89ms/step - loss: 0.1430 - mean_absolute_error: 0.1122 - val_loss: 0.1413 - val_mean_absolute_error: 0.1166\n",
      "Epoch 16/30\n",
      "3200/3200 [==============================] - 286s 89ms/step - loss: 0.1428 - mean_absolute_error: 0.1114 - val_loss: 0.1460 - val_mean_absolute_error: 0.1250\n",
      "Epoch 17/30\n",
      "3200/3200 [==============================] - 286s 89ms/step - loss: 0.1418 - mean_absolute_error: 0.1121 - val_loss: 0.1431 - val_mean_absolute_error: 0.1081\n",
      "Epoch 18/30\n",
      "3200/3200 [==============================] - 286s 89ms/step - loss: 0.1432 - mean_absolute_error: 0.1108 - val_loss: 0.1407 - val_mean_absolute_error: 0.1072\n",
      "Epoch 19/30\n",
      "3200/3200 [==============================] - 285s 89ms/step - loss: 0.1415 - mean_absolute_error: 0.1095 - val_loss: 0.1411 - val_mean_absolute_error: 0.1073\n",
      "Epoch 20/30\n",
      "3200/3200 [==============================] - 286s 89ms/step - loss: 0.1421 - mean_absolute_error: 0.1091 - val_loss: 0.1412 - val_mean_absolute_error: 0.1081\n",
      "Epoch 21/30\n",
      "3200/3200 [==============================] - 285s 89ms/step - loss: 0.1410 - mean_absolute_error: 0.1085 - val_loss: 0.1454 - val_mean_absolute_error: 0.1098\n",
      "Epoch 22/30\n",
      "3200/3200 [==============================] - 290s 90ms/step - loss: 0.1418 - mean_absolute_error: 0.1088 - val_loss: 0.1419 - val_mean_absolute_error: 0.1100\n",
      "Epoch 23/30\n",
      "3200/3200 [==============================] - 290s 91ms/step - loss: 0.1423 - mean_absolute_error: 0.1084 - val_loss: 0.1460 - val_mean_absolute_error: 0.1150\n",
      "Epoch 24/30\n",
      "3200/3200 [==============================] - 289s 90ms/step - loss: 0.1408 - mean_absolute_error: 0.1076 - val_loss: 0.1415 - val_mean_absolute_error: 0.1115\n",
      "Epoch 25/30\n",
      "3200/3200 [==============================] - 290s 91ms/step - loss: 0.1418 - mean_absolute_error: 0.1074 - val_loss: 0.1483 - val_mean_absolute_error: 0.1090\n",
      "Epoch 26/30\n",
      "3200/3200 [==============================] - 291s 91ms/step - loss: 0.1413 - mean_absolute_error: 0.1075 - val_loss: 0.1459 - val_mean_absolute_error: 0.1095\n",
      "Epoch 27/30\n",
      "3200/3200 [==============================] - 290s 91ms/step - loss: 0.1426 - mean_absolute_error: 0.1073 - val_loss: 0.1427 - val_mean_absolute_error: 0.1042\n",
      "Epoch 28/30\n",
      "3200/3200 [==============================] - 287s 90ms/step - loss: 0.1416 - mean_absolute_error: 0.1058 - val_loss: 0.1418 - val_mean_absolute_error: 0.1067\n",
      "Epoch 29/30\n",
      "3200/3200 [==============================] - 285s 89ms/step - loss: 0.1421 - mean_absolute_error: 0.1066 - val_loss: 0.1415 - val_mean_absolute_error: 0.0999\n",
      "Epoch 30/30\n",
      "3200/3200 [==============================] - 286s 89ms/step - loss: 0.1408 - mean_absolute_error: 0.1052 - val_loss: 0.1380 - val_mean_absolute_error: 0.1046\n",
      "Exporting plots\n",
      "Exporting CSV with test parameters and best mean absolute error result\n",
      "Exporting Test Data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_results(15,15,parent_dir,sqr_grid_width=50,gen=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1214 14:31:34.442872 22832 deprecation_wrapper.py:119] From C:\\Users\\aambr\\AppData\\Local\\Continuum\\anaconda3\\envs\\zern_poly_2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1214 14:31:34.455863 22832 deprecation_wrapper.py:119] From C:\\Users\\aambr\\AppData\\Local\\Continuum\\anaconda3\\envs\\zern_poly_2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1214 14:31:34.526648 22832 deprecation_wrapper.py:119] From C:\\Users\\aambr\\AppData\\Local\\Continuum\\anaconda3\\envs\\zern_poly_2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W1214 14:31:34.527652 22832 deprecation_wrapper.py:119] From C:\\Users\\aambr\\AppData\\Local\\Continuum\\anaconda3\\envs\\zern_poly_2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W1214 14:31:34.527652 22832 deprecation_wrapper.py:119] From C:\\Users\\aambr\\AppData\\Local\\Continuum\\anaconda3\\envs\\zern_poly_2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W1214 14:31:34.597485 22832 deprecation_wrapper.py:119] From C:\\Users\\aambr\\AppData\\Local\\Continuum\\anaconda3\\envs\\zern_poly_2\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BEGINNING TEST: 6\n",
      "C:\\Users\\aambr\\OneDrive\\Documents\\UNH Fall 2019\\Research\\zernike_polynomials\\Data_and_Results_12-12-19\\test_plan_generator.csv\n",
      "Creating storage directory\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_75 (Dense)             (None, 2500, 32)          160       \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 80000)             0         \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 15)                1200015   \n",
      "=================================================================\n",
      "Total params: 1,202,287\n",
      "Trainable params: 1,202,287\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fitting model\n",
      "SETTINGS FOR THIS TEST:\n",
      "   test_num  layer_num layer_type  num_nodes activation optimizer loss  \\\n",
      "0         6          1      Dense       32.0    sigmoid      Adam  mse   \n",
      "1         6          2      Dense       32.0    sigmoid       NaN  NaN   \n",
      "2         6          3      Dense       32.0    sigmoid       NaN  NaN   \n",
      "3         6          4    Flatten        NaN        NaN       NaN  NaN   \n",
      "4         6          5      Dense       15.0        NaN       NaN  NaN   \n",
      "\n",
      "   epochs  batch_size  steps_per_epoch     lr  max_queue_size  \\\n",
      "0    30.0         8.0           3200.0  0.001           320.0   \n",
      "1     NaN         NaN              NaN    NaN             NaN   \n",
      "2     NaN         NaN              NaN    NaN             NaN   \n",
      "3     NaN         NaN              NaN    NaN             NaN   \n",
      "4     NaN         NaN              NaN    NaN             NaN   \n",
      "\n",
      "   kernel_regularizer  \n",
      "0                 0.0  \n",
      "1                 0.0  \n",
      "2                 0.0  \n",
      "3                 0.0  \n",
      "4                 0.0   \n",
      "\n",
      "Epoch 1/30\n",
      "3200/3200 [==============================] - 286s 89ms/step - loss: 0.1461 - mean_absolute_error: 0.1301 - val_loss: 0.1473 - val_mean_absolute_error: 0.1226\n",
      "Epoch 2/30\n",
      "3200/3200 [==============================] - 220s 69ms/step - loss: 0.1450 - mean_absolute_error: 0.1268 - val_loss: 0.1402 - val_mean_absolute_error: 0.1232\n",
      "Epoch 3/30\n",
      "3200/3200 [==============================] - 214s 67ms/step - loss: 0.1444 - mean_absolute_error: 0.1248 - val_loss: 0.1415 - val_mean_absolute_error: 0.1274\n",
      "Epoch 4/30\n",
      "3200/3200 [==============================] - 210s 65ms/step - loss: 0.1431 - mean_absolute_error: 0.1234 - val_loss: 0.1434 - val_mean_absolute_error: 0.1217\n",
      "Epoch 5/30\n",
      "3200/3200 [==============================] - 209s 65ms/step - loss: 0.1434 - mean_absolute_error: 0.1223 - val_loss: 0.1456 - val_mean_absolute_error: 0.1280\n",
      "Epoch 6/30\n",
      "3200/3200 [==============================] - 213s 67ms/step - loss: 0.1440 - mean_absolute_error: 0.1217 - val_loss: 0.1445 - val_mean_absolute_error: 0.1217\n",
      "Epoch 7/30\n",
      "3200/3200 [==============================] - 218s 68ms/step - loss: 0.1414 - mean_absolute_error: 0.1195 - val_loss: 0.1420 - val_mean_absolute_error: 0.1176\n",
      "Epoch 8/30\n",
      "3200/3200 [==============================] - 213s 67ms/step - loss: 0.1433 - mean_absolute_error: 0.1201 - val_loss: 0.1435 - val_mean_absolute_error: 0.1245\n",
      "Epoch 9/30\n",
      "3200/3200 [==============================] - 210s 66ms/step - loss: 0.1433 - mean_absolute_error: 0.1193 - val_loss: 0.1401 - val_mean_absolute_error: 0.1182\n",
      "Epoch 10/30\n",
      "3200/3200 [==============================] - 217s 68ms/step - loss: 0.1426 - mean_absolute_error: 0.1187 - val_loss: 0.1465 - val_mean_absolute_error: 0.1217\n",
      "Epoch 11/30\n",
      "3200/3200 [==============================] - 213s 66ms/step - loss: 0.1440 - mean_absolute_error: 0.1188 - val_loss: 0.1443 - val_mean_absolute_error: 0.1192\n",
      "Epoch 12/30\n",
      "3200/3200 [==============================] - 212s 66ms/step - loss: 0.1417 - mean_absolute_error: 0.1171 - val_loss: 0.1429 - val_mean_absolute_error: 0.1168\n",
      "Epoch 13/30\n",
      "3200/3200 [==============================] - 211s 66ms/step - loss: 0.1410 - mean_absolute_error: 0.1164 - val_loss: 0.1420 - val_mean_absolute_error: 0.1125\n",
      "Epoch 14/30\n",
      "3200/3200 [==============================] - 209s 65ms/step - loss: 0.1415 - mean_absolute_error: 0.1163 - val_loss: 0.1406 - val_mean_absolute_error: 0.1150\n",
      "Epoch 15/30\n",
      "3200/3200 [==============================] - 206s 65ms/step - loss: 0.1426 - mean_absolute_error: 0.1164 - val_loss: 0.1403 - val_mean_absolute_error: 0.1167\n",
      "Epoch 16/30\n",
      "3200/3200 [==============================] - 205s 64ms/step - loss: 0.1427 - mean_absolute_error: 0.1163 - val_loss: 0.1425 - val_mean_absolute_error: 0.1119\n",
      "Epoch 17/30\n",
      "3200/3200 [==============================] - 206s 64ms/step - loss: 0.1423 - mean_absolute_error: 0.1158 - val_loss: 0.1406 - val_mean_absolute_error: 0.1129\n",
      "Epoch 18/30\n",
      "3200/3200 [==============================] - 207s 65ms/step - loss: 0.1416 - mean_absolute_error: 0.1155 - val_loss: 0.1396 - val_mean_absolute_error: 0.1137\n",
      "Epoch 19/30\n",
      "3200/3200 [==============================] - 207s 65ms/step - loss: 0.1418 - mean_absolute_error: 0.1151 - val_loss: 0.1482 - val_mean_absolute_error: 0.1157\n",
      "Epoch 20/30\n",
      "3200/3200 [==============================] - 206s 64ms/step - loss: 0.1415 - mean_absolute_error: 0.1148 - val_loss: 0.1419 - val_mean_absolute_error: 0.1159\n",
      "Epoch 21/30\n",
      "3200/3200 [==============================] - 207s 65ms/step - loss: 0.1442 - mean_absolute_error: 0.1155 - val_loss: 0.1443 - val_mean_absolute_error: 0.1115\n",
      "Epoch 22/30\n",
      "3200/3200 [==============================] - 206s 64ms/step - loss: 0.1423 - mean_absolute_error: 0.1149 - val_loss: 0.1389 - val_mean_absolute_error: 0.1152\n",
      "Epoch 23/30\n",
      "3200/3200 [==============================] - 206s 64ms/step - loss: 0.1423 - mean_absolute_error: 0.1148 - val_loss: 0.1441 - val_mean_absolute_error: 0.1228\n",
      "Epoch 24/30\n",
      "3200/3200 [==============================] - 205s 64ms/step - loss: 0.1426 - mean_absolute_error: 0.1144 - val_loss: 0.1422 - val_mean_absolute_error: 0.1122\n",
      "Epoch 25/30\n",
      "3200/3200 [==============================] - 207s 65ms/step - loss: 0.1412 - mean_absolute_error: 0.1135 - val_loss: 0.1424 - val_mean_absolute_error: 0.1111\n",
      "Epoch 26/30\n",
      "3200/3200 [==============================] - 206s 64ms/step - loss: 0.1424 - mean_absolute_error: 0.1143 - val_loss: 0.1395 - val_mean_absolute_error: 0.1121\n",
      "Epoch 27/30\n",
      "3200/3200 [==============================] - 206s 64ms/step - loss: 0.1417 - mean_absolute_error: 0.1142 - val_loss: 0.1377 - val_mean_absolute_error: 0.1097\n",
      "Epoch 28/30\n",
      "3200/3200 [==============================] - 206s 64ms/step - loss: 0.1427 - mean_absolute_error: 0.1138 - val_loss: 0.1446 - val_mean_absolute_error: 0.1173\n",
      "Epoch 29/30\n",
      "3200/3200 [==============================] - 206s 65ms/step - loss: 0.1412 - mean_absolute_error: 0.1131 - val_loss: 0.1414 - val_mean_absolute_error: 0.1106\n",
      "Epoch 30/30\n",
      "3200/3200 [==============================] - 210s 66ms/step - loss: 0.1429 - mean_absolute_error: 0.1137 - val_loss: 0.1435 - val_mean_absolute_error: 0.1222\n",
      "Exporting plots\n",
      "Exporting CSV with test parameters and best mean absolute error result\n",
      "Exporting Test Data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_results_blocknoise(6,6,parent_dir,sqr_grid_width=50,load=True,load_name_append='_Perfect',name_append='_BlockNoise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BEGINNING TEST: 6\n",
      "C:\\Users\\aambr\\OneDrive\\Documents\\UNH Fall 2019\\Research\\zernike_polynomials\\Data_and_Results_12-12-19\\test_plan_generator.csv\n",
      "Creating storage directory\n",
      "Creating the model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 2500, 32)          160       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 80000)             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 15)                1200015   \n",
      "=================================================================\n",
      "Total params: 1,202,287\n",
      "Trainable params: 1,202,287\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fitting model\n",
      "SETTINGS FOR THIS TEST:\n",
      "   test_num  layer_num layer_type  num_nodes activation optimizer loss  \\\n",
      "0         6          1      Dense       32.0    sigmoid      Adam  mse   \n",
      "1         6          2      Dense       32.0    sigmoid       NaN  NaN   \n",
      "2         6          3      Dense       32.0    sigmoid       NaN  NaN   \n",
      "3         6          4    Flatten        NaN        NaN       NaN  NaN   \n",
      "4         6          5      Dense       15.0        NaN       NaN  NaN   \n",
      "\n",
      "   epochs  batch_size  steps_per_epoch     lr  max_queue_size  \\\n",
      "0    30.0         8.0           3200.0  0.001           320.0   \n",
      "1     NaN         NaN              NaN    NaN             NaN   \n",
      "2     NaN         NaN              NaN    NaN             NaN   \n",
      "3     NaN         NaN              NaN    NaN             NaN   \n",
      "4     NaN         NaN              NaN    NaN             NaN   \n",
      "\n",
      "   kernel_regularizer  \n",
      "0                 0.0  \n",
      "1                 0.0  \n",
      "2                 0.0  \n",
      "3                 0.0  \n",
      "4                 0.0   \n",
      "\n",
      "Epoch 1/30\n",
      "3200/3200 [==============================] - 210s 66ms/step - loss: 1.8527 - mean_absolute_error: 0.5681 - val_loss: 0.3461 - val_mean_absolute_error: 0.2994\n",
      "Epoch 2/30\n",
      "3200/3200 [==============================] - 209s 65ms/step - loss: 0.2217 - mean_absolute_error: 0.2427 - val_loss: 0.1955 - val_mean_absolute_error: 0.2268\n",
      "Epoch 3/30\n",
      "3200/3200 [==============================] - 209s 65ms/step - loss: 0.1771 - mean_absolute_error: 0.1862 - val_loss: 0.1514 - val_mean_absolute_error: 0.1539\n",
      "Epoch 4/30\n",
      "3200/3200 [==============================] - 209s 65ms/step - loss: 0.1592 - mean_absolute_error: 0.1600 - val_loss: 0.1574 - val_mean_absolute_error: 0.1599\n",
      "Epoch 5/30\n",
      "3200/3200 [==============================] - 209s 65ms/step - loss: 0.1552 - mean_absolute_error: 0.1520 - val_loss: 0.1523 - val_mean_absolute_error: 0.1497\n",
      "Epoch 6/30\n",
      "3200/3200 [==============================] - 209s 65ms/step - loss: 0.1509 - mean_absolute_error: 0.1443 - val_loss: 0.1468 - val_mean_absolute_error: 0.1349\n",
      "Epoch 7/30\n",
      "3200/3200 [==============================] - 209s 65ms/step - loss: 0.1489 - mean_absolute_error: 0.1391 - val_loss: 0.1427 - val_mean_absolute_error: 0.1337\n",
      "Epoch 8/30\n",
      "3200/3200 [==============================] - 210s 66ms/step - loss: 0.1476 - mean_absolute_error: 0.1346 - val_loss: 0.1473 - val_mean_absolute_error: 0.1370\n",
      "Epoch 9/30\n",
      "3200/3200 [==============================] - 207s 65ms/step - loss: 0.1463 - mean_absolute_error: 0.1315 - val_loss: 0.1480 - val_mean_absolute_error: 0.1234\n",
      "Epoch 10/30\n",
      "3200/3200 [==============================] - 206s 65ms/step - loss: 0.1472 - mean_absolute_error: 0.1283 - val_loss: 0.1442 - val_mean_absolute_error: 0.1236\n",
      "Epoch 11/30\n",
      "3200/3200 [==============================] - 207s 65ms/step - loss: 0.1442 - mean_absolute_error: 0.1266 - val_loss: 0.1412 - val_mean_absolute_error: 0.1234\n",
      "Epoch 12/30\n",
      "3200/3200 [==============================] - 207s 65ms/step - loss: 0.1450 - mean_absolute_error: 0.1253 - val_loss: 0.1459 - val_mean_absolute_error: 0.1269\n",
      "Epoch 13/30\n",
      "3200/3200 [==============================] - 206s 65ms/step - loss: 0.1432 - mean_absolute_error: 0.1239 - val_loss: 0.1458 - val_mean_absolute_error: 0.1176\n",
      "Epoch 14/30\n",
      "3200/3200 [==============================] - 207s 65ms/step - loss: 0.1441 - mean_absolute_error: 0.1231 - val_loss: 0.1483 - val_mean_absolute_error: 0.1255\n",
      "Epoch 15/30\n",
      "3200/3200 [==============================] - 207s 65ms/step - loss: 0.1438 - mean_absolute_error: 0.1217 - val_loss: 0.1422 - val_mean_absolute_error: 0.1188\n",
      "Epoch 16/30\n",
      "3200/3200 [==============================] - 207s 65ms/step - loss: 0.1431 - mean_absolute_error: 0.1215 - val_loss: 0.1445 - val_mean_absolute_error: 0.1186\n",
      "Epoch 17/30\n",
      "3200/3200 [==============================] - 209s 65ms/step - loss: 0.1443 - mean_absolute_error: 0.1203 - val_loss: 0.1436 - val_mean_absolute_error: 0.1188\n",
      "Epoch 18/30\n",
      "3200/3200 [==============================] - 210s 66ms/step - loss: 0.1441 - mean_absolute_error: 0.1203 - val_loss: 0.1398 - val_mean_absolute_error: 0.1178\n",
      "Epoch 19/30\n",
      "3200/3200 [==============================] - 210s 66ms/step - loss: 0.1439 - mean_absolute_error: 0.1194 - val_loss: 0.1467 - val_mean_absolute_error: 0.1191\n",
      "Epoch 20/30\n",
      "3200/3200 [==============================] - 209s 65ms/step - loss: 0.1429 - mean_absolute_error: 0.1185 - val_loss: 0.1449 - val_mean_absolute_error: 0.1218\n",
      "Epoch 21/30\n",
      "3200/3200 [==============================] - 206s 64ms/step - loss: 0.1433 - mean_absolute_error: 0.1183 - val_loss: 0.1417 - val_mean_absolute_error: 0.1178\n",
      "Epoch 22/30\n",
      "3200/3200 [==============================] - 208s 65ms/step - loss: 0.1428 - mean_absolute_error: 0.1178 - val_loss: 0.1432 - val_mean_absolute_error: 0.1135\n",
      "Epoch 23/30\n",
      "3200/3200 [==============================] - 207s 65ms/step - loss: 0.1432 - mean_absolute_error: 0.1183 - val_loss: 0.1458 - val_mean_absolute_error: 0.1166\n",
      "Epoch 24/30\n",
      "3200/3200 [==============================] - 206s 64ms/step - loss: 0.1435 - mean_absolute_error: 0.1171 - val_loss: 0.1447 - val_mean_absolute_error: 0.1138\n",
      "Epoch 25/30\n",
      "3200/3200 [==============================] - 206s 64ms/step - loss: 0.1426 - mean_absolute_error: 0.1165 - val_loss: 0.1403 - val_mean_absolute_error: 0.1156\n",
      "Epoch 26/30\n",
      "3200/3200 [==============================] - 207s 65ms/step - loss: 0.1423 - mean_absolute_error: 0.1163 - val_loss: 0.1425 - val_mean_absolute_error: 0.1129\n",
      "Epoch 27/30\n",
      "3200/3200 [==============================] - 212s 66ms/step - loss: 0.1426 - mean_absolute_error: 0.1167 - val_loss: 0.1409 - val_mean_absolute_error: 0.1121\n",
      "Epoch 28/30\n",
      "3200/3200 [==============================] - 218s 68ms/step - loss: 0.1440 - mean_absolute_error: 0.1161 - val_loss: 0.1347 - val_mean_absolute_error: 0.1098\n",
      "Epoch 29/30\n",
      "3200/3200 [==============================] - 219s 69ms/step - loss: 0.1429 - mean_absolute_error: 0.1159 - val_loss: 0.1407 - val_mean_absolute_error: 0.1139\n",
      "Epoch 30/30\n",
      "3200/3200 [==============================] - 220s 69ms/step - loss: 0.1424 - mean_absolute_error: 0.1151 - val_loss: 0.1412 - val_mean_absolute_error: 0.1126\n",
      "Exporting plots\n",
      "Exporting CSV with test parameters and best mean absolute error result\n",
      "Exporting Test Data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Trained from scratch\n",
    "get_results_blocknoise(6,6,parent_dir,sqr_grid_width=50,load=False,name_append='_scratch_blocknoise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BEGINNING TEST: 6\n",
      "C:\\Users\\aambr\\OneDrive\\Documents\\UNH Fall 2019\\Research\\zernike_polynomials\\Data_and_Results_12-12-19\\test_plan_generator.csv\n",
      "Creating storage directory\n",
      "Creating the model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 2500, 32)          160       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2500, 32)          1056      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 80000)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 15)                1200015   \n",
      "=================================================================\n",
      "Total params: 1,202,287\n",
      "Trainable params: 1,202,287\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Fitting model\n",
      "SETTINGS FOR THIS TEST:\n",
      "   test_num  layer_num layer_type  num_nodes activation optimizer loss  \\\n",
      "0         6          1      Dense       32.0    sigmoid      Adam  mse   \n",
      "1         6          2      Dense       32.0    sigmoid       NaN  NaN   \n",
      "2         6          3      Dense       32.0    sigmoid       NaN  NaN   \n",
      "3         6          4    Flatten        NaN        NaN       NaN  NaN   \n",
      "4         6          5      Dense       15.0        NaN       NaN  NaN   \n",
      "\n",
      "   epochs  batch_size  steps_per_epoch     lr  max_queue_size  \\\n",
      "0    30.0         8.0           3200.0  0.001           320.0   \n",
      "1     NaN         NaN              NaN    NaN             NaN   \n",
      "2     NaN         NaN              NaN    NaN             NaN   \n",
      "3     NaN         NaN              NaN    NaN             NaN   \n",
      "4     NaN         NaN              NaN    NaN             NaN   \n",
      "\n",
      "   kernel_regularizer  \n",
      "0                 0.0  \n",
      "1                 0.0  \n",
      "2                 0.0  \n",
      "3                 0.0  \n",
      "4                 0.0   \n",
      "\n",
      "Epoch 1/30\n",
      "3200/3200 [==============================] - 143s 45ms/step - loss: 1.8803 - mean_absolute_error: 0.5749 - val_loss: 0.2140 - val_mean_absolute_error: 0.2535\n",
      "Epoch 2/30\n",
      "3200/3200 [==============================] - 144s 45ms/step - loss: 0.2083 - mean_absolute_error: 0.2176 - val_loss: 0.1627 - val_mean_absolute_error: 0.1825\n",
      "Epoch 3/30\n",
      "3200/3200 [==============================] - 147s 46ms/step - loss: 0.1721 - mean_absolute_error: 0.1726 - val_loss: 0.1601 - val_mean_absolute_error: 0.1689\n",
      "Epoch 4/30\n",
      "3200/3200 [==============================] - 157s 49ms/step - loss: 0.1604 - mean_absolute_error: 0.1607 - val_loss: 0.1579 - val_mean_absolute_error: 0.1531\n",
      "Epoch 5/30\n",
      "3200/3200 [==============================] - 178s 56ms/step - loss: 0.1550 - mean_absolute_error: 0.1519 - val_loss: 0.1542 - val_mean_absolute_error: 0.1450\n",
      "Epoch 6/30\n",
      "3200/3200 [==============================] - 146s 46ms/step - loss: 0.1517 - mean_absolute_error: 0.1457 - val_loss: 0.1521 - val_mean_absolute_error: 0.1446\n",
      "Epoch 7/30\n",
      "3200/3200 [==============================] - 146s 46ms/step - loss: 0.1483 - mean_absolute_error: 0.1386 - val_loss: 0.1528 - val_mean_absolute_error: 0.1363\n",
      "Epoch 8/30\n",
      "3200/3200 [==============================] - 149s 46ms/step - loss: 0.1481 - mean_absolute_error: 0.1348 - val_loss: 0.1487 - val_mean_absolute_error: 0.1277\n",
      "Epoch 9/30\n",
      "3200/3200 [==============================] - 148s 46ms/step - loss: 0.1473 - mean_absolute_error: 0.1315 - val_loss: 0.1443 - val_mean_absolute_error: 0.1253\n",
      "Epoch 10/30\n",
      "3200/3200 [==============================] - 146s 46ms/step - loss: 0.1452 - mean_absolute_error: 0.1283 - val_loss: 0.1472 - val_mean_absolute_error: 0.1285\n",
      "Epoch 11/30\n",
      "3200/3200 [==============================] - 143s 45ms/step - loss: 0.1452 - mean_absolute_error: 0.1270 - val_loss: 0.1470 - val_mean_absolute_error: 0.1277\n",
      "Epoch 12/30\n",
      "3200/3200 [==============================] - 140s 44ms/step - loss: 0.1463 - mean_absolute_error: 0.1257 - val_loss: 0.1423 - val_mean_absolute_error: 0.1267\n",
      "Epoch 13/30\n",
      "3200/3200 [==============================] - 141s 44ms/step - loss: 0.1465 - mean_absolute_error: 0.1246 - val_loss: 0.1439 - val_mean_absolute_error: 0.1214\n",
      "Epoch 14/30\n",
      "3200/3200 [==============================] - 140s 44ms/step - loss: 0.1440 - mean_absolute_error: 0.1231 - val_loss: 0.1478 - val_mean_absolute_error: 0.1188\n",
      "Epoch 15/30\n",
      "3200/3200 [==============================] - 140s 44ms/step - loss: 0.1440 - mean_absolute_error: 0.1219 - val_loss: 0.1425 - val_mean_absolute_error: 0.1191\n",
      "Epoch 16/30\n",
      "3200/3200 [==============================] - 140s 44ms/step - loss: 0.1445 - mean_absolute_error: 0.1213 - val_loss: 0.1474 - val_mean_absolute_error: 0.1227\n",
      "Epoch 17/30\n",
      "3200/3200 [==============================] - 140s 44ms/step - loss: 0.1444 - mean_absolute_error: 0.1207 - val_loss: 0.1425 - val_mean_absolute_error: 0.1200\n",
      "Epoch 18/30\n",
      "3200/3200 [==============================] - 140s 44ms/step - loss: 0.1449 - mean_absolute_error: 0.1203 - val_loss: 0.1459 - val_mean_absolute_error: 0.1245\n",
      "Epoch 19/30\n",
      "3200/3200 [==============================] - 140s 44ms/step - loss: 0.1433 - mean_absolute_error: 0.1194 - val_loss: 0.1406 - val_mean_absolute_error: 0.1172\n",
      "Epoch 20/30\n",
      "3200/3200 [==============================] - 141s 44ms/step - loss: 0.1427 - mean_absolute_error: 0.1183 - val_loss: 0.1454 - val_mean_absolute_error: 0.1201\n",
      "Epoch 21/30\n",
      "3200/3200 [==============================] - 141s 44ms/step - loss: 0.1425 - mean_absolute_error: 0.1184 - val_loss: 0.1424 - val_mean_absolute_error: 0.1145\n",
      "Epoch 22/30\n",
      "3200/3200 [==============================] - 142s 44ms/step - loss: 0.1430 - mean_absolute_error: 0.1174 - val_loss: 0.1443 - val_mean_absolute_error: 0.1149\n",
      "Epoch 23/30\n",
      "3200/3200 [==============================] - 140s 44ms/step - loss: 0.1436 - mean_absolute_error: 0.1177 - val_loss: 0.1495 - val_mean_absolute_error: 0.1161\n",
      "Epoch 24/30\n",
      "3200/3200 [==============================] - 141s 44ms/step - loss: 0.1416 - mean_absolute_error: 0.1170 - val_loss: 0.1454 - val_mean_absolute_error: 0.1254\n",
      "Epoch 25/30\n",
      "3200/3200 [==============================] - 141s 44ms/step - loss: 0.1437 - mean_absolute_error: 0.1171 - val_loss: 0.1405 - val_mean_absolute_error: 0.1140\n",
      "Epoch 26/30\n",
      "3200/3200 [==============================] - 139s 43ms/step - loss: 0.1434 - mean_absolute_error: 0.1169 - val_loss: 0.1450 - val_mean_absolute_error: 0.1175\n",
      "Epoch 27/30\n",
      "3200/3200 [==============================] - 137s 43ms/step - loss: 0.1425 - mean_absolute_error: 0.1164 - val_loss: 0.1442 - val_mean_absolute_error: 0.1260\n",
      "Epoch 28/30\n",
      "3200/3200 [==============================] - 138s 43ms/step - loss: 0.1423 - mean_absolute_error: 0.1158 - val_loss: 0.1429 - val_mean_absolute_error: 0.1177\n",
      "Epoch 29/30\n",
      "3200/3200 [==============================] - 137s 43ms/step - loss: 0.1426 - mean_absolute_error: 0.1157 - val_loss: 0.1460 - val_mean_absolute_error: 0.1177\n",
      "Epoch 30/30\n",
      "3200/3200 [==============================] - 140s 44ms/step - loss: 0.1420 - mean_absolute_error: 0.1156 - val_loss: 0.1388 - val_mean_absolute_error: 0.1097\n",
      "Exporting plots\n",
      "Exporting CSV with test parameters and best mean absolute error result\n",
      "Exporting Test Data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Trained from scratch\n",
    "get_results_block(6,6,parent_dir,sqr_grid_width=50,load=False,name_append='_scratch_block')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Zernike_CPU_DNN-12-10-19-R2_regularize_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
